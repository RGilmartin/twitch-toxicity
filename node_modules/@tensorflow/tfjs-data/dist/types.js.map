{"version":3,"file":"types.js","sourceRoot":"","sources":["../src/types.ts"],"names":[],"mappings":"AAAA;;;;;;;;;;;;;;;;GAgBG","sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\n * =============================================================================\n */\n\nimport {DataType, TensorContainer} from '@tensorflow/tfjs-core';\n\n// Maybe this should be called 'NestedContainer'-- that's just a bit unwieldy.\nexport type Container<T> = ContainerObject<T>|ContainerArray<T>;\n\nexport type ContainerOrT<T> = Container<T>|T;\n\nexport interface ContainerObject<T> {\n  [x: string]: ContainerOrT<T>;\n}\nexport interface ContainerArray<T> extends Array<ContainerOrT<T>> {}\n\n/**\n * Types supported by FileChunkIterator in both Browser and Node Environment.\n */\nexport type FileElement = File|Blob|Uint8Array;\n\n/**\n * A dictionary containing column level configurations when reading and decoding\n * CSV file(s) from csv source.\n * Has the following fields:\n * - `required` If value in this column is required. If set to `true`, throw an\n * error when it finds an empty value.\n *\n * - `dtype` Data type of this column. Could be int32, float32, bool, or string.\n *\n * - `default` Default value of this column.\n *\n * - `isLabel` Whether this column is label instead of features. If isLabel is\n * `true` for at least one column, the .csv() API will return an array of two\n * items: the first item is a dict of features key/value pairs, the second item\n * is a dict of labels key/value pairs. If no column is marked as label returns\n * a dict of features only.\n */\nexport interface ColumnConfig {\n  required?: boolean;\n  dtype?: DataType;\n  default?: TensorContainer;\n  isLabel?: boolean;\n}\n\n/**\n * Interface for configuring dataset when reading and decoding from CSV file(s).\n */\nexport interface CSVConfig {\n  /**\n   * A boolean value that indicates whether the first row of provided CSV file\n   * is a header line with column names, and should not be included in the data.\n   */\n  hasHeader?: boolean;\n\n  /**\n   * A list of strings that corresponds to the CSV column names, in order. If\n   * provided, it ignores the column names inferred from the header row. If not\n   * provided, infers the column names from the first row of the records. If\n   * `hasHeader` is false and `columnNames` is not provided, this method will\n   * throw an error.\n   */\n  columnNames?: string[];\n\n  /**\n   * A dictionary whose key is column names, value is an object stating if this\n   * column is required, column's data type, default value, and if this column\n   * is label. If provided, keys must correspond to names provided in\n   * `columnNames` or inferred from the file header lines. If any column is\n   * marked as label, the .csv() API will return an array of two items: the\n   * first item is a dict of features key/value pairs, the second item is a dict\n   * of labels key/value pairs. If no column is marked as label returns a dict\n   * of features only.\n   *\n   * Has the following fields:\n   * - `required` If value in this column is required. If set to `true`, throw\n   * an error when it finds an empty value.\n   *\n   * - `dtype` Data type of this column. Could be int32, float32, bool, or\n   * string.\n   *\n   * - `default` Default value of this column.\n   *\n   * - `isLabel` Whether this column is label instead of features. If isLabel is\n   * `true` for at least one column, the element in returned `CSVDataset` will\n   * be an object of {xs: features, ys: labels}: xs is a dict of features\n   * key/value pairs, ys is a dict of labels key/value pairs. If no column is\n   * marked as label, returns a dict of features only.\n   */\n  columnConfigs?: {[key: string]: ColumnConfig};\n\n  /**\n   * If true, only columns provided in `columnConfigs` will be parsed and\n   * provided during iteration.\n   */\n  configuredColumnsOnly?: boolean;\n\n  /**\n   * The string used to parse each line of the input file.\n   */\n  delimiter?: string;\n\n  /**\n   * If true, delimiter field should be null. Parsing delimiter is whitespace\n   * and treat continuous multiple whitespace as one delimiter.\n   */\n  delimWhitespace?: boolean;\n}\n\n/**\n * Interface configuring data from webcam video stream.\n */\nexport interface WebcamConfig {\n  /**\n   * A string specifying which camera to use on device. If the value is\n   * 'user', it will use front camera. If the value is 'environment', it will\n   * use rear camera.\n   */\n  facingMode?: 'user'|'environment';\n\n  /**\n   * A string used to request a specific camera. The deviceId can be obtained by\n   * calling `mediaDevices.enumerateDevices()`.\n   */\n  deviceId?: string;\n\n  /**\n   * Specifies the width of the output tensor. The actual width of the\n   * HTMLVideoElement (if provided) can be different and the final image will be\n   * resized to match resizeWidth.\n   */\n  resizeWidth?: number;\n\n  /**\n   * Specifies the height of the output tensor. The actual height of the\n   * HTMLVideoElement (if provided) can be different and the final image will be\n   * resized to match resizeHeight.\n   */\n  resizeHeight?: number;\n\n  /**\n   * A boolean value that indicates whether to crop the video frame from center.\n   * If true, `resizeWidth` and `resizeHeight` must be specified; then an image\n   * of size `[resizeWidth, resizeHeight]` is taken from the center of the frame\n   * without scaling. If false, the entire image is returned (perhaps scaled to\n   * fit in `[resizeWidth, resizeHeight]`, if those are provided).\n   */\n  centerCrop?: boolean;\n}\n\n/**\n * Interface configuring data from microphone audio stream.\n */\nexport interface MicrophoneConfig {\n  // A number representing Audio sampling rate in Hz. either 44,100 or 48,000.\n  // If provided sample rate is not available on the device, it will throw an\n  // error. Optional, defaults to the sample rate available on device.\n  sampleRateHz?: 44100|48000;\n\n  // The FFT length of each spectrogram column. A higher value will result in\n  // more details in the frequency domain but fewer details in the time domain.\n  // Must be a power of 2 between 2 to 4 and 2 to 14, so one of: 16, 32, 64,\n  // 128, 256, 512, 1024, 2048, 4096, 8192, and 16384. It will throw an error if\n  // it is an invalid number.Defaults to 1024.\n  fftSize?: number;\n\n  // Truncate each spectrogram column at how many frequency points. Each audio\n  // frame contains fftSize, for example, 1024 samples which covers voice\n  // frequency from 0 to 22,500 Hz. However, the frequency content relevant to\n  // human speech is generally in the frequency range from 0 to 5000 Hz. So each\n  // audio frame only need 232 columns to cover the frequency range of human\n  // voice. This will be part of the output spectrogram tensor shape. Optional,\n  // defaults to null which means no truncation.\n  columnTruncateLength?: number;\n\n  // Number of audio frames per spectrogram. The time duration of one\n  // spectrogram equals to numFramesPerSpectrogram*fftSize/sampleRateHz second.\n  // For example: the device sampling rate is 44,100 Hz, and fftSize is 1024,\n  // then each frame duration between two sampling is 0.023 second. If the\n  // purpose is for an audio model to recognize speech command that last 1\n  // second, each spectrogram should contain 1/0.023, which is 43 frames. This\n  // will be part of the output spectrogram tensor shape. Optional, defaults to\n  // 43 so that each audio data last 1 second.\n  numFramesPerSpectrogram?: number;\n\n  // A dictionary specifying the requirements of audio to request, such as\n  // deviceID, echoCancellation, etc. Optional.\n  audioTrackConstraints?: MediaTrackConstraints;\n\n  // The averaging constant with the last analysis frame -- basically, it makes\n  // the transition between values over time smoother. It is used by\n  // AnalyserNode interface during FFT. Optional, has to be between 0 and 1,\n  // defaults to 0.\n  smoothingTimeConstant?: number;\n\n  // Whether to collect the frequency domain audio spectrogram in\n  // MicrophoneIterator result. If both includeSpectrogram and includeWaveform\n  // are false, it will throw an error. Defaults to true.\n  includeSpectrogram?: boolean;\n\n  // Whether to collect the time domain audio waveform in MicrophoneIterator\n  // result. If both includeSpectrogram and includeWaveform are false, it will\n  // throw an error. Defaults to false.\n  includeWaveform?: boolean;\n}\n"]}