{"version":3,"file":"adadelta_optimizer_test.js","sourceRoot":"","sources":["../../src/optimizers/adadelta_optimizer_test.ts"],"names":[],"mappings":"AAAA;;;;;;;;;;;;;;;GAeG;AAEH,OAAO,KAAK,EAAE,MAAM,UAAU,CAAC;AAC/B,OAAO,EAAC,QAAQ,EAAE,iBAAiB,EAAC,MAAM,iBAAiB,CAAC;AAC5D,OAAO,EAAC,iBAAiB,EAAC,MAAM,cAAc,CAAC;AAE/C,iBAAiB,CAAC,mBAAmB,EAAE,QAAQ,EAAE,GAAG,EAAE;IACpD,EAAE,CAAC,OAAO,EAAE,KAAK,IAAI,EAAE;QACrB,MAAM,cAAc,GAAG,EAAE,CAAC,MAAM,EAAE,CAAC,UAAU,CAAC;QAC9C,MAAM,YAAY,GAAG,EAAE,CAAC;QACxB,MAAM,GAAG,GAAG,GAAG,CAAC;QAChB,MAAM,SAAS,GAAG,EAAE,CAAC,KAAK,CAAC,QAAQ,CAAC,YAAY,EAAE,GAAG,CAAC,CAAC;QAEvD,MAAM,CAAC,GAAG,EAAE,CAAC,QAAQ,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,QAAQ,EAAE,CAAC;QAEzC,MAAM,CAAC,GAAoB,GAAG,EAAE,CAAC,CAAC,CAAC,MAAM,EAAE,CAAC,GAAG,EAAE,CAAC;QAElD,IAAI,UAAU,GAAG,EAAE,CAAC,MAAM,EAAE,CAAC,UAAU,CAAC;QAExC,IAAI,IAAI,GAAG,SAAS,CAAC,QAAQ,CAAC,CAAC,EAAE,gBAAgB,CAAC,IAAI,CAAC,CAAC;QAExD,8DAA8D;QAC9D,MAAM,CAAC,EAAE,CAAC,MAAM,EAAE,CAAC,UAAU,CAAC,CAAC,IAAI,CAAC,UAAU,GAAG,CAAC,CAAC,CAAC;QAEpD,iBAAiB;QACjB,oEAAoE;QACpE,wDAAwD;QACxD,sCAAsC;QACtC,2EAA2E;QAC3E,8BAA8B;QAC9B,EAAE;QACF,iBAAiB;QACjB,2BAA2B;QAC3B,gCAAgC;QAChC,qBAAqB;QACrB,kCAAkC;QAClC,iBAAiB;QACjB,iBAAiB,CAAC,MAAM,CAAC,CAAC,IAAI,EAAE,EAAE,CAAC,GAAG,EAAE,GAAG,CAAC,CAAC,CAAC;QAE9C,IAAI,CAAC,OAAO,EAAE,CAAC;QACf,UAAU,GAAG,EAAE,CAAC,MAAM,EAAE,CAAC,UAAU,CAAC;QAEpC,IAAI,GAAG,SAAS,CAAC,QAAQ,CAAC,CAAC,EAAE,gBAAgB,CAAC,KAAK,CAAC,CAAC;QAErD,qBAAqB;QACrB,6BAA6B;QAC7B,+BAA+B;QAC/B,sCAAsC;QACtC,yBAAyB;QACzB,mBAAmB;QACnB,iBAAiB,CAAC,MAAM,CAAC,CAAC,IAAI,EAAE,EAAE,CAAC,IAAI,EAAE,IAAI,CAAC,CAAC,CAAC;QAEhD,6CAA6C;QAC7C,MAAM,CAAC,EAAE,CAAC,MAAM,EAAE,CAAC,UAAU,CAAC,CAAC,IAAI,CAAC,UAAU,CAAC,CAAC;QAEhD,MAAM,CAAC,IAAI,CAAC,CAAC,IAAI,CAAC,IAAI,CAAC,CAAC;QAExB,CAAC,CAAC,OAAO,EAAE,CAAC;QACZ,SAAS,CAAC,OAAO,EAAE,CAAC;QAEpB,sEAAsE;QACtE,MAAM,CAAC,EAAE,CAAC,MAAM,EAAE,CAAC,UAAU,CAAC,CAAC,IAAI,CAAC,cAAc,GAAG,CAAC,CAAC,CAAC;IAC1D,CAAC,CAAC,CAAC;IAEH,EAAE,CAAC,0CAA0C,EAAE,KAAK,IAAI,EAAE;QACxD,MAAM,YAAY,GAAG,EAAE,CAAC;QACxB,MAAM,GAAG,GAAG,GAAG,CAAC;QAChB,MAAM,UAAU,GAAG,EAAE,CAAC,KAAK,CAAC,QAAQ,CAAC,YAAY,EAAE,GAAG,CAAC,CAAC;QAExD,MAAM,CAAC,GAAG,EAAE,CAAC,QAAQ,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,QAAQ,EAAE,CAAC;QACzC,MAAM,CAAC,GAAoB,GAAG,EAAE,CAAC,CAAC,CAAC,MAAM,EAAE,CAAC,GAAG,EAAE,CAAC;QAElD,IAAI,IAAI,GAAG,UAAU,CAAC,QAAQ,CAAC,CAAC,EAAE,gBAAgB,CAAC,IAAI,CAAC,CAAC;QACzD,iBAAiB,CAAC,MAAM,IAAI,CAAC,IAAI,EAAE,EAAE,CAAC,CAAC,CAAC;QACxC,iBAAiB,CAAC,MAAM,CAAC,CAAC,IAAI,EAAE,EAAE,CAAC,GAAG,EAAE,GAAG,CAAC,CAAC,CAAC;QAE9C,MAAM,OAAO,GAAG,MAAM,UAAU,CAAC,UAAU,EAAE,CAAC;QAC9C,MAAM,CAAC,OAAO,CAAC,MAAM,CAAC,CAAC,OAAO,CAAC,CAAC,CAAC,CAAC;QAClC,MAAM,CAAC,OAAO,CAAC,CAAC,CAAC,CAAC,IAAI,CAAC,CAAC,OAAO,CAAC,MAAM,CAAC,CAAC;QAExC,MAAM,UAAU,GAAG,EAAE,CAAC,KAAK,CAAC,QAAQ,CAAC,YAAY,EAAE,GAAG,CAAC,CAAC;QACxD,MAAM,UAAU,CAAC,UAAU,CAAC,OAAO,CAAC,CAAC;QAErC,IAAI,GAAG,UAAU,CAAC,QAAQ,CAAC,CAAC,EAAE,gBAAgB,CAAC,IAAI,CAAC,CAAC;QACrD,iBAAiB,CAAC,MAAM,IAAI,CAAC,IAAI,EAAE,EAAE,GAAG,CAAC,CAAC;QAC1C,iBAAiB,CAAC,MAAM,CAAC,CAAC,IAAI,EAAE,EAAE,CAAC,IAAI,EAAE,IAAI,CAAC,CAAC,CAAC;QAChD,MAAM,CAAC,UAAU,CAAC,UAAU,CAAC,CAAC,OAAO,CAAC,CAAC,CAAC,CAAC;IAC3C,CAAC,CAAC,CAAC;IAEH,EAAE,CAAC,0BAA0B,EAAE,GAAG,EAAE;QAClC,MAAM,WAAW,GAAG,EAAE,CAAC,KAAK,CAAC,QAAQ,CAAC,GAAG,EAAE,GAAG,EAAE,IAAI,CAAC,CAAC;QACtD,MAAM,YAAY,GAAG,EAAE,CAAC,iBAAiB,CAAC,UAAU,CAChD,EAAE,CAAC,iBAAiB,EAAE,WAAW,CAAC,SAAS,EAAE,CAAC,CAAC;QACnD,MAAM,CAAC,YAAY,CAAC,SAAS,EAAE,CAAC,CAAC,OAAO,CAAC,WAAW,CAAC,SAAS,EAAE,CAAC,CAAC;IACpE,CAAC,CAAC,CAAC;AACL,CAAC,CAAC,CAAC","sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport * as tf from '../index';\nimport {ALL_ENVS, describeWithFlags} from '../jasmine_util';\nimport {expectArraysClose} from '../test_util';\n\ndescribeWithFlags('AdadeltaOptimizer', ALL_ENVS, () => {\n  it('basic', async () => {\n    const initialTensors = tf.memory().numTensors;\n    const learningRate = .1;\n    const rho = .95;\n    const optimizer = tf.train.adadelta(learningRate, rho);\n\n    const x = tf.tensor1d([1, 2]).variable();\n\n    const f: () => tf.Scalar = () => x.square().sum();\n\n    let numTensors = tf.memory().numTensors;\n\n    let cost = optimizer.minimize(f, /* returnCost */ true);\n\n    // Cost & 2 accumulators should be the only additional arrays.\n    expect(tf.memory().numTensors).toBe(numTensors + 3);\n\n    // epsilon = 1-e8\n    // newAccumulatedGrad = rho * accumulatedGrad + (1 - rho) * grad ^ 2\n    // updates = -grad * sqrt(accumulatedUpdate + epsilon) /\n    //     sqrt(accumulatedGrad + epsilon)\n    // newAccumulatedUpdate = rho * accumulatedUpdate + (1 - rho) * updates ^ 2\n    // x += learningRate * updates\n    //\n    // de/dx = [2, 4]\n    // accumulatedGrad = [0, 0]\n    // newAccumulatedGrad = [.2, .8]\n    // updates = [-2, -4]\n    // newAccumulatedUpdate = [.2, .8]\n    // x = [0.8, 1.6]\n    expectArraysClose(await x.data(), [0.8, 1.6]);\n\n    cost.dispose();\n    numTensors = tf.memory().numTensors;\n\n    cost = optimizer.minimize(f, /* returnCost */ false);\n\n    // de/dx = [1.6, 3.2]\n    // accumulatedGrad = [.2, .8]\n    // accumulatedUpdate = [.2, .8]\n    // newAccumulatedGrad = [0.318, 1.272]\n    // updates = [-1.6, -3.2]\n    // x = [0.64, 1.28]\n    expectArraysClose(await x.data(), [0.64, 1.28]);\n\n    // There should be no new additional Tensors.\n    expect(tf.memory().numTensors).toBe(numTensors);\n\n    expect(cost).toBe(null);\n\n    x.dispose();\n    optimizer.dispose();\n\n    // The only additional tensor remaining is the argument to variable().\n    expect(tf.memory().numTensors).toBe(initialTensors + 1);\n  });\n\n  it('Save, load weights and continue training', async () => {\n    const learningRate = .1;\n    const rho = .95;\n    const optimizer1 = tf.train.adadelta(learningRate, rho);\n\n    const x = tf.tensor1d([1, 2]).variable();\n    const f: () => tf.Scalar = () => x.square().sum();\n\n    let cost = optimizer1.minimize(f, /* returnCost */ true);\n    expectArraysClose(await cost.data(), 5);\n    expectArraysClose(await x.data(), [0.8, 1.6]);\n\n    const weights = await optimizer1.getWeights();\n    expect(weights.length).toEqual(3);\n    expect(weights[0].name).toEqual('iter');\n\n    const optimizer2 = tf.train.adadelta(learningRate, rho);\n    await optimizer2.setWeights(weights);\n\n    cost = optimizer2.minimize(f, /* returnCost */ true);\n    expectArraysClose(await cost.data(), 3.2);\n    expectArraysClose(await x.data(), [0.64, 1.28]);\n    expect(optimizer2.iterations).toEqual(2);\n  });\n\n  it('serialization round-trip', () => {\n    const originalOpt = tf.train.adadelta(0.1, 0.2, 2e-8);\n    const reserialized = tf.AdadeltaOptimizer.fromConfig(\n        tf.AdadeltaOptimizer, originalOpt.getConfig());\n    expect(reserialized.getConfig()).toEqual(originalOpt.getConfig());\n  });\n});\n"]}