{"version":3,"file":"adamax_optimizer_test.js","sourceRoot":"","sources":["../../src/optimizers/adamax_optimizer_test.ts"],"names":[],"mappings":"AAAA;;;;;;;;;;;;;;;GAeG;AAEH,OAAO,KAAK,EAAE,MAAM,UAAU,CAAC;AAC/B,OAAO,EAAC,QAAQ,EAAE,iBAAiB,EAAC,MAAM,iBAAiB,CAAC;AAC5D,OAAO,EAAC,iBAAiB,EAAC,MAAM,cAAc,CAAC;AAE/C,iBAAiB,CAAC,iBAAiB,EAAE,QAAQ,EAAE,GAAG,EAAE;IAClD,EAAE,CAAC,OAAO,EAAE,KAAK,IAAI,EAAE;QACrB,MAAM,cAAc,GAAG,EAAE,CAAC,MAAM,EAAE,CAAC,UAAU,CAAC;QAC9C,MAAM,YAAY,GAAG,GAAG,CAAC;QACzB,MAAM,KAAK,GAAG,GAAG,CAAC;QAClB,MAAM,KAAK,GAAG,GAAG,CAAC;QAClB,MAAM,KAAK,GAAG,GAAG,CAAC;QAClB,MAAM,SAAS,GACX,EAAE,CAAC,KAAK,CAAC,MAAM,CAAC,YAAY,EAAE,KAAK,EAAE,KAAK,EAAE,SAAS,EAAE,KAAK,CAAC,CAAC;QAElE,MAAM,CAAC,GAAG,EAAE,CAAC,QAAQ,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,QAAQ,EAAE,CAAC;QAEzC,MAAM,CAAC,GAAoB,GAAG,EAAE,CAAC,CAAC,CAAC,MAAM,EAAE,CAAC,GAAG,EAAE,CAAC;QAElD,IAAI,UAAU,GAAG,EAAE,CAAC,MAAM,EAAE,CAAC,UAAU,CAAC;QAExC,IAAI,IAAI,GAAG,SAAS,CAAC,QAAQ,CAAC,CAAC,EAAE,gBAAgB,CAAC,IAAI,CAAC,CAAC;QAExD,8DAA8D;QAC9D,MAAM,CAAC,EAAE,CAAC,MAAM,EAAE,CAAC,UAAU,CAAC,CAAC,IAAI,CAAC,UAAU,GAAG,CAAC,CAAC,CAAC;QACpD,kBAAkB;QAClB,mDAAmD;QACnD,kDAAkD;QAClD,gBAAgB;QAChB,EAAE;QACF,gDAAgD;QAChD,WAAW;QACX,mBAAmB;QACnB,kBAAkB;QAClB,aAAa;QACb,mDAAmD;QACnD,EAAE;QACF,wCAAwC;QACxC,4BAA4B;QAC5B,4CAA4C;QAC5C,2CAA2C;QAC3C,iBAAiB;QACjB,QAAQ;QACR,yBAAyB;QACzB,wBAAwB;QACxB,iBAAiB;QACjB,EAAE;QACF,iBAAiB,CAAC,MAAM,CAAC,CAAC,IAAI,EAAE,EAAE,CAAC,GAAG,EAAE,GAAG,CAAC,CAAC,CAAC;QAE9C,IAAI,CAAC,OAAO,EAAE,CAAC;QACf,UAAU,GAAG,EAAE,CAAC,MAAM,EAAE,CAAC,UAAU,CAAC;QAEpC,IAAI,GAAG,SAAS,CAAC,QAAQ,CAAC,CAAC,EAAE,gBAAgB,CAAC,KAAK,CAAC,CAAC;QAErD,wBAAwB;QACxB,kBAAkB;QAClB,mDAAmD;QACnD,kDAAkD;QAClD,QAAQ;QACR,4BAA4B;QAC5B,2BAA2B;QAC3B,kBAAkB;QAClB,EAAE;QACF,2CAA2C;QAC3C,cAAc;QACd,aAAa;QACb,iBAAiB;QACjB,WAAW;QACX,mBAAmB;QACnB,kBAAkB;QAClB,iBAAiB;QACjB,uDAAuD;QACvD,EAAE;QACF,6CAA6C;QAC7C,EAAE;QACF,uDAAuD;QACvD,4BAA4B;QAC5B,4CAA4C;QAC5C,2CAA2C;QAC3C,yBAAyB;QACzB,QAAQ;QACR,yBAAyB;QACzB,wBAAwB;QACxB,wBAAwB;QACxB,EAAE;QACF,iBAAiB,CAAC,MAAM,CAAC,CAAC,IAAI,EAAE,EAAE,CAAC,OAAO,EAAE,MAAM,CAAC,CAAC,CAAC;QACrD,6CAA6C;QAC7C,MAAM,CAAC,EAAE,CAAC,MAAM,EAAE,CAAC,UAAU,CAAC,CAAC,IAAI,CAAC,UAAU,CAAC,CAAC;QAEhD,MAAM,CAAC,IAAI,CAAC,CAAC,IAAI,CAAC,IAAI,CAAC,CAAC;QAExB,CAAC,CAAC,OAAO,EAAE,CAAC;QACZ,SAAS,CAAC,OAAO,EAAE,CAAC;QAEpB,iEAAiE;QACjE,cAAc;QACd,MAAM,CAAC,EAAE,CAAC,MAAM,EAAE,CAAC,UAAU,CAAC,CAAC,IAAI,CAAC,cAAc,GAAG,CAAC,CAAC,CAAC;IAC1D,CAAC,CAAC,CAAC;IAEH,EAAE,CAAC,0BAA0B,EAAE,GAAG,EAAE;QAClC,MAAM,WAAW,GAAG,EAAE,CAAC,KAAK,CAAC,MAAM,CAAC,GAAG,EAAE,GAAG,EAAE,GAAG,EAAE,IAAI,EAAE,GAAG,CAAC,CAAC;QAC9D,MAAM,YAAY,GAAG,EAAE,CAAC,eAAe,CAAC,UAAU,CAC9C,EAAE,CAAC,eAAe,EAAE,WAAW,CAAC,SAAS,EAAE,CAAC,CAAC;QACjD,MAAM,CAAC,YAAY,CAAC,SAAS,EAAE,CAAC,CAAC,OAAO,CAAC,WAAW,CAAC,SAAS,EAAE,CAAC,CAAC;IACpE,CAAC,CAAC,CAAC;AACL,CAAC,CAAC,CAAC","sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport * as tf from '../index';\nimport {ALL_ENVS, describeWithFlags} from '../jasmine_util';\nimport {expectArraysClose} from '../test_util';\n\ndescribeWithFlags('AdamaxOptimizer', ALL_ENVS, () => {\n  it('basic', async () => {\n    const initialTensors = tf.memory().numTensors;\n    const learningRate = 0.1;\n    const beta1 = 0.8;\n    const beta2 = 0.9;\n    const decay = 0.1;\n    const optimizer =\n        tf.train.adamax(learningRate, beta1, beta2, undefined, decay);\n\n    const x = tf.tensor1d([2, 4]).variable();\n\n    const f: () => tf.Scalar = () => x.square().sum();\n\n    let numTensors = tf.memory().numTensors;\n\n    let cost = optimizer.minimize(f, /* returnCost */ true);\n\n    // Cost & 2 accumulators should be the only additional arrays.\n    expect(tf.memory().numTensors).toBe(numTensors + 3);\n    // new_first_m = [\n    //    beta1 * old_first_m_w1 + (1-beta1) * grad_w1,\n    //    beta1 * old_first_m_w2 + (1-beta1) * grad_w2\n    // ] = [.8, 1.6]\n    //\n    // ut_0 = beta2 * old_weighted_inf_norm = [0, 0]\n    // u1_1 = [\n    //    abs(grad_w1),\n    //    abs(grad_w2)\n    // ] = [4, 8]\n    // new_weighted_inf_norm = max(ut_0, ut_1) = [4, 8]\n    //\n    // coefficient = alpha / (1-beta1) = 0.5\n    // updates = coefficient * [\n    //    new_first_m1 / new_weighted_inf_norm1,\n    //    new_first_m2 / new_weighted_inf_norm2\n    // ] = [0.1, 0.1]\n    // w = [\n    //    w1_old - updates_1,\n    //    w2_old - updates_2\n    // ] = [1.9, 3.9]\n    //\n    expectArraysClose(await x.data(), [1.9, 3.9]);\n\n    cost.dispose();\n    numTensors = tf.memory().numTensors;\n\n    cost = optimizer.minimize(f, /* returnCost */ false);\n\n    // gradient = [3.8, 7.8]\n    // new_first_m = [\n    //    beta1 * old_first_m_w1 + (1-beta1) * grad_w1,\n    //    beta1 * old_first_m_w2 + (1-beta1) * grad_w2\n    // ] = [\n    //    0.8 * 0.8 + 0.2 * 3.8,\n    //    0.8 * 1.6 + 0.2 * 7.8\n    // ] = [1.4, 2.84]\n    //\n    // ut_0 = beta2 * old_weighted_inf_norm = [\n    //    0.9 * 4,\n    //    0.9 * 8\n    // ] = [3.6, 7.2]\n    // u1_1 = [\n    //    abs(grad_w1),\n    //    abs(grad_w2)\n    // ] = [3.8, 7.8]\n    // new_weighted_inf_norm = max(ut_0, ut_1) = [3.8, 7.8]\n    //\n    // alpha = 0.1 / (1 + 0.1 * 1) = 0.0909090909\n    //\n    // coefficient = alpha / (1 - beta1*beta1) = 0.25252525\n    // updates = coefficient * [\n    //    new_first_m1 / new_weighted_inf_norm1,\n    //    new_first_m2 / new_weighted_inf_norm2\n    // ] = [0.09303, 0.09194]\n    // w = [\n    //    w1_old - updates_1,\n    //    w2_old - updates_2\n    // ] = [1.80697, 3.8086]\n    //\n    expectArraysClose(await x.data(), [1.80697, 3.8086]);\n    // There should be no new additional Tensors.\n    expect(tf.memory().numTensors).toBe(numTensors);\n\n    expect(cost).toBe(null);\n\n    x.dispose();\n    optimizer.dispose();\n\n    // The only additional tensor remaining should be the argument to\n    // variable().\n    expect(tf.memory().numTensors).toBe(initialTensors + 1);\n  });\n\n  it('serialization round-trip', () => {\n    const originalOpt = tf.train.adamax(0.1, 0.2, 0.3, 2e-8, 0.1);\n    const reserialized = tf.AdamaxOptimizer.fromConfig(\n        tf.AdamaxOptimizer, originalOpt.getConfig());\n    expect(reserialized.getConfig()).toEqual(originalOpt.getConfig());\n  });\n});\n"]}