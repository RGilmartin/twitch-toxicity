{"version":3,"file":"adam_optimizer_test.js","sourceRoot":"","sources":["../../src/optimizers/adam_optimizer_test.ts"],"names":[],"mappings":"AAAA;;;;;;;;;;;;;;;GAeG;AAEH,OAAO,KAAK,EAAE,MAAM,UAAU,CAAC;AAC/B,OAAO,EAAC,QAAQ,EAAE,iBAAiB,EAAC,MAAM,iBAAiB,CAAC;AAC5D,OAAO,EAAC,iBAAiB,EAAC,MAAM,cAAc,CAAC;AAE/C,iBAAiB,CAAC,eAAe,EAAE,QAAQ,EAAE,GAAG,EAAE;IAChD,EAAE,CAAC,OAAO,EAAE,KAAK,IAAI,EAAE;QACrB,MAAM,cAAc,GAAG,EAAE,CAAC,MAAM,EAAE,CAAC,UAAU,CAAC;QAC9C,MAAM,YAAY,GAAG,EAAE,CAAC;QACxB,MAAM,KAAK,GAAG,EAAE,CAAC;QACjB,MAAM,KAAK,GAAG,EAAE,CAAC;QACjB,MAAM,SAAS,GAAG,EAAE,CAAC,KAAK,CAAC,IAAI,CAAC,YAAY,EAAE,KAAK,EAAE,KAAK,CAAC,CAAC;QAE5D,MAAM,CAAC,GAAG,EAAE,CAAC,QAAQ,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,QAAQ,EAAE,CAAC;QAEzC,MAAM,CAAC,GAAoB,GAAG,EAAE,CAAC,CAAC,CAAC,MAAM,EAAE,CAAC,GAAG,EAAE,CAAC;QAElD,IAAI,UAAU,GAAG,EAAE,CAAC,MAAM,EAAE,CAAC,UAAU,CAAC;QAExC,IAAI,IAAI,GAAG,SAAS,CAAC,QAAQ,CAAC,CAAC,EAAE,gBAAgB,CAAC,IAAI,CAAC,CAAC;QAExD,8DAA8D;QAC9D,MAAM,CAAC,EAAE,CAAC,MAAM,EAAE,CAAC,UAAU,CAAC,CAAC,IAAI,CAAC,UAAU,GAAG,CAAC,CAAC,CAAC;QACpD,kBAAkB;QAClB,mDAAmD;QACnD,kDAAkD;QAClD,gBAAgB;QAChB,mBAAmB;QACnB,uDAAuD;QACvD,sDAAsD;QACtD,iBAAiB;QACjB,2CAA2C;QAC3C,8CAA8C;QAC9C,0CAA0C;QAC1C,EAAE;QACF,iBAAiB,CAAC,MAAM,CAAC,CAAC,IAAI,EAAE,EAAE,CAAC,GAAG,EAAE,GAAG,CAAC,CAAC,CAAC;QAE9C,IAAI,CAAC,OAAO,EAAE,CAAC;QACf,UAAU,GAAG,EAAE,CAAC,MAAM,EAAE,CAAC,UAAU,CAAC;QAEpC,IAAI,GAAG,SAAS,CAAC,QAAQ,CAAC,CAAC,EAAE,gBAAgB,CAAC,KAAK,CAAC,CAAC;QAErD,kBAAkB;QAClB,mDAAmD;QACnD,kDAAkD;QAClD,kBAAkB;QAClB,mBAAmB;QACnB,uDAAuD;QACvD,sDAAsD;QACtD,sBAAsB;QACtB,wDAAwD;QACxD,wDAAwD;QACxD,mDAAmD;QACnD,EAAE;QACF,iBAAiB,CAAC,MAAM,CAAC,CAAC,IAAI,EAAE,EAAE,CAAC,SAAS,EAAE,MAAM,CAAC,CAAC,CAAC;QACvD,6CAA6C;QAC7C,MAAM,CAAC,EAAE,CAAC,MAAM,EAAE,CAAC,UAAU,CAAC,CAAC,IAAI,CAAC,UAAU,CAAC,CAAC;QAEhD,MAAM,CAAC,IAAI,CAAC,CAAC,IAAI,CAAC,IAAI,CAAC,CAAC;QAExB,CAAC,CAAC,OAAO,EAAE,CAAC;QACZ,SAAS,CAAC,OAAO,EAAE,CAAC;QAEpB,iEAAiE;QACjE,cAAc;QACd,MAAM,CAAC,EAAE,CAAC,MAAM,EAAE,CAAC,UAAU,CAAC,CAAC,IAAI,CAAC,cAAc,GAAG,CAAC,CAAC,CAAC;IAC1D,CAAC,CAAC,CAAC;IAEH,EAAE,CAAC,yCAAyC,EAAE,KAAK,IAAI,EAAE;QACvD,MAAM,cAAc,GAAG,EAAE,CAAC,MAAM,EAAE,CAAC,UAAU,CAAC;QAC9C,MAAM,YAAY,GAAG,EAAE,CAAC;QACxB,MAAM,KAAK,GAAG,EAAE,CAAC;QACjB,MAAM,KAAK,GAAG,EAAE,CAAC;QACjB,MAAM,UAAU,GAAG,EAAE,CAAC,KAAK,CAAC,IAAI,CAAC,YAAY,EAAE,KAAK,EAAE,KAAK,CAAC,CAAC;QAE7D,MAAM,CAAC,GAAG,EAAE,CAAC,QAAQ,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,QAAQ,EAAE,CAAC;QACzC,MAAM,CAAC,GAAoB,GAAG,EAAE,CAAC,CAAC,CAAC,MAAM,EAAE,CAAC,GAAG,EAAE,CAAC;QAClD,IAAI,IAAI,GAAG,UAAU,CAAC,QAAQ,CAAC,CAAC,EAAE,gBAAgB,CAAC,IAAI,CAAC,CAAC;QACzD,MAAM,CAAC,UAAU,CAAC,UAAU,CAAC,CAAC,OAAO,CAAC,CAAC,CAAC,CAAC;QACzC,iBAAiB,CAAC,MAAM,IAAI,CAAC,IAAI,EAAE,EAAE,EAAE,CAAC,CAAC;QAEzC,MAAM,OAAO,GAAG,MAAM,UAAU,CAAC,UAAU,EAAE,CAAC;QAC9C,MAAM,CAAC,OAAO,CAAC,MAAM,CAAC,CAAC,OAAO,CAAC,CAAC,CAAC,CAAC;QAClC,MAAM,CAAC,OAAO,CAAC,CAAC,CAAC,CAAC,IAAI,CAAC,CAAC,OAAO,CAAC,MAAM,CAAC,CAAC;QACxC,MAAM,CAAC,OAAO,CAAC,CAAC,CAAC,CAAC,IAAI,CAAC,CAAC,OAAO,CAAC,GAAG,CAAC,CAAC,IAAI,IAAI,CAAC,CAAC;QAC/C,MAAM,CAAC,OAAO,CAAC,CAAC,CAAC,CAAC,IAAI,CAAC,CAAC,OAAO,CAAC,GAAG,CAAC,CAAC,IAAI,IAAI,CAAC,CAAC;QAE/C,MAAM,UAAU,GAAG,EAAE,CAAC,KAAK,CAAC,IAAI,CAAC,YAAY,EAAE,KAAK,EAAE,KAAK,CAAC,CAAC;QAC7D,MAAM,UAAU,CAAC,UAAU,CAAC,OAAO,CAAC,CAAC;QAErC,IAAI,GAAG,UAAU,CAAC,QAAQ,CAAC,CAAC,EAAE,gBAAgB,CAAC,IAAI,CAAC,CAAC;QACrD,iBAAiB,CAAC,MAAM,IAAI,CAAC,IAAI,EAAE,EAAE,KAAK,CAAC,CAAC;QAC5C,MAAM,CAAC,UAAU,CAAC,UAAU,CAAC,CAAC,OAAO,CAAC,CAAC,CAAC,CAAC;QAEzC,MAAM,UAAU,GAAG,EAAE,CAAC,KAAK,CAAC,IAAI,CAAC,YAAY,EAAE,KAAK,EAAE,KAAK,CAAC,CAAC;QAC7D,MAAM,UAAU,CAAC,UAAU,CAAC,MAAM,UAAU,CAAC,UAAU,EAAE,CAAC,CAAC;QAC3D,IAAI,GAAG,UAAU,CAAC,QAAQ,CAAC,CAAC,EAAE,gBAAgB,CAAC,IAAI,CAAC,CAAC;QACrD,iBAAiB,CAAC,MAAM,IAAI,CAAC,IAAI,EAAE,EAAE,SAAS,CAAC,CAAC;QAChD,MAAM,CAAC,UAAU,CAAC,UAAU,CAAC,CAAC,OAAO,CAAC,cAAc,GAAG,CAAC,CAAC,CAAC;IAC5D,CAAC,CAAC,CAAC;IAEH,EAAE,CAAC,0BAA0B,EAAE,GAAG,EAAE;QAClC,MAAM,WAAW,GAAG,EAAE,CAAC,KAAK,CAAC,IAAI,CAAC,GAAG,EAAE,GAAG,EAAE,GAAG,EAAE,IAAI,CAAC,CAAC;QACvD,MAAM,YAAY,GACd,EAAE,CAAC,aAAa,CAAC,UAAU,CAAC,EAAE,CAAC,aAAa,EAAE,WAAW,CAAC,SAAS,EAAE,CAAC,CAAC;QAC3E,MAAM,CAAC,YAAY,CAAC,SAAS,EAAE,CAAC,CAAC,OAAO,CAAC,WAAW,CAAC,SAAS,EAAE,CAAC,CAAC;IACpE,CAAC,CAAC,CAAC;AACL,CAAC,CAAC,CAAC","sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport * as tf from '../index';\nimport {ALL_ENVS, describeWithFlags} from '../jasmine_util';\nimport {expectArraysClose} from '../test_util';\n\ndescribeWithFlags('AdamOptimizer', ALL_ENVS, () => {\n  it('basic', async () => {\n    const initialTensors = tf.memory().numTensors;\n    const learningRate = .1;\n    const beta1 = .8;\n    const beta2 = .9;\n    const optimizer = tf.train.adam(learningRate, beta1, beta2);\n\n    const x = tf.tensor1d([2, 4]).variable();\n\n    const f: () => tf.Scalar = () => x.square().sum();\n\n    let numTensors = tf.memory().numTensors;\n\n    let cost = optimizer.minimize(f, /* returnCost */ true);\n\n    // Cost & 2 accumulators should be the only additional arrays.\n    expect(tf.memory().numTensors).toBe(numTensors + 3);\n    // new_first_m = [\n    //    beta1 * old_first_m_w1 + (1-beta1) * grad_w1,\n    //    beta1 * old_first_m_w2 + (1-beta1) * grad_w2\n    // ] = [.8, 1.6]\n    // new_second_m = [\n    //    beta2 * old_second_m_w1 + (1-beta2) * grad_w1**2,\n    //    beta2 * old_second_m_w2 + (1-beta2) * grad_w2**2\n    // ] = [1.6, 6.4]\n    // m = [new_first_m/(1-acc_beta1)] = [4, 8]\n    // v = [new_second_m/(1-acc_beta2)] = [16, 64]\n    // x = [x - lr * m / sqrt(v)] = [1.9, 3.9]\n    //\n    expectArraysClose(await x.data(), [1.9, 3.9]);\n\n    cost.dispose();\n    numTensors = tf.memory().numTensors;\n\n    cost = optimizer.minimize(f, /* returnCost */ false);\n\n    // new_first_m = [\n    //    beta1 * old_first_m_w1 + (1-beta1) * grad_w1,\n    //    beta1 * old_first_m_w2 + (1-beta1) * grad_w2\n    // ] = [1.4, 2.84]\n    // new_second_m = [\n    //    beta2 * old_second_m_w1 + (1-beta2) * grad_w1**2,\n    //    beta2 * old_second_m_w2 + (1-beta2) * grad_w2**2\n    // ] = [2.884, 11.884]\n    // m = [new_first_m/(1-acc_beta1)] = [3.888888, 7.88889]\n    // v = [new_second_m/(1-acc_beta2)] = [15.1789, 62.5473]\n    // x = [x - lr * m / sqrt(v)] = [1.8000001, 3.8002]\n    //\n    expectArraysClose(await x.data(), [1.8000001, 3.8002]);\n    // There should be no new additional Tensors.\n    expect(tf.memory().numTensors).toBe(numTensors);\n\n    expect(cost).toBe(null);\n\n    x.dispose();\n    optimizer.dispose();\n\n    // The only additional tensor remaining should be the argument to\n    // variable().\n    expect(tf.memory().numTensors).toBe(initialTensors + 1);\n  });\n\n  it('Continue training after loading weights', async () => {\n    const initialTensors = tf.memory().numTensors;\n    const learningRate = .1;\n    const beta1 = .8;\n    const beta2 = .9;\n    const optimizer1 = tf.train.adam(learningRate, beta1, beta2);\n\n    const x = tf.tensor1d([2, 4]).variable();\n    const f: () => tf.Scalar = () => x.square().sum();\n    let cost = optimizer1.minimize(f, /* returnCost */ true);\n    expect(optimizer1.iterations).toEqual(1);\n    expectArraysClose(await cost.data(), 20);\n\n    const weights = await optimizer1.getWeights();\n    expect(weights.length).toEqual(3);\n    expect(weights[0].name).toEqual('iter');\n    expect(weights[1].name).toEqual(`${x.name}/m`);\n    expect(weights[2].name).toEqual(`${x.name}/v`);\n\n    const optimizer2 = tf.train.adam(learningRate, beta1, beta2);\n    await optimizer2.setWeights(weights);\n\n    cost = optimizer2.minimize(f, /* returnCost */ true);\n    expectArraysClose(await cost.data(), 18.82);\n    expect(optimizer2.iterations).toEqual(2);\n\n    const optimizer3 = tf.train.adam(learningRate, beta1, beta2);\n    await optimizer3.setWeights(await optimizer2.getWeights());\n    cost = optimizer2.minimize(f, /* returnCost */ true);\n    expectArraysClose(await cost.data(), 17.681284);\n    expect(optimizer3.iterations).toEqual(initialTensors + 2);\n  });\n\n  it('serialization round-trip', () => {\n    const originalOpt = tf.train.adam(0.1, 0.2, 0.3, 2e-8);\n    const reserialized =\n        tf.AdamOptimizer.fromConfig(tf.AdamOptimizer, originalOpt.getConfig());\n    expect(reserialized.getConfig()).toEqual(originalOpt.getConfig());\n  });\n});\n"]}