{"version":3,"file":"rmsprop_optimizer_test.js","sourceRoot":"","sources":["../../src/optimizers/rmsprop_optimizer_test.ts"],"names":[],"mappings":"AAAA;;;;;;;;;;;;;;;GAeG;AAEH,OAAO,KAAK,EAAE,MAAM,UAAU,CAAC;AAC/B,OAAO,EAAC,QAAQ,EAAE,iBAAiB,EAAC,MAAM,iBAAiB,CAAC;AAC5D,OAAO,EAAC,iBAAiB,EAAC,MAAM,cAAc,CAAC;AAE/C,iBAAiB,CAAC,kBAAkB,EAAE,QAAQ,EAAE,GAAG,EAAE;IACnD,EAAE,CAAC,OAAO,EAAE,KAAK,IAAI,EAAE;QACrB,MAAM,cAAc,GAAG,EAAE,CAAC,MAAM,EAAE,CAAC,UAAU,CAAC;QAC9C,MAAM,YAAY,GAAG,GAAG,CAAC;QACzB,MAAM,MAAM,GAAG,GAAG,CAAC;QACnB,MAAM,GAAG,GAAG,IAAI,CAAC;QACjB,MAAM,SAAS,GAAG,EAAE,CAAC,KAAK,CAAC,OAAO,CAAC,YAAY,EAAE,GAAG,EAAE,MAAM,CAAC,CAAC;QAE9D,MAAM,CAAC,GAAG,EAAE,CAAC,QAAQ,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,QAAQ,EAAE,CAAC;QAEzC,MAAM,CAAC,GAAG,GAAG,EAAE,CAAC,CAAC,CAAC,MAAM,EAAE,CAAC,GAAG,EAAE,CAAC;QAEjC,IAAI,UAAU,GAAG,EAAE,CAAC,MAAM,EAAE,CAAC,UAAU,CAAC;QAExC,IAAI,IAAI,GAAG,SAAS,CAAC,QAAQ,CAAC,CAAoB,EAAE,gBAAgB,CAAC,IAAI,CAAC,CAAC;QAE3E,8DAA8D;QAC9D,MAAM,CAAC,EAAE,CAAC,MAAM,EAAE,CAAC,UAAU,CAAC,CAAC,IAAI,CAAC,UAAU,GAAG,CAAC,CAAC,CAAC;QAEpD,iBAAiB;QACjB,6BAA6B;QAC7B,sEAAsE;QACtE,0DAA0D;QAC1D,sEAAsE;QACtE,yEAAyE;QACzE,4CAA4C;QAC5C,EAAE;QACF,iBAAiB;QACjB,iCAAiC;QACjC,sCAAsC;QACtC,8BAA8B;QAC9B,6CAA6C;QAC7C,yBAAyB;QACzB,iBAAiB,CAAC,MAAM,CAAC,CAAC,IAAI,EAAE,EAAE,CAAC,OAAO,EAAE,OAAO,CAAC,CAAC,CAAC;QAEtD,IAAI,CAAC,OAAO,EAAE,CAAC;QACf,UAAU,GAAG,EAAE,CAAC,MAAM,EAAE,CAAC,UAAU,CAAC;QAEpC,IAAI,GAAG,SAAS,CAAC,QAAQ,CAAC,CAAoB,EAAE,gBAAgB,CAAC,KAAK,CAAC,CAAC;QAExE,yBAAyB;QACzB,6BAA6B;QAC7B,qCAAqC;QACrC,oDAAoD;QACpD,0CAA0C;QAC1C,6CAA6C;QAC7C,yBAAyB;QAEzB,iCAAiC;QACjC,iBAAiB,CAAC,MAAM,CAAC,CAAC,IAAI,EAAE,EAAE,CAAC,OAAO,EAAE,QAAQ,CAAC,EAAE,IAAI,CAAC,CAAC;QAE7D,6CAA6C;QAC7C,MAAM,CAAC,EAAE,CAAC,MAAM,EAAE,CAAC,UAAU,CAAC,CAAC,IAAI,CAAC,UAAU,CAAC,CAAC;QAEhD,MAAM,CAAC,IAAI,CAAC,CAAC,IAAI,CAAC,IAAI,CAAC,CAAC;QAExB,CAAC,CAAC,OAAO,EAAE,CAAC;QACZ,SAAS,CAAC,OAAO,EAAE,CAAC;QACpB,sEAAsE;QACtE,MAAM,CAAC,EAAE,CAAC,MAAM,EAAE,CAAC,UAAU,CAAC,CAAC,IAAI,CAAC,cAAc,GAAG,CAAC,CAAC,CAAC;IAC1D,CAAC,CAAC,CAAC;IAEH,EAAE,CAAC,iCAAiC,EAAE,KAAK,IAAI,EAAE;QAC/C,MAAM,cAAc,GAAG,EAAE,CAAC,MAAM,EAAE,CAAC,UAAU,CAAC;QAC9C,MAAM,YAAY,GAAG,GAAG,CAAC;QACzB,MAAM,MAAM,GAAG,GAAG,CAAC;QACnB,MAAM,GAAG,GAAG,IAAI,CAAC;QACjB,MAAM,GAAG,GAAG,IAAI,CAAC;QACjB,MAAM,SAAS,GAAG,EAAE,CAAC,KAAK,CAAC,OAAO,CAAC,YAAY,EAAE,GAAG,EAAE,MAAM,EAAE,GAAG,EAAE,IAAI,CAAC,CAAC;QAEzE,MAAM,CAAC,GAAG,EAAE,CAAC,QAAQ,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,QAAQ,EAAE,CAAC;QAEzC,MAAM,CAAC,GAAG,GAAG,EAAE,CAAC,CAAC,CAAC,MAAM,EAAE,CAAC,GAAG,EAAE,CAAC;QAEjC,IAAI,UAAU,GAAG,EAAE,CAAC,MAAM,EAAE,CAAC,UAAU,CAAC;QAExC,IAAI,IAAI,GAAG,SAAS,CAAC,QAAQ,CAAC,CAAoB,EAAE,gBAAgB,CAAC,IAAI,CAAC,CAAC;QAE3E,8DAA8D;QAC9D,MAAM,CAAC,EAAE,CAAC,MAAM,EAAE,CAAC,UAAU,CAAC,CAAC,IAAI,CAAC,UAAU,GAAG,CAAC,CAAC,CAAC;QAEpD,iBAAiB;QACjB,6BAA6B;QAC7B,yEAAyE;QACzE,2BAA2B;QAC3B,qEAAqE;QACrE,0DAA0D;QAC1D,oEAAoE;QACpE,4CAA4C;QAC5C,+CAA+C;QAC/C,uDAAuD;QACvD,4CAA4C;QAC5C,EAAE;QACF,iBAAiB;QACjB,iCAAiC;QACjC,sCAAsC;QACtC,oCAAoC;QACpC,8BAA8B;QAC9B,8CAA8C;QAC9C,0BAA0B;QAC1B,iBAAiB,CAAC,MAAM,CAAC,CAAC,IAAI,EAAE,EAAE,CAAC,OAAO,EAAE,QAAQ,CAAC,CAAC,CAAC;QAEvD,IAAI,CAAC,OAAO,EAAE,CAAC;QACf,UAAU,GAAG,EAAE,CAAC,MAAM,EAAE,CAAC,UAAU,CAAC;QAEpC,IAAI,GAAG,SAAS,CAAC,QAAQ,CAAC,CAAoB,EAAE,gBAAgB,CAAC,KAAK,CAAC,CAAC;QAExE,0BAA0B;QAC1B,8BAA8B;QAC9B,qCAAqC;QACrC,iCAAiC;QACjC,kDAAkD;QAClD,iDAAiD;QACjD,2CAA2C;QAC3C,gDAAgD;QAChD,4BAA4B;QAE5B,iCAAiC;QACjC,iBAAiB,CAAC,MAAM,CAAC,CAAC,IAAI,EAAE,EAAE,CAAC,QAAQ,EAAE,SAAS,CAAC,EAAE,IAAI,CAAC,CAAC;QAE/D,6CAA6C;QAC7C,MAAM,CAAC,EAAE,CAAC,MAAM,EAAE,CAAC,UAAU,CAAC,CAAC,IAAI,CAAC,UAAU,CAAC,CAAC;QAEhD,MAAM,CAAC,IAAI,CAAC,CAAC,IAAI,CAAC,IAAI,CAAC,CAAC;QAExB,CAAC,CAAC,OAAO,EAAE,CAAC;QACZ,SAAS,CAAC,OAAO,EAAE,CAAC;QACpB,sEAAsE;QACtE,MAAM,CAAC,EAAE,CAAC,MAAM,EAAE,CAAC,UAAU,CAAC,CAAC,IAAI,CAAC,cAAc,GAAG,CAAC,CAAC,CAAC;IAC1D,CAAC,CAAC,CAAC;IAEH,EAAE,CAAC,yCAAyC,EAAE,KAAK,IAAI,EAAE;QACvD,MAAM,YAAY,GAAG,GAAG,CAAC;QACzB,MAAM,MAAM,GAAG,GAAG,CAAC;QACnB,MAAM,GAAG,GAAG,IAAI,CAAC;QACjB,MAAM,UAAU,GAAG,EAAE,CAAC,KAAK,CAAC,OAAO,CAAC,YAAY,EAAE,GAAG,EAAE,MAAM,CAAC,CAAC;QAE/D,MAAM,CAAC,GAAG,EAAE,CAAC,QAAQ,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,QAAQ,EAAE,CAAC;QACzC,MAAM,CAAC,GAAoB,GAAG,EAAE,CAAC,CAAC,CAAC,MAAM,EAAE,CAAC,GAAG,EAAE,CAAC;QAElD,IAAI,IAAI,GAAG,UAAU,CAAC,QAAQ,CAAC,CAAC,EAAE,gBAAgB,CAAC,IAAI,CAAC,CAAC;QACzD,iBAAiB,CAAC,MAAM,IAAI,CAAC,IAAI,EAAE,EAAE,CAAC,CAAC,CAAC;QACxC,iBAAiB,CAAC,MAAM,CAAC,CAAC,IAAI,EAAE,EAAE,CAAC,SAAS,EAAE,SAAS,CAAC,CAAC,CAAC;QAE1D,MAAM,OAAO,GAAG,MAAM,UAAU,CAAC,UAAU,EAAE,CAAC;QAC9C,2DAA2D;QAC3D,MAAM,CAAC,OAAO,CAAC,MAAM,CAAC,CAAC,OAAO,CAAC,CAAC,CAAC,CAAC;QAElC,MAAM,UAAU,GAAG,EAAE,CAAC,KAAK,CAAC,OAAO,CAAC,YAAY,EAAE,GAAG,EAAE,MAAM,CAAC,CAAC;QAC/D,MAAM,UAAU,CAAC,UAAU,CAAC,OAAO,CAAC,CAAC;QAErC,IAAI,GAAG,UAAU,CAAC,QAAQ,CAAC,CAAC,EAAE,gBAAgB,CAAC,IAAI,CAAC,CAAC;QACrD,iBAAiB,CAAC,MAAM,IAAI,CAAC,IAAI,EAAE,EAAE,SAAS,CAAC,CAAC;QAChD,iBAAiB,CAAC,MAAM,CAAC,CAAC,IAAI,EAAE,EAAE,CAAC,SAAS,EAAE,SAAS,CAAC,CAAC,CAAC;QAC1D,MAAM,CAAC,UAAU,CAAC,UAAU,CAAC,CAAC,OAAO,CAAC,CAAC,CAAC,CAAC;IAC3C,CAAC,CAAC,CAAC;IAEH,EAAE,CAAC,2DAA2D,EAAE,KAAK,IAAI,EAAE;QACzE,MAAM,YAAY,GAAG,GAAG,CAAC;QACzB,MAAM,MAAM,GAAG,GAAG,CAAC;QACnB,MAAM,GAAG,GAAG,IAAI,CAAC;QACjB,MAAM,OAAO,GAAW,SAAS,CAAC;QAClC,MAAM,QAAQ,GAAG,IAAI,CAAC;QACtB,MAAM,UAAU,GACZ,EAAE,CAAC,KAAK,CAAC,OAAO,CAAC,YAAY,EAAE,GAAG,EAAE,MAAM,EAAE,OAAO,EAAE,QAAQ,CAAC,CAAC;QAEnE,MAAM,CAAC,GAAG,EAAE,CAAC,QAAQ,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,QAAQ,EAAE,CAAC;QACzC,MAAM,CAAC,GAAG,GAAG,EAAE,CAAC,CAAC,CAAC,MAAM,EAAE,CAAC,GAAG,EAAE,CAAC;QAEjC,IAAI,IAAI,GAAG,UAAU,CAAC,QAAQ,CAAC,CAAoB,EAAE,gBAAgB,CAAC,IAAI,CAAC,CAAC;QAC5E,iBAAiB,CAAC,MAAM,IAAI,CAAC,IAAI,EAAE,EAAE,CAAC,CAAC,CAAC;QACxC,iBAAiB,CAAC,MAAM,CAAC,CAAC,IAAI,EAAE,EAAE,CAAC,SAAS,EAAE,SAAS,CAAC,CAAC,CAAC;QAE1D,MAAM,OAAO,GAAG,MAAM,UAAU,CAAC,UAAU,EAAE,CAAC;QAC9C,6DAA6D;QAC7D,MAAM,CAAC,OAAO,CAAC,MAAM,CAAC,CAAC,OAAO,CAAC,CAAC,CAAC,CAAC;QAElC,MAAM,UAAU,GACZ,EAAE,CAAC,KAAK,CAAC,OAAO,CAAC,YAAY,EAAE,GAAG,EAAE,MAAM,EAAE,OAAO,EAAE,QAAQ,CAAC,CAAC;QACnE,MAAM,UAAU,CAAC,UAAU,CAAC,OAAO,CAAC,CAAC;QAErC,IAAI,GAAG,UAAU,CAAC,QAAQ,CAAC,CAAoB,EAAE,gBAAgB,CAAC,IAAI,CAAC,CAAC;QACxE,iBAAiB,CAAC,MAAM,IAAI,CAAC,IAAI,EAAE,EAAE,QAAQ,CAAC,CAAC;QAC/C,iBAAiB,CAAC,MAAM,CAAC,CAAC,IAAI,EAAE,EAAE,CAAC,SAAS,EAAE,SAAS,CAAC,CAAC,CAAC;QAC1D,MAAM,CAAC,UAAU,CAAC,UAAU,CAAC,CAAC,OAAO,CAAC,CAAC,CAAC,CAAC;QAEzC,MAAM,UAAU,GACZ,EAAE,CAAC,KAAK,CAAC,OAAO,CAAC,YAAY,EAAE,GAAG,EAAE,MAAM,EAAE,OAAO,EAAE,QAAQ,CAAC,CAAC;QACnE,MAAM,UAAU,CAAC,UAAU,CAAC,MAAM,UAAU,CAAC,UAAU,EAAE,CAAC,CAAC;QAC3D,IAAI,GAAG,UAAU,CAAC,QAAQ,CAAC,CAAoB,EAAE,gBAAgB,CAAC,IAAI,CAAC,CAAC;QACxE,iBAAiB,CAAC,MAAM,IAAI,CAAC,IAAI,EAAE,EAAE,QAAQ,CAAC,CAAC;QAC/C,MAAM,CAAC,UAAU,CAAC,UAAU,CAAC,CAAC,OAAO,CAAC,CAAC,CAAC,CAAC;IAC3C,CAAC,CAAC,CAAC;IAEH,EAAE,CAAC,0BAA0B,EAAE,GAAG,EAAE;QAClC,MAAM,WAAW,GAAG,EAAE,CAAC,KAAK,CAAC,OAAO,CAAC,GAAG,EAAE,GAAG,EAAE,GAAG,EAAE,IAAI,EAAE,IAAI,CAAC,CAAC;QAChE,MAAM,YAAY,GAAG,EAAE,CAAC,gBAAgB,CAAC,UAAU,CAC/C,EAAE,CAAC,gBAAgB,EAAE,WAAW,CAAC,SAAS,EAAE,CAAC,CAAC;QAClD,MAAM,CAAC,YAAY,CAAC,SAAS,EAAE,CAAC,CAAC,OAAO,CAAC,WAAW,CAAC,SAAS,EAAE,CAAC,CAAC;IACpE,CAAC,CAAC,CAAC;IAEH,EAAE,CAAC,2BAA2B,EAAE,GAAG,EAAE;QACnC,MAAM,YAAY,GAAW,SAAS,CAAC;QACvC,MAAM,CAAC,GAAG,EAAE,CAAC,EAAE,CAAC,KAAK,CAAC,OAAO,CAAC,YAAY,CAAC,CAAC;aACvC,YAAY,CAAC,oDAAoD,CAAC,CAAC;IAC1E,CAAC,CAAC,CAAC;AACL,CAAC,CAAC,CAAC","sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport * as tf from '../index';\nimport {ALL_ENVS, describeWithFlags} from '../jasmine_util';\nimport {expectArraysClose} from '../test_util';\n\ndescribeWithFlags('RMSPropOptimizer', ALL_ENVS, () => {\n  it('basic', async () => {\n    const initialTensors = tf.memory().numTensors;\n    const learningRate = 0.1;\n    const moment = 0.1;\n    const rho = 0.95;\n    const optimizer = tf.train.rmsprop(learningRate, rho, moment);\n\n    const x = tf.tensor1d([1, 2]).variable();\n\n    const f = () => x.square().sum();\n\n    let numTensors = tf.memory().numTensors;\n\n    let cost = optimizer.minimize(f as () => tf.Scalar, /* returnCost */ true);\n\n    // Cost & 2 accumulators should be the only additional arrays.\n    expect(tf.memory().numTensors).toBe(numTensors + 3);\n\n    // epsilon = 1e-8\n    // newAccumulatedMeanSquare =\n    //          rho * accumulatedMeanSquare + (1 - rho) * grad ^ 2 = (0.2)\n    // newAccumulatedMoments = momentum * accumulatedMoments +\n    //          learning_rate * gradient / sqrt(newAccumulatedMeanSquare +\n    //          epsilon) = 0.1 * 0 + ((0.1 * 2) / sqrt(0.2 + 1e-8)) = 0.44721\n    // x -= learningRate * newAccumulatedMoments\n    //\n    // de/dx = [2, 4]\n    // accumulatedMeanSquare = [0, 0]\n    // newAccumulatedMeanSquare = [.2, .8]\n    // accumulatedMoments = [0, 0]\n    // newAccumulatedMoments = [0.44721, 0.44721]\n    // x = [0.55279, 1.55279]\n    expectArraysClose(await x.data(), [0.55279, 1.55279]);\n\n    cost.dispose();\n    numTensors = tf.memory().numTensors;\n\n    cost = optimizer.minimize(f as () => tf.Scalar, /* returnCost */ false);\n\n    // x = [0.55279, 1.55279]\n    // de/dx = [1.10558, 3.10558]\n    // accumulatedMeanSquare = [0.2, 0.8]\n    // newAccumulatedMeanSquare = [0.25105125, 1.242231]\n    // accumulatedMoments = [0.44721, 0.44721]\n    // newAccumulatedMoments = [0.26534, 0.32336]\n    // x = [0.28745, 1.22943]\n\n    // TODO: Fix numerical precision.\n    expectArraysClose(await x.data(), [0.28745, 1.222943], 1e-2);\n\n    // There should be no new additional Tensors.\n    expect(tf.memory().numTensors).toBe(numTensors);\n\n    expect(cost).toBe(null);\n\n    x.dispose();\n    optimizer.dispose();\n    // The only additional tensor remaining is the argument to variable().\n    expect(tf.memory().numTensors).toBe(initialTensors + 1);\n  });\n\n  it('gradient with centered momentum', async () => {\n    const initialTensors = tf.memory().numTensors;\n    const learningRate = 0.1;\n    const moment = 0.1;\n    const rho = 0.95;\n    const eps = 1e-8;\n    const optimizer = tf.train.rmsprop(learningRate, rho, moment, eps, true);\n\n    const x = tf.tensor1d([1, 2]).variable();\n\n    const f = () => x.square().sum();\n\n    let numTensors = tf.memory().numTensors;\n\n    let cost = optimizer.minimize(f as () => tf.Scalar, /* returnCost */ true);\n\n    // Cost & 3 accumulators should be the only additional arrays.\n    expect(tf.memory().numTensors).toBe(numTensors + 4);\n\n    // epsilon = 1e-8\n    // newAccumulatedMeanSquare =\n    //          rho * accumulatedMeanSquare + (1 - rho) * grad ^ 2 = [.2, .8]\n    // newAccumulatedMeanGrad =\n    //          rho * accumulatedMeanGrad + (1 - rho) * grad = [0.1, 0.2]\n    // newAccumulatedMoments = momentum * accumulatedMoments +\n    //          learning_rate * gradient / sqrt(newAccumulatedMeanSquare\n    //            - newAccumulatedMeanGrad * 2 +\n    //              epsilon) = 0.1 * 0 + ((0.1 * 2)\n    //                / sqrt(0.2 - 0.01 + 1e-8)) = 0.458831\n    // x -= learningRate * newAccumulatedMoments\n    //\n    // de/dx = [2, 4]\n    // accumulatedMeanSquare = [0, 0]\n    // newAccumulatedMeanSquare = [.2, .8]\n    // newAccumulatedMeanGrad = [.1, .2]\n    // accumulatedMoments = [0, 0]\n    // newAccumulatedMoments = [0.45883, 0.458831]\n    // x = [0.54117, 1.541169]\n    expectArraysClose(await x.data(), [0.54117, 1.541169]);\n\n    cost.dispose();\n    numTensors = tf.memory().numTensors;\n\n    cost = optimizer.minimize(f as () => tf.Scalar, /* returnCost */ false);\n\n    // x = [0.54117, 1.541169]\n    // de/dx = [1.08234, 3.082338]\n    // accumulatedMeanSquare = [0.2, 0.8]\n    // accumulatedMeanGrad = [.1, .2]\n    // newAccumulatedMeanSquare = [0.248572, 1.235040]\n    // newAccumulatedMeanGrad = [0.149117, 0.3441169]\n    // accumulatedMoments = [0.45883, 0.458831]\n    // newAccumulatedMoments = [0.273385, 0.3375766]\n    // x = [0.267785, 1.2035924]\n\n    // TODO: Fix numerical precision.\n    expectArraysClose(await x.data(), [0.267785, 1.2035924], 1e-2);\n\n    // There should be no new additional Tensors.\n    expect(tf.memory().numTensors).toBe(numTensors);\n\n    expect(cost).toBe(null);\n\n    x.dispose();\n    optimizer.dispose();\n    // The only additional tensor remaining is the argument to variable().\n    expect(tf.memory().numTensors).toBe(initialTensors + 1);\n  });\n\n  it('Save and load weights: centered = false', async () => {\n    const learningRate = 0.1;\n    const moment = 0.1;\n    const rho = 0.95;\n    const optimizer1 = tf.train.rmsprop(learningRate, rho, moment);\n\n    const x = tf.tensor1d([1, 2]).variable();\n    const f: () => tf.Scalar = () => x.square().sum();\n\n    let cost = optimizer1.minimize(f, /* returnCost */ true);\n    expectArraysClose(await cost.data(), 5);\n    expectArraysClose(await x.data(), [0.5527865, 1.5527864]);\n\n    const weights = await optimizer1.getWeights();\n    // An iteration variable and two optimizer state variables.\n    expect(weights.length).toEqual(3);\n\n    const optimizer2 = tf.train.rmsprop(learningRate, rho, moment);\n    await optimizer2.setWeights(weights);\n\n    cost = optimizer2.minimize(f, /* returnCost */ true);\n    expectArraysClose(await cost.data(), 2.7167187);\n    expectArraysClose(await x.data(), [0.2874418, 1.2294267]);\n    expect(optimizer2.iterations).toEqual(2);\n  });\n\n  it('Save, load weights and continue training: centered = true', async () => {\n    const learningRate = 0.1;\n    const moment = 0.1;\n    const rho = 0.95;\n    const epsilon: number = undefined;\n    const centered = true;\n    const optimizer1 =\n        tf.train.rmsprop(learningRate, rho, moment, epsilon, centered);\n\n    const x = tf.tensor1d([1, 2]).variable();\n    const f = () => x.square().sum();\n\n    let cost = optimizer1.minimize(f as () => tf.Scalar, /* returnCost */ true);\n    expectArraysClose(await cost.data(), 5);\n    expectArraysClose(await x.data(), [0.5411684, 1.5411685]);\n\n    const weights = await optimizer1.getWeights();\n    // An iteration variable and three optimizer state variables.\n    expect(weights.length).toEqual(4);\n\n    const optimizer2 =\n        tf.train.rmsprop(learningRate, rho, moment, epsilon, centered);\n    await optimizer2.setWeights(weights);\n\n    cost = optimizer2.minimize(f as () => tf.Scalar, /* returnCost */ true);\n    expectArraysClose(await cost.data(), 2.668063);\n    expectArraysClose(await x.data(), [0.2677834, 1.2035918]);\n    expect(optimizer2.iterations).toEqual(2);\n\n    const optimizer3 =\n        tf.train.rmsprop(learningRate, rho, moment, epsilon, centered);\n    await optimizer3.setWeights(await optimizer2.getWeights());\n    cost = optimizer3.minimize(f as () => tf.Scalar, /* returnCost */ true);\n    expectArraysClose(await cost.data(), 1.520341);\n    expect(optimizer3.iterations).toEqual(3);\n  });\n\n  it('serialization round-trip', () => {\n    const originalOpt = tf.train.rmsprop(0.1, 0.5, 0.1, 1e-7, true);\n    const reserialized = tf.RMSPropOptimizer.fromConfig(\n        tf.RMSPropOptimizer, originalOpt.getConfig());\n    expect(reserialized.getConfig()).toEqual(originalOpt.getConfig());\n  });\n\n  it('must define learning rate', () => {\n    const learningRate: number = undefined;\n    expect(() => tf.train.rmsprop(learningRate))\n        .toThrowError(/learningRate for RMSPropOptimizer must be defined./);\n  });\n});\n"]}