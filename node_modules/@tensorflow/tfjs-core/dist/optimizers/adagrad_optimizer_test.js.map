{"version":3,"file":"adagrad_optimizer_test.js","sourceRoot":"","sources":["../../src/optimizers/adagrad_optimizer_test.ts"],"names":[],"mappings":"AAAA;;;;;;;;;;;;;;;GAeG;AAEH,OAAO,KAAK,EAAE,MAAM,UAAU,CAAC;AAC/B,OAAO,EAAC,QAAQ,EAAE,iBAAiB,EAAC,MAAM,iBAAiB,CAAC;AAC5D,OAAO,EAAC,iBAAiB,EAAC,MAAM,cAAc,CAAC;AAE/C,iBAAiB,CAAC,kBAAkB,EAAE,QAAQ,EAAE,GAAG,EAAE;IACnD,EAAE,CAAC,OAAO,EAAE,KAAK,IAAI,EAAE;QACrB,MAAM,cAAc,GAAG,EAAE,CAAC,MAAM,EAAE,CAAC,UAAU,CAAC;QAC9C,MAAM,YAAY,GAAG,EAAE,CAAC;QACxB,MAAM,uBAAuB,GAAG,EAAE,CAAC;QACnC,MAAM,SAAS,GAAG,EAAE,CAAC,KAAK,CAAC,OAAO,CAAC,YAAY,EAAE,uBAAuB,CAAC,CAAC;QAE1E,MAAM,CAAC,GAAG,EAAE,CAAC,QAAQ,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,QAAQ,EAAE,CAAC;QAEzC,MAAM,CAAC,GAAoB,GAAG,EAAE,CAAC,CAAC,CAAC,MAAM,EAAE,CAAC,GAAG,EAAE,CAAC;QAElD,IAAI,UAAU,GAAG,EAAE,CAAC,MAAM,EAAE,CAAC,UAAU,CAAC;QAExC,IAAI,IAAI,GAAG,SAAS,CAAC,QAAQ,CAAC,CAAC,EAAE,gBAAgB,CAAC,IAAI,CAAC,CAAC;QAExD,2DAA2D;QAC3D,MAAM,CAAC,EAAE,CAAC,MAAM,EAAE,CAAC,UAAU,CAAC,CAAC,IAAI,CAAC,UAAU,GAAG,CAAC,CAAC,CAAC;QAEpD,iBAAiB;QACjB,gDAAgD;QAChD,8DAA8D;QAC9D,iBAAiB;QACjB,+BAA+B;QAC/B,mCAAmC;QACnC,kCAAkC;QAClC,iBAAiB,CAAC,MAAM,CAAC,CAAC,IAAI,EAAE,EAAE,CAAC,YAAY,EAAE,YAAY,CAAC,CAAC,CAAC;QAEhE,IAAI,CAAC,OAAO,EAAE,CAAC;QACf,UAAU,GAAG,EAAE,CAAC,MAAM,EAAE,CAAC,UAAU,CAAC;QAEpC,IAAI,GAAG,SAAS,CAAC,QAAQ,CAAC,CAAC,EAAE,gBAAgB,CAAC,KAAK,CAAC,CAAC;QAErD,sCAAsC;QACtC,gCAAgC;QAChC,qDAAqD;QACrD,oCAAoC;QAEpC,iCAAiC;QACjC,iBAAiB,CAAC,MAAM,CAAC,CAAC,IAAI,EAAE,EAAE,CAAC,YAAY,EAAE,aAAa,CAAC,EAAE,IAAI,CAAC,CAAC;QAEvE,6CAA6C;QAC7C,MAAM,CAAC,EAAE,CAAC,MAAM,EAAE,CAAC,UAAU,CAAC,CAAC,IAAI,CAAC,UAAU,CAAC,CAAC;QAEhD,MAAM,CAAC,IAAI,CAAC,CAAC,IAAI,CAAC,IAAI,CAAC,CAAC;QAExB,CAAC,CAAC,OAAO,EAAE,CAAC;QACZ,SAAS,CAAC,OAAO,EAAE,CAAC;QAEpB,sEAAsE;QACtE,MAAM,CAAC,EAAE,CAAC,MAAM,EAAE,CAAC,UAAU,CAAC,CAAC,IAAI,CAAC,cAAc,GAAG,CAAC,CAAC,CAAC;IAC1D,CAAC,CAAC,CAAC;IAEH,EAAE,CAAC,yCAAyC,EAAE,KAAK,IAAI,EAAE;QACvD,MAAM,YAAY,GAAG,EAAE,CAAC;QACxB,MAAM,uBAAuB,GAAG,EAAE,CAAC;QACnC,MAAM,UAAU,GAAG,EAAE,CAAC,KAAK,CAAC,OAAO,CAAC,YAAY,EAAE,uBAAuB,CAAC,CAAC;QAE3E,MAAM,CAAC,GAAG,EAAE,CAAC,QAAQ,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,QAAQ,EAAE,CAAC;QACzC,MAAM,CAAC,GAAoB,GAAG,EAAE,CAAC,CAAC,CAAC,MAAM,EAAE,CAAC,GAAG,EAAE,CAAC;QAClD,IAAI,IAAI,GAAG,UAAU,CAAC,QAAQ,CAAC,CAAC,EAAE,gBAAgB,CAAC,IAAI,CAAC,CAAC;QACzD,iBAAiB,CAAC,MAAM,IAAI,CAAC,IAAI,EAAE,EAAE,EAAE,CAAC,CAAC;QAEzC,MAAM,OAAO,GAAG,MAAM,UAAU,CAAC,UAAU,EAAE,CAAC;QAC9C,MAAM,CAAC,OAAO,CAAC,MAAM,CAAC,CAAC,OAAO,CAAC,CAAC,CAAC,CAAC;QAClC,MAAM,CAAC,OAAO,CAAC,CAAC,CAAC,CAAC,IAAI,CAAC,CAAC,OAAO,CAAC,MAAM,CAAC,CAAC;QACxC,MAAM,CAAC,OAAO,CAAC,CAAC,CAAC,CAAC,IAAI,CAAC,CAAC,OAAO,CAAC,GAAG,CAAC,CAAC,IAAI,cAAc,CAAC,CAAC;QAEzD,MAAM,UAAU,GAAG,EAAE,CAAC,KAAK,CAAC,IAAI,CAAC,YAAY,EAAE,uBAAuB,CAAC,CAAC;QACxE,MAAM,UAAU,CAAC,UAAU,CAAC,OAAO,CAAC,CAAC;QAErC,IAAI,GAAG,UAAU,CAAC,QAAQ,CAAC,CAAC,EAAE,gBAAgB,CAAC,IAAI,CAAC,CAAC;QACrD,iBAAiB,CAAC,MAAM,IAAI,CAAC,IAAI,EAAE,EAAE,QAAQ,CAAC,CAAC;QAC/C,MAAM,CAAC,UAAU,CAAC,UAAU,CAAC,CAAC,OAAO,CAAC,CAAC,CAAC,CAAC;IAC3C,CAAC,CAAC,CAAC;IAEH,EAAE,CAAC,0BAA0B,EAAE,GAAG,EAAE;QAClC,MAAM,WAAW,GAAG,EAAE,CAAC,KAAK,CAAC,OAAO,CAAC,GAAG,EAAE,GAAG,CAAC,CAAC;QAC/C,MAAM,YAAY,GAAG,EAAE,CAAC,gBAAgB,CAAC,UAAU,CAC/C,EAAE,CAAC,gBAAgB,EAAE,WAAW,CAAC,SAAS,EAAE,CAAC,CAAC;QAClD,MAAM,CAAC,YAAY,CAAC,SAAS,EAAE,CAAC,CAAC,OAAO,CAAC,WAAW,CAAC,SAAS,EAAE,CAAC,CAAC;IACpE,CAAC,CAAC,CAAC;AACL,CAAC,CAAC,CAAC","sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport * as tf from '../index';\nimport {ALL_ENVS, describeWithFlags} from '../jasmine_util';\nimport {expectArraysClose} from '../test_util';\n\ndescribeWithFlags('AdagradOptimizer', ALL_ENVS, () => {\n  it('basic', async () => {\n    const initialTensors = tf.memory().numTensors;\n    const learningRate = .1;\n    const initialAccumulatorValue = .1;\n    const optimizer = tf.train.adagrad(learningRate, initialAccumulatorValue);\n\n    const x = tf.tensor1d([1, 2]).variable();\n\n    const f: () => tf.Scalar = () => x.square().sum();\n\n    let numTensors = tf.memory().numTensors;\n\n    let cost = optimizer.minimize(f, /* returnCost */ true);\n\n    // Cost & accumulator should be the only additional arrays.\n    expect(tf.memory().numTensors).toBe(numTensors + 2);\n\n    // epsilon = 1-e8\n    // newAccumulatedGrad = accumulatedGrad + grad^2\n    // x -= (learningRate * grad) / sqrt(newAccumulatedGrad + eps)\n    // de/dx = [2, 4]\n    // accumulatedGrad = [0.1, 0.1]\n    // newAccumulatedGrad = [4.1, 16.1]\n    // x = [0.9012270405, 1.900311042]\n    expectArraysClose(await x.data(), [0.9012270405, 1.9003110428]);\n\n    cost.dispose();\n    numTensors = tf.memory().numTensors;\n\n    cost = optimizer.minimize(f, /* returnCost */ false);\n\n    // de/dx = [1.802454081, 3.9501555214]\n    // accumulatedGrad = [4.1, 16.1]\n    // newAccumulatedGrad = [7.3488407141, 31.7037286432]\n    // x = [0.8347372764, 1.83015597828]\n\n    // TODO: Fix numerical precision.\n    expectArraysClose(await x.data(), [0.8347372764, 1.83015597828], 1e-2);\n\n    // There should be no new additional Tensors.\n    expect(tf.memory().numTensors).toBe(numTensors);\n\n    expect(cost).toBe(null);\n\n    x.dispose();\n    optimizer.dispose();\n\n    // The only additional tensor remaining is the argument to variable().\n    expect(tf.memory().numTensors).toBe(initialTensors + 1);\n  });\n\n  it('Continue training after loading weights', async () => {\n    const learningRate = .1;\n    const initialAccumulatorValue = .1;\n    const optimizer1 = tf.train.adagrad(learningRate, initialAccumulatorValue);\n\n    const x = tf.tensor1d([2, 4]).variable();\n    const f: () => tf.Scalar = () => x.square().sum();\n    let cost = optimizer1.minimize(f, /* returnCost */ true);\n    expectArraysClose(await cost.data(), 20);\n\n    const weights = await optimizer1.getWeights();\n    expect(weights.length).toEqual(2);\n    expect(weights[0].name).toEqual('iter');\n    expect(weights[1].name).toEqual(`${x.name}/accumulator`);\n\n    const optimizer2 = tf.train.adam(learningRate, initialAccumulatorValue);\n    await optimizer2.setWeights(weights);\n\n    cost = optimizer2.minimize(f, /* returnCost */ true);\n    expectArraysClose(await cost.data(), 18.82179);\n    expect(optimizer2.iterations).toEqual(2);\n  });\n\n  it('serialization round-trip', () => {\n    const originalOpt = tf.train.adagrad(0.1, 0.2);\n    const reserialized = tf.AdagradOptimizer.fromConfig(\n        tf.AdagradOptimizer, originalOpt.getConfig());\n    expect(reserialized.getConfig()).toEqual(originalOpt.getConfig());\n  });\n});\n"]}