{"version":3,"file":"momentum_optimizer_test.js","sourceRoot":"","sources":["../../src/optimizers/momentum_optimizer_test.ts"],"names":[],"mappings":"AAAA;;;;;;;;;;;;;;;GAeG;AAEH,OAAO,KAAK,EAAE,MAAM,UAAU,CAAC;AAC/B,OAAO,EAAC,QAAQ,EAAE,iBAAiB,EAAC,MAAM,iBAAiB,CAAC;AAC5D,OAAO,EAAC,iBAAiB,EAAC,MAAM,cAAc,CAAC;AAE/C,iBAAiB,CAAC,mBAAmB,EAAE,QAAQ,EAAE,GAAG,EAAE;IACpD,EAAE,CAAC,OAAO,EAAE,KAAK,IAAI,EAAE;QACrB,MAAM,YAAY,GAAG,EAAE,CAAC;QACxB,MAAM,QAAQ,GAAG,EAAE,CAAC;QACpB,MAAM,SAAS,GAAG,EAAE,CAAC,KAAK,CAAC,QAAQ,CAAC,YAAY,EAAE,QAAQ,CAAC,CAAC;QAE5D,MAAM,CAAC,GAAG,EAAE,CAAC,QAAQ,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,QAAQ,EAAE,CAAC;QAEzC,MAAM,CAAC,GAAoB,GAAG,EAAE,CAAC,CAAC,CAAC,MAAM,EAAE,CAAC,GAAG,EAAE,CAAC;QAElD,IAAI,UAAU,GAAG,EAAE,CAAC,MAAM,EAAE,CAAC,UAAU,CAAC;QAExC,IAAI,IAAI,GAAG,SAAS,CAAC,QAAQ,CAAC,CAAC,EAAE,gBAAgB,CAAC,IAAI,CAAC,CAAC;QAExD,wDAAwD;QACxD,MAAM,CAAC,EAAE,CAAC,MAAM,EAAE,CAAC,UAAU,CAAC,CAAC,IAAI,CAAC,UAAU,GAAG,CAAC,CAAC,CAAC;QAEpD,uDAAuD;QACvD,4DAA4D;QAC5D,EAAE;QACF,iBAAiB;QACjB,2BAA2B;QAC3B,gBAAgB;QAChB,iBAAiB,CAAC,MAAM,CAAC,CAAC,IAAI,EAAE,EAAE,CAAC,EAAE,EAAE,GAAG,CAAC,CAAC,CAAC;QAE7C,IAAI,CAAC,OAAO,EAAE,CAAC;QACf,UAAU,GAAG,EAAE,CAAC,MAAM,EAAE,CAAC,UAAU,CAAC;QAEpC,IAAI,GAAG,SAAS,CAAC,QAAQ,CAAC,CAAC,EAAE,gBAAgB,CAAC,KAAK,CAAC,CAAC;QAErD,qBAAqB;QACrB,wBAAwB;QACxB,+BAA+B;QAC/B,mBAAmB;QACnB,iBAAiB,CAAC,MAAM,CAAC,CAAC,IAAI,EAAE,EAAE,CAAC,IAAI,EAAE,IAAI,CAAC,CAAC,CAAC;QAEhD,6CAA6C;QAC7C,MAAM,CAAC,EAAE,CAAC,MAAM,EAAE,CAAC,UAAU,CAAC,CAAC,IAAI,CAAC,UAAU,CAAC,CAAC;QAEhD,MAAM,CAAC,IAAI,CAAC,CAAC,IAAI,CAAC,IAAI,CAAC,CAAC;QAExB,CAAC,CAAC,OAAO,EAAE,CAAC;QACZ,UAAU,GAAG,EAAE,CAAC,MAAM,EAAE,CAAC,UAAU,CAAC;QACpC,SAAS,CAAC,OAAO,EAAE,CAAC;QAEpB,2EAA2E;QAC3E,2BAA2B;QAC3B,MAAM,CAAC,EAAE,CAAC,MAAM,EAAE,CAAC,UAAU,CAAC,CAAC,IAAI,CAAC,UAAU,GAAG,CAAC,CAAC,CAAC;IACtD,CAAC,CAAC,CAAC;IAEH,EAAE,CAAC,uBAAuB,EAAE,KAAK,IAAI,EAAE;QACrC,MAAM,YAAY,GAAG,EAAE,CAAC;QACxB,MAAM,QAAQ,GAAG,EAAE,CAAC;QACpB,MAAM,WAAW,GAAG,IAAI,CAAC;QACzB,MAAM,SAAS,GAAG,EAAE,CAAC,KAAK,CAAC,QAAQ,CAAC,YAAY,EAAE,QAAQ,EAAE,WAAW,CAAC,CAAC;QAEzE,MAAM,CAAC,GAAG,EAAE,CAAC,QAAQ,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,QAAQ,EAAE,CAAC;QAEzC,MAAM,CAAC,GAAoB,GAAG,EAAE,CAAC,CAAC,CAAC,MAAM,EAAE,CAAC,GAAG,EAAE,CAAC;QAElD,IAAI,UAAU,GAAG,EAAE,CAAC,MAAM,EAAE,CAAC,UAAU,CAAC;QAExC,IAAI,IAAI,GAAG,SAAS,CAAC,QAAQ,CAAC,CAAC,EAAE,gBAAgB,CAAC,IAAI,CAAC,CAAC;QAExD,0DAA0D;QAC1D,MAAM,CAAC,EAAE,CAAC,MAAM,EAAE,CAAC,UAAU,CAAC,CAAC,IAAI,CAAC,UAAU,GAAG,CAAC,CAAC,CAAC;QAEpD,uDAAuD;QACvD,0EAA0E;QAC1E,0BAA0B;QAC1B,EAAE;QACF,iBAAiB;QACjB,2BAA2B;QAC3B,wDAAwD;QACxD,gBAAgB;QAChB,iBAAiB,CAAC,MAAM,CAAC,CAAC,IAAI,EAAE,EAAE,CAAC,EAAE,EAAE,GAAG,CAAC,CAAC,CAAC;QAE7C,IAAI,CAAC,OAAO,EAAE,CAAC;QACf,UAAU,GAAG,EAAE,CAAC,MAAM,EAAE,CAAC,UAAU,CAAC;QAEpC,IAAI,GAAG,SAAS,CAAC,QAAQ,CAAC,CAAC,EAAE,gBAAgB,CAAC,KAAK,CAAC,CAAC;QAErD,qBAAqB;QACrB,wBAAwB;QACxB,gEAAgE;QAChE,oEAAoE;QACpE,mBAAmB;QACnB,iBAAiB,CAAC,MAAM,CAAC,CAAC,IAAI,EAAE,EAAE,CAAC,IAAI,EAAE,IAAI,CAAC,CAAC,CAAC;QAEhD,6CAA6C;QAC7C,MAAM,CAAC,EAAE,CAAC,MAAM,EAAE,CAAC,UAAU,CAAC,CAAC,IAAI,CAAC,UAAU,CAAC,CAAC;QAEhD,MAAM,CAAC,IAAI,CAAC,CAAC,IAAI,CAAC,IAAI,CAAC,CAAC;QAExB,CAAC,CAAC,OAAO,EAAE,CAAC;QACZ,UAAU,GAAG,EAAE,CAAC,MAAM,EAAE,CAAC,UAAU,CAAC;QACpC,SAAS,CAAC,OAAO,EAAE,CAAC;QAEpB,2EAA2E;QAC3E,2BAA2B;QAC3B,MAAM,CAAC,EAAE,CAAC,MAAM,EAAE,CAAC,UAAU,CAAC,CAAC,IAAI,CAAC,UAAU,GAAG,CAAC,CAAC,CAAC;IACtD,CAAC,CAAC,CAAC;IAEH,EAAE,CAAC,2CAA2C,EAAE,KAAK,IAAI,EAAE;QACzD,MAAM,YAAY,GAAG,EAAE,CAAC;QACxB,MAAM,QAAQ,GAAG,EAAE,CAAC;QACpB,MAAM,WAAW,GAAG,IAAI,CAAC;QACzB,MAAM,UAAU,GAAG,EAAE,CAAC,KAAK,CAAC,QAAQ,CAAC,YAAY,EAAE,QAAQ,EAAE,WAAW,CAAC,CAAC;QAE1E,MAAM,CAAC,GAAG,EAAE,CAAC,QAAQ,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,QAAQ,EAAE,CAAC;QACzC,MAAM,CAAC,GAAoB,GAAG,EAAE,CAAC,CAAC,CAAC,MAAM,EAAE,CAAC,GAAG,EAAE,CAAC;QAElD,IAAI,IAAI,GAAG,UAAU,CAAC,QAAQ,CAAC,CAAC,EAAE,gBAAgB,CAAC,IAAI,CAAC,CAAC;QAEzD,kEAAkE;QAClE,MAAM,UAAU,GAAG,EAAE,CAAC,KAAK,CAAC,QAAQ,CAAC,YAAY,EAAE,QAAQ,EAAE,WAAW,CAAC,CAAC;QAC1E,MAAM,UAAU,CAAC,UAAU,CAAC,MAAM,UAAU,CAAC,UAAU,EAAE,CAAC,CAAC;QAC3D,IAAI,GAAG,UAAU,CAAC,QAAQ,CAAC,CAAC,EAAE,gBAAgB,CAAC,IAAI,CAAC,CAAC;QACrD,iBAAiB,CAAC,MAAM,IAAI,CAAC,IAAI,EAAE,EAAE,IAAI,CAAC,CAAC;QAC3C,iBAAiB,CAAC,MAAM,CAAC,CAAC,IAAI,EAAE,EAAE,CAAC,IAAI,EAAE,IAAI,CAAC,CAAC,CAAC;QAChD,MAAM,CAAC,UAAU,CAAC,UAAU,CAAC,CAAC,OAAO,CAAC,CAAC,CAAC,CAAC;IAC3C,CAAC,CAAC,CAAC;IAEH,EAAE,CAAC,0BAA0B,EAAE,GAAG,EAAE;QAClC,MAAM,WAAW,GAAG,EAAE,CAAC,KAAK,CAAC,QAAQ,CAAC,GAAG,EAAE,GAAG,EAAE,IAAI,CAAC,CAAC;QACtD,MAAM,YAAY,GAAG,EAAE,CAAC,iBAAiB,CAAC,UAAU,CAChD,EAAE,CAAC,iBAAiB,EAAE,WAAW,CAAC,SAAS,EAAE,CAAC,CAAC;QACnD,MAAM,CAAC,YAAY,CAAC,SAAS,EAAE,CAAC,CAAC,OAAO,CAAC,WAAW,CAAC,SAAS,EAAE,CAAC,CAAC;IACpE,CAAC,CAAC,CAAC;AACL,CAAC,CAAC,CAAC","sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport * as tf from '../index';\nimport {ALL_ENVS, describeWithFlags} from '../jasmine_util';\nimport {expectArraysClose} from '../test_util';\n\ndescribeWithFlags('MomentumOptimizer', ALL_ENVS, () => {\n  it('basic', async () => {\n    const learningRate = .1;\n    const momentum = .5;\n    const optimizer = tf.train.momentum(learningRate, momentum);\n\n    const x = tf.tensor1d([1, 2]).variable();\n\n    const f: () => tf.Scalar = () => x.square().sum();\n\n    let numTensors = tf.memory().numTensors;\n\n    let cost = optimizer.minimize(f, /* returnCost */ true);\n\n    // Cost & velocity should be the only additional arrays.\n    expect(tf.memory().numTensors).toBe(numTensors + 2);\n\n    // newAccumulation = momentum * accumulation + gradient\n    // newVariable += -learningRate * newAccumulation + variable\n    //\n    // de/dx = [2, 4]\n    // newAccumulation = [2, 4]\n    // x = [.8, 1.6]\n    expectArraysClose(await x.data(), [.8, 1.6]);\n\n    cost.dispose();\n    numTensors = tf.memory().numTensors;\n\n    cost = optimizer.minimize(f, /* returnCost */ false);\n\n    // de/dx = [1.6, 3.2]\n    // accumulation = [2, 4]\n    // newAccumulation = [2.6, 5.2]\n    // x = [0.54, 1.08]\n    expectArraysClose(await x.data(), [0.54, 1.08]);\n\n    // There should be no new additional Tensors.\n    expect(tf.memory().numTensors).toBe(numTensors);\n\n    expect(cost).toBe(null);\n\n    x.dispose();\n    numTensors = tf.memory().numTensors;\n    optimizer.dispose();\n\n    // The optimizer.dispose() call should have disposed the m variable and the\n    // momentum variable for x.\n    expect(tf.memory().numTensors).toBe(numTensors - 2);\n  });\n\n  it('basic - with Nesterov', async () => {\n    const learningRate = .1;\n    const momentum = .5;\n    const useNesterov = true;\n    const optimizer = tf.train.momentum(learningRate, momentum, useNesterov);\n\n    const x = tf.tensor1d([1, 2]).variable();\n\n    const f: () => tf.Scalar = () => x.square().sum();\n\n    let numTensors = tf.memory().numTensors;\n\n    let cost = optimizer.minimize(f, /* returnCost */ true);\n\n    // Cost and velocity should be the only additional arrays.\n    expect(tf.memory().numTensors).toBe(numTensors + 2);\n\n    // newAccumulation = momentum * accumulation + gradient\n    // newVariable = -learningRate * (newAccumulation * momentum + gradient) +\n    //                variable\n    //\n    // de/dx = [2, 4]\n    // newAccumulation = [2, 4]\n    // newVariable = -0.1 * ([2, 4] * 0.5 + [2, 4]) + [1, 2]\n    // x = [.7, 1.4]\n    expectArraysClose(await x.data(), [.7, 1.4]);\n\n    cost.dispose();\n    numTensors = tf.memory().numTensors;\n\n    cost = optimizer.minimize(f, /* returnCost */ false);\n\n    // de/dx = [1.4, 2.8]\n    // accumulation = [2, 4]\n    // newAccumulation = [0.5 * 2 + 1.4, 0.5 * 4 + 2.8] = [2.4, 4.8]\n    // newVariable = -0.1 * ([2.4, 4.8] * 0.5 + [1.4, 2.8]) + [0.7, 1.4]\n    // x = [0.44, 0.88]\n    expectArraysClose(await x.data(), [0.44, 0.88]);\n\n    // There should be no new additional Tensors.\n    expect(tf.memory().numTensors).toBe(numTensors);\n\n    expect(cost).toBe(null);\n\n    x.dispose();\n    numTensors = tf.memory().numTensors;\n    optimizer.dispose();\n\n    // The optimizer.dispose() call should have disposed the m variable and the\n    // momentum variable for x.\n    expect(tf.memory().numTensors).toBe(numTensors - 2);\n  });\n\n  it('Save, load weights and conntinue training', async () => {\n    const learningRate = .1;\n    const momentum = .5;\n    const useNesterov = true;\n    const optimizer1 = tf.train.momentum(learningRate, momentum, useNesterov);\n\n    const x = tf.tensor1d([1, 2]).variable();\n    const f: () => tf.Scalar = () => x.square().sum();\n\n    let cost = optimizer1.minimize(f, /* returnCost */ true);\n\n    // The iterations counter and the accumulation for the variable x.\n    const optimizer2 = tf.train.momentum(learningRate, momentum, useNesterov);\n    await optimizer2.setWeights(await optimizer1.getWeights());\n    cost = optimizer2.minimize(f, /* returnCost */ true);\n    expectArraysClose(await cost.data(), 2.45);\n    expectArraysClose(await x.data(), [0.44, 0.88]);\n    expect(optimizer2.iterations).toEqual(2);\n  });\n\n  it('serialization round-trip', () => {\n    const originalOpt = tf.train.momentum(0.1, 0.2, true);\n    const reserialized = tf.MomentumOptimizer.fromConfig(\n        tf.MomentumOptimizer, originalOpt.getConfig());\n    expect(reserialized.getConfig()).toEqual(originalOpt.getConfig());\n  });\n});\n"]}