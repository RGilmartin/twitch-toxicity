{"version":3,"file":"optimizer_test.js","sourceRoot":"","sources":["../../src/optimizers/optimizer_test.ts"],"names":[],"mappings":"AAAA;;;;;;;;;;;;;;;GAeG;AAEH,OAAO,KAAK,EAAE,MAAM,UAAU,CAAC;AAC/B,OAAO,EAAC,QAAQ,EAAE,iBAAiB,EAAC,MAAM,iBAAiB,CAAC;AAE5D,OAAO,EAAC,iBAAiB,EAAC,MAAM,cAAc,CAAC;AAE/C,OAAO,EAAC,SAAS,EAAC,MAAM,aAAa,CAAC;AACtC,OAAO,EAAC,YAAY,EAAC,MAAM,iBAAiB,CAAC;AAE7C,iBAAiB,CAAC,WAAW,EAAE,QAAQ,EAAE,GAAG,EAAE;IAC5C,EAAE,CAAC,OAAO,EAAE,KAAK,IAAI,EAAE;QACrB,MAAM,cAAc,GAAG,EAAE,CAAC,MAAM,EAAE,CAAC,UAAU,CAAC;QAC9C,MAAM,YAAY,GAAG,EAAE,CAAC;QACxB,MAAM,SAAS,GAAG,EAAE,CAAC,KAAK,CAAC,GAAG,CAAC,YAAY,CAAC,CAAC;QAE7C,MAAM,CAAC,GAAG,EAAE,CAAC,MAAM,CAAC,CAAC,CAAC,CAAC,QAAQ,EAAE,CAAC;QAClC,MAAM,IAAI,GAAG,EAAE,CAAC,MAAM,CAAC,CAAC,CAAC,CAAC,QAAQ,EAAE,CAAC;QACrC,MAAM,aAAa,GAAG,EAAE,CAAC,MAAM,CAAC,CAAC,CAAC,CAAC,CAAC,QAAQ,EAAE,CAAC;QAE/C,IAAI,UAAU,GAAG,EAAE,CAAC,MAAM,EAAE,CAAC,UAAU,CAAC;QAExC,0DAA0D;QAC1D,MAAM,CAAC,GAAG,GAAG,EAAE,CAAC,CAAC,CAAC,MAAM,EAAE,CAAC,GAAG,CAAC,IAAI,CAAc,CAAC;QAElD,IAAI,IAAI,GAAG,SAAS,CAAC,QAAQ,CAAC,CAAC,EAAE,gBAAgB,CAAC,IAAI,CAAC,CAAC;QAExD,6CAA6C;QAC7C,MAAM,CAAC,EAAE,CAAC,MAAM,EAAE,CAAC,UAAU,CAAC,CAAC,IAAI,CAAC,UAAU,GAAG,CAAC,CAAC,CAAC;QAEpD,aAAa;QACb,MAAM,UAAU,GAAG,CAAC,CAAC,GAAG,CAAC,GAAG,YAAY,GAAG,CAAC,CAAC;QAC7C,YAAY;QACZ,MAAM,aAAa,GAAG,CAAC,CAAC,GAAG,YAAY,GAAG,CAAC,CAAC;QAC5C,iBAAiB,CAAC,MAAM,CAAC,CAAC,IAAI,EAAE,EAAE,CAAC,UAAU,CAAC,CAAC,CAAC;QAChD,iBAAiB,CAAC,MAAM,IAAI,CAAC,IAAI,EAAE,EAAE,CAAC,aAAa,CAAC,CAAC,CAAC;QACtD,iBAAiB,CAAC,MAAM,IAAI,CAAC,IAAI,EAAE,EAAE,CAAC,IAAI,CAAC,GAAG,CAAC,CAAC,EAAE,CAAC,CAAC,GAAG,CAAC,CAAC,CAAC,CAAC;QAC3D,8CAA8C;QAC9C,iBAAiB,CAAC,MAAM,aAAa,CAAC,IAAI,EAAE,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC;QAEpD,IAAI,CAAC,OAAO,EAAE,CAAC;QACf,UAAU,GAAG,EAAE,CAAC,MAAM,EAAE,CAAC,UAAU,CAAC;QAEpC,IAAI,GAAG,SAAS,CAAC,QAAQ,CAAC,CAAC,EAAE,gBAAgB,CAAC,KAAK,CAAC,CAAC;QACrD,6CAA6C;QAC7C,MAAM,CAAC,EAAE,CAAC,MAAM,EAAE,CAAC,UAAU,CAAC,CAAC,IAAI,CAAC,UAAU,CAAC,CAAC;QAEhD,MAAM,UAAU,GAAG,CAAC,CAAC,GAAG,UAAU,GAAG,YAAY,GAAG,UAAU,CAAC;QAC/D,MAAM,aAAa,GAAG,CAAC,YAAY,GAAG,aAAa,CAAC;QACpD,iBAAiB,CAAC,MAAM,CAAC,CAAC,IAAI,EAAE,EAAE,CAAC,UAAU,CAAC,CAAC,CAAC;QAChD,iBAAiB,CAAC,MAAM,IAAI,CAAC,IAAI,EAAE,EAAE,CAAC,aAAa,CAAC,CAAC,CAAC;QACtD,MAAM,CAAC,IAAI,CAAC,CAAC,IAAI,CAAC,IAAI,CAAC,CAAC;QACxB,8CAA8C;QAC9C,iBAAiB,CAAC,MAAM,aAAa,CAAC,IAAI,EAAE,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC;QAEpD,SAAS,CAAC,OAAO,EAAE,CAAC;QACpB,CAAC,CAAC,OAAO,EAAE,CAAC;QACZ,IAAI,CAAC,OAAO,EAAE,CAAC;QACf,aAAa,CAAC,OAAO,EAAE,CAAC;QACxB,yEAAyE;QACzE,MAAM,CAAC,EAAE,CAAC,MAAM,EAAE,CAAC,UAAU,CAAC,CAAC,IAAI,CAAC,cAAc,GAAG,CAAC,CAAC,CAAC;IAC1D,CAAC,CAAC,CAAC;IAEH,EAAE,CAAC,gCAAgC,EAAE,KAAK,IAAI,EAAE;QAC9C,MAAM,YAAY,GAAG,EAAE,CAAC;QACxB,MAAM,SAAS,GAAG,IAAI,YAAY,CAAC,YAAY,CAAC,CAAC;QAEjD,MAAM,CAAC,GAAG,EAAE,CAAC,MAAM,CAAC,CAAC,CAAC,CAAC,QAAQ,EAAE,CAAC;QAClC,MAAM,IAAI,GAAG,EAAE,CAAC,MAAM,CAAC,CAAC,CAAC,CAAC,QAAQ,EAAE,CAAC;QACrC,MAAM,aAAa,GAAG,EAAE,CAAC,MAAM,CAAC,CAAC,CAAC,CAAC,CAAC,QAAQ,EAAE,CAAC;QAC/C,MAAM,OAAO,GAAG,CAAC,CAAC,EAAE,IAAI,CAAC,CAAC;QAE1B,0DAA0D;QAC1D,MAAM,CAAC,GAAG,GAAG,EAAE,CAAC,CAAC,CAAC,MAAM,EAAE,CAAC,GAAG,CAAC,IAAI,CAAc,CAAC;QAElD,IAAI,IAAI,GAAG,SAAS,CAAC,QAAQ,CAAC,CAAC,EAAE,gBAAgB,CAAC,IAAI,EAAE,OAAO,CAAC,CAAC;QAEjE,aAAa;QACb,MAAM,UAAU,GAAG,CAAC,CAAC,GAAG,CAAC,GAAG,YAAY,GAAG,CAAC,CAAC;QAC7C,YAAY;QACZ,MAAM,aAAa,GAAG,CAAC,CAAC,GAAG,YAAY,GAAG,CAAC,CAAC;QAC5C,iBAAiB,CAAC,MAAM,CAAC,CAAC,IAAI,EAAE,EAAE,CAAC,UAAU,CAAC,CAAC,CAAC;QAChD,iBAAiB,CAAC,MAAM,IAAI,CAAC,IAAI,EAAE,EAAE,CAAC,aAAa,CAAC,CAAC,CAAC;QACtD,iBAAiB,CAAC,MAAM,IAAI,CAAC,IAAI,EAAE,EAAE,CAAC,IAAI,CAAC,GAAG,CAAC,CAAC,EAAE,CAAC,CAAC,GAAG,CAAC,CAAC,CAAC,CAAC;QAC3D,8CAA8C;QAC9C,iBAAiB,CAAC,MAAM,aAAa,CAAC,IAAI,EAAE,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC;QAEpD,IAAI,GAAG,SAAS,CAAC,QAAQ,CAAC,CAAC,EAAE,gBAAgB,CAAC,KAAK,EAAE,OAAO,CAAC,CAAC;QAE9D,MAAM,UAAU,GAAG,CAAC,CAAC,GAAG,UAAU,GAAG,YAAY,GAAG,UAAU,CAAC;QAC/D,MAAM,aAAa,GAAG,CAAC,YAAY,GAAG,aAAa,CAAC;QACpD,iBAAiB,CAAC,MAAM,CAAC,CAAC,IAAI,EAAE,EAAE,CAAC,UAAU,CAAC,CAAC,CAAC;QAChD,iBAAiB,CAAC,MAAM,IAAI,CAAC,IAAI,EAAE,EAAE,CAAC,aAAa,CAAC,CAAC,CAAC;QACtD,8CAA8C;QAC9C,iBAAiB,CAAC,MAAM,aAAa,CAAC,IAAI,EAAE,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC;QACpD,MAAM,CAAC,IAAI,CAAC,CAAC,IAAI,CAAC,IAAI,CAAC,CAAC;IAC1B,CAAC,CAAC,CAAC;IAEH,EAAE,CAAC,+CAA+C,EAAE,GAAG,EAAE;QACvD,MAAM,YAAY,GAAG,EAAE,CAAC;QACxB,MAAM,SAAS,GAAG,IAAI,YAAY,CAAC,YAAY,CAAC,CAAC;QAEjD,MAAM,CAAC,GAAG,EAAE,CAAC,MAAM,CAAC,CAAC,CAAC,CAAC,QAAQ,EAAE,CAAC;QAClC,MAAM,IAAI,GAAG,EAAE,CAAC,MAAM,CAAC,CAAC,CAAC,CAAC,QAAQ,EAAE,CAAC;QACrC,kBAAkB;QAClB,EAAE,CAAC,MAAM,CAAC,CAAC,CAAC,CAAC,CAAC,QAAQ,EAAE,CAAC;QACzB,MAAM,OAAO,GAAe,EAAE,CAAC;QAE/B,0DAA0D;QAC1D,MAAM,CAAC,GAAG,GAAG,EAAE,CAAC,CAAC,CAAC,MAAM,EAAE,CAAC,GAAG,CAAC,IAAI,CAAc,CAAC;QAElD,MAAM,CAAC,GAAG,EAAE,CAAC,SAAS,CAAC,QAAQ,CAAC,CAAC,EAAE,gBAAgB,CAAC,IAAI,EAAE,OAAO,CAAC,CAAC;aAC9D,YAAY,EAAE,CAAC;IACtB,CAAC,CAAC,CAAC;IAEH,EAAE,CAAC,oCAAoC,EAAE,KAAK,IAAI,EAAE;QAClD,MAAM,YAAY,GAAG,EAAE,CAAC;QACxB,MAAM,SAAS,GAAG,IAAI,YAAY,CAAC,YAAY,CAAC,CAAC;QAEjD,MAAM,CAAC,GAAG,EAAE,CAAC,MAAM,CAAC,CAAC,CAAC,CAAC,QAAQ,EAAE,CAAC;QAClC,MAAM,IAAI,GAAG,EAAE,CAAC,MAAM,CAAC,CAAC,CAAC,CAAC,QAAQ,EAAE,CAAC;QACrC,MAAM,aAAa,GAAG,EAAE,CAAC,MAAM,CAAC,CAAC,CAAC,CAAC,CAAC,QAAQ,EAAE,CAAC;QAC/C,MAAM,OAAO,GAAG,CAAC,CAAC,CAAC,CAAC;QAEpB,0DAA0D;QAC1D,MAAM,CAAC,GAAG,GAAG,EAAE,CAAC,CAAC,CAAC,MAAM,EAAE,CAAC,GAAG,CAAC,IAAI,CAAc,CAAC;QAElD,IAAI,IAAI,GAAG,SAAS,CAAC,QAAQ,CAAC,CAAC,EAAE,gBAAgB,CAAC,IAAI,EAAE,OAAO,CAAC,CAAC;QAEjE,aAAa;QACb,MAAM,cAAc,GAAG,CAAC,CAAC,GAAG,CAAC,GAAG,YAAY,GAAG,CAAC,CAAC;QACjD,iBAAiB,CAAC,MAAM,CAAC,CAAC,IAAI,EAAE,EAAE,CAAC,cAAc,CAAC,CAAC,CAAC;QACpD,gCAAgC;QAChC,iBAAiB,CAAC,MAAM,IAAI,CAAC,IAAI,EAAE,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC;QAC1C,iBAAiB,CAAC,MAAM,IAAI,CAAC,IAAI,EAAE,EAAE,CAAC,IAAI,CAAC,GAAG,CAAC,CAAC,EAAE,CAAC,CAAC,GAAG,CAAC,CAAC,CAAC,CAAC;QAC3D,8CAA8C;QAC9C,iBAAiB,CAAC,MAAM,aAAa,CAAC,IAAI,EAAE,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC;QAEpD,IAAI,GAAG,SAAS,CAAC,QAAQ,CAAC,CAAC,EAAE,gBAAgB,CAAC,KAAK,EAAE,OAAO,CAAC,CAAC;QAE9D,MAAM,cAAc,GAAG,CAAC,CAAC,GAAG,cAAc,GAAG,YAAY,GAAG,cAAc,CAAC;QAC3E,iBAAiB,CAAC,MAAM,CAAC,CAAC,IAAI,EAAE,EAAE,CAAC,cAAc,CAAC,CAAC,CAAC;QACpD,sCAAsC;QACtC,iBAAiB,CAAC,MAAM,IAAI,CAAC,IAAI,EAAE,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC;QAC1C,MAAM,CAAC,IAAI,CAAC,CAAC,IAAI,CAAC,IAAI,CAAC,CAAC;QACxB,8CAA8C;QAC9C,iBAAiB,CAAC,MAAM,aAAa,CAAC,IAAI,EAAE,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC;IACtD,CAAC,CAAC,CAAC;IAEH,EAAE,CAAC,qBAAqB,EAAE,KAAK,IAAI,EAAE;QACnC,MAAM,YAAY,GAAG,EAAE,CAAC;QACxB,MAAM,SAAS,GAAG,IAAI,YAAY,CAAC,YAAY,CAAC,CAAC;QAEjD,MAAM,SAAS,GAAG,KAAK,CAAC;QACxB,MAAM,CAAC,GAAG,EAAE,CAAC,MAAM,CAAC,CAAC,CAAC,CAAC,QAAQ,CAAC,SAAS,CAAC,CAAC;QAC3C,MAAM,IAAI,GAAG,EAAE,CAAC,MAAM,CAAC,CAAC,CAAC,CAAC,QAAQ,EAAE,CAAC;QACrC,MAAM,aAAa,GAAG,EAAE,CAAC,MAAM,CAAC,CAAC,CAAC,CAAC,CAAC,QAAQ,EAAE,CAAC;QAE/C,0DAA0D;QAC1D,MAAM,CAAC,GAAG,GAAG,EAAE,CAAC,CAAC,CAAC,MAAM,EAAE,CAAC,GAAG,CAAC,IAAI,CAAc,CAAC;QAElD,IAAI,IAAI,GAAG,SAAS,CAAC,QAAQ,CAAC,CAAC,EAAE,gBAAgB,CAAC,IAAI,CAAC,CAAC;QAExD,kCAAkC;QAClC,iBAAiB,CAAC,MAAM,CAAC,CAAC,IAAI,EAAE,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC;QACvC,YAAY;QACZ,MAAM,aAAa,GAAG,CAAC,CAAC,GAAG,YAAY,GAAG,CAAC,CAAC;QAC5C,iBAAiB,CAAC,MAAM,IAAI,CAAC,IAAI,EAAE,EAAE,CAAC,aAAa,CAAC,CAAC,CAAC;QACtD,iBAAiB,CAAC,MAAM,IAAI,CAAC,IAAI,EAAE,EAAE,CAAC,IAAI,CAAC,GAAG,CAAC,CAAC,EAAE,CAAC,CAAC,GAAG,CAAC,CAAC,CAAC,CAAC;QAC3D,8CAA8C;QAC9C,iBAAiB,CAAC,MAAM,aAAa,CAAC,IAAI,EAAE,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC;QAEpD,IAAI,GAAG,SAAS,CAAC,QAAQ,CAAC,CAAC,EAAE,gBAAgB,CAAC,KAAK,CAAC,CAAC;QAErD,kCAAkC;QAClC,iBAAiB,CAAC,MAAM,CAAC,CAAC,IAAI,EAAE,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC;QACvC,MAAM,aAAa,GAAG,CAAC,YAAY,GAAG,aAAa,CAAC;QACpD,iBAAiB,CAAC,MAAM,IAAI,CAAC,IAAI,EAAE,EAAE,CAAC,aAAa,CAAC,CAAC,CAAC;QACtD,MAAM,CAAC,IAAI,CAAC,CAAC,IAAI,CAAC,IAAI,CAAC,CAAC;QACxB,8CAA8C;QAC9C,iBAAiB,CAAC,MAAM,aAAa,CAAC,IAAI,EAAE,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC;IACtD,CAAC,CAAC,CAAC;IAEH,EAAE,CAAC,qDAAqD,EAAE,GAAG,EAAE;QAC7D,MAAM,YAAY,GAAG,EAAE,CAAC;QACxB,MAAM,SAAS,GAAG,IAAI,YAAY,CAAC,YAAY,CAAC,CAAC;QAEjD,MAAM,SAAS,GAAG,KAAK,CAAC;QACxB,MAAM,CAAC,GAAG,EAAE,CAAC,MAAM,CAAC,CAAC,CAAC,CAAC,QAAQ,CAAC,SAAS,CAAC,CAAC;QAC3C,MAAM,IAAI,GAAG,EAAE,CAAC,MAAM,CAAC,CAAC,CAAC,CAAC,QAAQ,EAAE,CAAC;QACrC,kBAAkB;QAClB,EAAE,CAAC,MAAM,CAAC,CAAC,CAAC,CAAC,CAAC,QAAQ,EAAE,CAAC;QACzB,MAAM,OAAO,GAAG,CAAC,CAAC,CAAC,CAAC;QAEpB,0DAA0D;QAC1D,MAAM,CAAC,GAAG,GAAG,EAAE,CAAC,CAAC,CAAC,MAAM,EAAE,CAAC,GAAG,CAAC,IAAI,CAAc,CAAC;QAElD,MAAM,CAAC,GAAG,EAAE,CAAC,SAAS,CAAC,QAAQ,CAAC,CAAC,EAAE,gBAAgB,CAAC,IAAI,EAAE,OAAO,CAAC,CAAC;aAC9D,YAAY,EAAE,CAAC;IACtB,CAAC,CAAC,CAAC;IAEH,EAAE,CAAC,sBAAsB,EAAE,GAAG,EAAE;QAC9B,MAAM,YAAY,GAAG,EAAE,CAAC;QACxB,MAAM,SAAS,GAAG,IAAI,YAAY,CAAC,YAAY,CAAC,CAAC;QAEjD,MAAM,CAAC,SAAS,YAAY,SAAS,CAAC,CAAC,IAAI,CAAC,IAAI,CAAC,CAAC;IACpD,CAAC,CAAC,CAAC;IAEH,EAAE,CAAC,0CAA0C,EAAE,GAAG,EAAE;QAClD,MAAM,YAAY,GAAG,EAAE,CAAC;QACxB,MAAM,SAAS,GAAG,IAAI,YAAY,CAAC,YAAY,CAAC,CAAC;QAEjD,MAAM,CAAC,GAAG,EAAE,CAAC,QAAQ,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,QAAQ,EAAE,CAAC;QACzC,MAAM,CAAC,GAAG,GAAG,EAAE,CAAC,CAAC,CAAC,MAAM,EAAE,CAAC;QAE3B,kCAAkC;QAClC,MAAM,CAAC,GAAG,EAAE,CAAC,SAAS,CAAC,QAAQ,CAAC,CAAQ,CAAC,CAAC,CAAC,YAAY,EAAE,CAAC;IAC5D,CAAC,CAAC,CAAC;AACL,CAAC,CAAC,CAAC","sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport * as tf from '../index';\nimport {ALL_ENVS, describeWithFlags} from '../jasmine_util';\nimport {Variable} from '../tensor';\nimport {expectArraysClose} from '../test_util';\n\nimport {Optimizer} from './optimizer';\nimport {SGDOptimizer} from './sgd_optimizer';\n\ndescribeWithFlags('optimizer', ALL_ENVS, () => {\n  it('basic', async () => {\n    const initialTensors = tf.memory().numTensors;\n    const learningRate = .1;\n    const optimizer = tf.train.sgd(learningRate);\n\n    const x = tf.scalar(4).variable();\n    const bias = tf.scalar(1).variable();\n    const strayVariable = tf.scalar(-1).variable();\n\n    let numTensors = tf.memory().numTensors;\n\n    // tslint:disable-next-line: no-unnecessary-type-assertion\n    const f = () => x.square().add(bias) as tf.Scalar;\n\n    let cost = optimizer.minimize(f, /* returnCost */ true);\n\n    // Cost should be the only additional arrays.\n    expect(tf.memory().numTensors).toBe(numTensors + 1);\n\n    // de/dx = 2x\n    const expectedX1 = -2 * 4 * learningRate + 4;\n    // de/db = 1\n    const expectedBias1 = -1 * learningRate + 1;\n    expectArraysClose(await x.data(), [expectedX1]);\n    expectArraysClose(await bias.data(), [expectedBias1]);\n    expectArraysClose(await cost.data(), [Math.pow(4, 2) + 1]);\n    // The stray variable should remain unchanged.\n    expectArraysClose(await strayVariable.data(), [-1]);\n\n    cost.dispose();\n    numTensors = tf.memory().numTensors;\n\n    cost = optimizer.minimize(f, /* returnCost */ false);\n    // There should be no new additional Tensors.\n    expect(tf.memory().numTensors).toBe(numTensors);\n\n    const expectedX2 = -2 * expectedX1 * learningRate + expectedX1;\n    const expectedBias2 = -learningRate + expectedBias1;\n    expectArraysClose(await x.data(), [expectedX2]);\n    expectArraysClose(await bias.data(), [expectedBias2]);\n    expect(cost).toBe(null);\n    // The stray variable should remain unchanged.\n    expectArraysClose(await strayVariable.data(), [-1]);\n\n    optimizer.dispose();\n    x.dispose();\n    bias.dispose();\n    strayVariable.dispose();\n    // The only additional tensors remaining are the arguments to variable().\n    expect(tf.memory().numTensors).toBe(initialTensors + 3);\n  });\n\n  it('varList array of all variables', async () => {\n    const learningRate = .1;\n    const optimizer = new SGDOptimizer(learningRate);\n\n    const x = tf.scalar(4).variable();\n    const bias = tf.scalar(1).variable();\n    const strayVariable = tf.scalar(-1).variable();\n    const varList = [x, bias];\n\n    // tslint:disable-next-line: no-unnecessary-type-assertion\n    const f = () => x.square().add(bias) as tf.Scalar;\n\n    let cost = optimizer.minimize(f, /* returnCost */ true, varList);\n\n    // de/dx = 2x\n    const expectedX1 = -2 * 4 * learningRate + 4;\n    // de/db = 1\n    const expectedBias1 = -1 * learningRate + 1;\n    expectArraysClose(await x.data(), [expectedX1]);\n    expectArraysClose(await bias.data(), [expectedBias1]);\n    expectArraysClose(await cost.data(), [Math.pow(4, 2) + 1]);\n    // The stray variable should remain unchanged.\n    expectArraysClose(await strayVariable.data(), [-1]);\n\n    cost = optimizer.minimize(f, /* returnCost */ false, varList);\n\n    const expectedX2 = -2 * expectedX1 * learningRate + expectedX1;\n    const expectedBias2 = -learningRate + expectedBias1;\n    expectArraysClose(await x.data(), [expectedX2]);\n    expectArraysClose(await bias.data(), [expectedBias2]);\n    // The stray variable should remain unchanged.\n    expectArraysClose(await strayVariable.data(), [-1]);\n    expect(cost).toBe(null);\n  });\n\n  it('varList empty array of variables throws error', () => {\n    const learningRate = .1;\n    const optimizer = new SGDOptimizer(learningRate);\n\n    const x = tf.scalar(4).variable();\n    const bias = tf.scalar(1).variable();\n    // Stray variable.\n    tf.scalar(-1).variable();\n    const varList: Variable[] = [];\n\n    // tslint:disable-next-line: no-unnecessary-type-assertion\n    const f = () => x.square().add(bias) as tf.Scalar;\n\n    expect(() => optimizer.minimize(f, /* returnCost */ true, varList))\n        .toThrowError();\n  });\n\n  it('varList subset of variables update', async () => {\n    const learningRate = .1;\n    const optimizer = new SGDOptimizer(learningRate);\n\n    const x = tf.scalar(4).variable();\n    const bias = tf.scalar(1).variable();\n    const strayVariable = tf.scalar(-1).variable();\n    const varList = [x];\n\n    // tslint:disable-next-line: no-unnecessary-type-assertion\n    const f = () => x.square().add(bias) as tf.Scalar;\n\n    let cost = optimizer.minimize(f, /* returnCost */ true, varList);\n\n    // de/dx = 2x\n    const expectedValue1 = -2 * 4 * learningRate + 4;\n    expectArraysClose(await x.data(), [expectedValue1]);\n    // bias should remain unchanged.\n    expectArraysClose(await bias.data(), [1]);\n    expectArraysClose(await cost.data(), [Math.pow(4, 2) + 1]);\n    // The stray variable should remain unchanged.\n    expectArraysClose(await strayVariable.data(), [-1]);\n\n    cost = optimizer.minimize(f, /* returnCost */ false, varList);\n\n    const expectedValue2 = -2 * expectedValue1 * learningRate + expectedValue1;\n    expectArraysClose(await x.data(), [expectedValue2]);\n    // Bias still should remain unchanged.\n    expectArraysClose(await bias.data(), [1]);\n    expect(cost).toBe(null);\n    // The stray variable should remain unchanged.\n    expectArraysClose(await strayVariable.data(), [-1]);\n  });\n\n  it('only bias trainable', async () => {\n    const learningRate = .1;\n    const optimizer = new SGDOptimizer(learningRate);\n\n    const trainable = false;\n    const x = tf.scalar(4).variable(trainable);\n    const bias = tf.scalar(1).variable();\n    const strayVariable = tf.scalar(-1).variable();\n\n    // tslint:disable-next-line: no-unnecessary-type-assertion\n    const f = () => x.square().add(bias) as tf.Scalar;\n\n    let cost = optimizer.minimize(f, /* returnCost */ true);\n\n    // x should not have been updated.\n    expectArraysClose(await x.data(), [4]);\n    // de/db = 1\n    const expectedBias1 = -1 * learningRate + 1;\n    expectArraysClose(await bias.data(), [expectedBias1]);\n    expectArraysClose(await cost.data(), [Math.pow(4, 2) + 1]);\n    // The stray variable should remain unchanged.\n    expectArraysClose(await strayVariable.data(), [-1]);\n\n    cost = optimizer.minimize(f, /* returnCost */ false);\n\n    // x should not have been updated.\n    expectArraysClose(await x.data(), [4]);\n    const expectedBias2 = -learningRate + expectedBias1;\n    expectArraysClose(await bias.data(), [expectedBias2]);\n    expect(cost).toBe(null);\n    // The stray variable should remain unchanged.\n    expectArraysClose(await strayVariable.data(), [-1]);\n  });\n\n  it('only bias trainable, only x in varList throws error', () => {\n    const learningRate = .1;\n    const optimizer = new SGDOptimizer(learningRate);\n\n    const trainable = false;\n    const x = tf.scalar(4).variable(trainable);\n    const bias = tf.scalar(1).variable();\n    // stray variable.\n    tf.scalar(-1).variable();\n    const varList = [x];\n\n    // tslint:disable-next-line: no-unnecessary-type-assertion\n    const f = () => x.square().add(bias) as tf.Scalar;\n\n    expect(() => optimizer.minimize(f, /* returnCost */ true, varList))\n        .toThrowError();\n  });\n\n  it('instanceof Optimizer', () => {\n    const learningRate = .1;\n    const optimizer = new SGDOptimizer(learningRate);\n\n    expect(optimizer instanceof Optimizer).toBe(true);\n  });\n\n  it('throws error when f returns a non-scalar', () => {\n    const learningRate = .1;\n    const optimizer = new SGDOptimizer(learningRate);\n\n    const x = tf.tensor1d([1, 2]).variable();\n    const f = () => x.square();\n\n    // tslint:disable-next-line:no-any\n    expect(() => optimizer.minimize(f as any)).toThrowError();\n  });\n});\n"]}