{"version":3,"file":"prelu.js","sourceRoot":"","sources":["../../src/ops/prelu.ts"],"names":[],"mappings":"AAAA;;;;;;;;;;;;;;;GAeG;AAEH,OAAO,EAAC,MAAM,EAAC,MAAM,WAAW,CAAC;AACjC,OAAO,EAAC,KAAK,EAAc,MAAM,iBAAiB,CAAC;AAGnD,OAAO,EAAC,eAAe,EAAC,MAAM,oBAAoB,CAAC;AAGnD,OAAO,EAAC,EAAE,EAAC,MAAM,aAAa,CAAC;AAE/B;;;;;;;;;;;;;;;GAeG;AACH,SAAS,MAAM,CAAmB,CAAe,EAAE,KAAmB;IACpE,MAAM,EAAE,GAAG,eAAe,CAAC,CAAC,EAAE,GAAG,EAAE,OAAO,CAAC,CAAC;IAC5C,MAAM,MAAM,GAAG,eAAe,CAAC,KAAK,EAAE,OAAO,EAAE,OAAO,CAAC,CAAC;IAExD,MAAM,MAAM,GAAgB,EAAC,CAAC,EAAE,EAAE,EAAE,KAAK,EAAE,MAAM,EAAC,CAAC;IACnD,OAAO,MAAM,CAAC,SAAS,CAAC,KAAK,EAAE,MAA8B,CAAC,CAAC;AACjE,CAAC;AAED,MAAM,CAAC,MAAM,KAAK,GAAG,EAAE,CAAC,EAAC,MAAM,EAAC,CAAC,CAAC","sourcesContent":["/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {ENGINE} from '../engine';\nimport {Prelu, PreluInputs} from '../kernel_names';\nimport {Tensor} from '../tensor';\nimport {NamedTensorMap} from '../tensor_types';\nimport {convertToTensor} from '../tensor_util_env';\nimport {TensorLike} from '../types';\n\nimport {op} from './operation';\n\n/**\n * Computes leaky rectified linear element-wise with parametric alphas.\n *\n * `x < 0 ? alpha * x : f(x) = x`\n *\n * ```js\n * const x = tf.tensor1d([-1, 2, -3, 4]);\n * const alpha = tf.scalar(0.1);\n *\n * x.prelu(alpha).print();  // or tf.prelu(x, alpha)\n * ```\n * @param x The input tensor.\n * @param alpha Scaling factor for negative values.\n *\n * @doc {heading: 'Operations', subheading: 'Basic math'}\n */\nfunction prelu_<T extends Tensor>(x: T|TensorLike, alpha: T|TensorLike): T {\n  const $x = convertToTensor(x, 'x', 'prelu');\n  const $alpha = convertToTensor(alpha, 'alpha', 'prelu');\n\n  const inputs: PreluInputs = {x: $x, alpha: $alpha};\n  return ENGINE.runKernel(Prelu, inputs as {} as NamedTensorMap);\n}\n\nexport const prelu = op({prelu_});\n"]}