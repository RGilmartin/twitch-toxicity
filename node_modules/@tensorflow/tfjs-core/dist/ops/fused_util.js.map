{"version":3,"file":"fused_util.js","sourceRoot":"","sources":["../../src/ops/fused_util.ts"],"names":[],"mappings":"AAAA;;;;;;;;;;;;;;;GAeG;AAIH,OAAO,KAAK,cAAc,MAAM,kBAAkB,CAAC;AACnD,OAAO,EAAC,GAAG,EAAC,MAAM,OAAO,CAAC;AAE1B,OAAO,EAAC,SAAS,EAAC,MAAM,cAAc,CAAC;AACvC,OAAO,EAAC,GAAG,EAAC,MAAM,OAAO,CAAC;AAC1B,OAAO,EAAC,KAAK,EAAC,MAAM,SAAS,CAAC;AAC9B,OAAO,EAAC,IAAI,EAAC,MAAM,QAAQ,CAAC;AAC5B,OAAO,EAAC,KAAK,EAAC,MAAM,SAAS,CAAC;AAC9B,OAAO,EAAC,OAAO,EAAC,MAAM,WAAW,CAAC;AAClC,OAAO,EAAC,OAAO,EAAC,MAAM,WAAW,CAAC;AAClC,OAAO,EAAC,IAAI,EAAC,MAAM,QAAQ,CAAC;AAC5B,OAAO,EAAC,GAAG,EAAC,MAAM,OAAO,CAAC;AAE1B,yCAAyC;AACzC,MAAM,UAAU,oBAAoB,CAChC,EAAU,EAAE,CAAS,EAAE,UAAsB;IAC/C,IAAI,UAAU,IAAI,IAAI,IAAI,UAAU,KAAK,QAAQ,EAAE;QACjD,OAAO,EAAE,CAAC;KACX;IACD,IAAI,UAAU,KAAK,MAAM,EAAE;QACzB,OAAO,GAAG,CAAC,EAAE,EAAE,IAAI,CAAC,CAAC,CAAC,CAAC,CAAC;KACzB;IACD,MAAM,IAAI,KAAK,CACX,gDAAgD,UAAU,GAAG,CAAC,CAAC;AACrE,CAAC;AAED,mCAAmC;AACnC,MAAM,UAAU,oBAAoB,CAChC,IAAY,EAAE,YAAoB;IACpC,IAAI,GAAG,GAAG,YAAY,CAAC;IACvB,MAAM,UAAU,GACZ,cAAc,CAAC,gBAAgB,CAAC,IAAI,CAAC,KAAK,EAAE,YAAY,CAAC,KAAK,CAAC,CAAC;IACpE,IAAI,UAAU,CAAC,MAAM,GAAG,CAAC,EAAE;QACzB,GAAG,GAAG,GAAG,CAAC,GAAG,EAAE,UAAU,CAAC,CAAC;KAC5B;IACD,OAAO,OAAO,CAAC,GAAG,EAAE,IAAI,CAAC,KAAK,CAAC,CAAC;AAClC,CAAC;AAED,MAAM,UAAU,eAAe,CAC3B,CAAS,EAAE,UAAsB,EAAE,sBAA+B,EAClE,cAAuB;IACzB,IAAI,UAAU,KAAK,QAAQ,EAAE;QAC3B,OAAO,CAAC,CAAC;KACV;SAAM,IAAI,UAAU,KAAK,MAAM,EAAE;QAChC,OAAO,IAAI,CAAC,CAAC,CAAC,CAAC;KAChB;SAAM,IAAI,UAAU,KAAK,KAAK,EAAE;QAC/B,OAAO,GAAG,CAAC,CAAC,CAAC,CAAC;KACf;SAAM,IAAI,UAAU,KAAK,OAAO,EAAE;QACjC,OAAO,KAAK,CAAC,CAAC,CAAC,CAAC;KACjB;SAAM,IAAI,UAAU,KAAK,OAAO,EAAE;QACjC,OAAO,KAAK,CAAC,CAAC,EAAE,sBAAsB,CAAC,CAAC;KACzC;SAAM,IAAI,UAAU,KAAK,WAAW,EAAE;QACrC,OAAO,SAAS,CAAC,CAAC,EAAE,cAAc,CAAC,CAAC;KACrC;SAAM,IAAI,UAAU,KAAK,SAAS,EAAE;QACnC,OAAO,OAAO,CAAC,CAAC,CAAC,CAAC;KACnB;IACD,MAAM,IAAI,KAAK,CAAC,4BAA4B,UAAU,GAAG,CAAC,CAAC;AAC7D,CAAC;AAED,oCAAoC;AACpC,MAAM,CAAC,MAAM,UAAU,GAAG,CAAC,aAAqB,EAAE,UAAsB,EAAE,EAAE;IAC1E,MAAM,YAAY,GAAG,aAAa,GAAG,CAAC,CAAC;IACvC,OAAO,CAAC,YAAY,IAAI,UAAU,KAAK,QAAQ,CAAC;AAClD,CAAC,CAAC","sourcesContent":["/**\n * @license\n * Copyright 2019 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {Tensor} from '../tensor';\n\nimport * as broadcast_util from './broadcast_util';\nimport {elu} from './elu';\nimport {Activation} from './fused_types';\nimport {leakyRelu} from './leaky_relu';\nimport {mul} from './mul';\nimport {prelu} from './prelu';\nimport {relu} from './relu';\nimport {relu6} from './relu6';\nimport {reshape} from './reshape';\nimport {sigmoid} from './sigmoid';\nimport {step} from './step';\nimport {sum} from './sum';\n\n// Returns gradient for fused activation.\nexport function getFusedDyActivation(\n    dy: Tensor, y: Tensor, activation: Activation): Tensor {\n  if (activation == null || activation === 'linear') {\n    return dy;\n  }\n  if (activation === 'relu') {\n    return mul(dy, step(y));\n  }\n  throw new Error(\n      `Cannot compute gradient for fused activation ${activation}.`);\n}\n\n// Returns gradient for fused bias.\nexport function getFusedBiasGradient(\n    bias: Tensor, dyActivation: Tensor): Tensor {\n  let res = dyActivation;\n  const reduceAxes =\n      broadcast_util.getReductionAxes(bias.shape, dyActivation.shape);\n  if (reduceAxes.length > 0) {\n    res = sum(res, reduceAxes);\n  }\n  return reshape(res, bias.shape);\n}\n\nexport function applyActivation(\n    x: Tensor, activation: Activation, preluActivationWeights?: Tensor,\n    leakyreluAlpha?: number): Tensor {\n  if (activation === 'linear') {\n    return x;\n  } else if (activation === 'relu') {\n    return relu(x);\n  } else if (activation === 'elu') {\n    return elu(x);\n  } else if (activation === 'relu6') {\n    return relu6(x);\n  } else if (activation === 'prelu') {\n    return prelu(x, preluActivationWeights);\n  } else if (activation === 'leakyrelu') {\n    return leakyRelu(x, leakyreluAlpha);\n  } else if (activation === 'sigmoid') {\n    return sigmoid(x);\n  }\n  throw new Error(`Unknown fused activation ${activation}.`);\n}\n\n// Whether we should call fused ops.\nexport const shouldFuse = (gradientDepth: number, activation: Activation) => {\n  const gradientMode = gradientDepth > 0;\n  return !gradientMode || activation === 'linear';\n};\n"]}