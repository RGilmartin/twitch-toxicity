{"version":3,"file":"log_softmax.js","sourceRoot":"","sources":["../../src/ops/log_softmax.ts"],"names":[],"mappings":"AAAA;;;;;;;;;;;;;;;GAeG;AAEH,OAAO,EAAC,UAAU,EAAC,MAAM,cAAc,CAAC;AAIxC,OAAO,EAAC,eAAe,EAAC,MAAM,oBAAoB,CAAC;AAGnD,OAAO,EAAC,IAAI,EAAC,MAAM,QAAQ,CAAC;AAC5B,OAAO,EAAC,GAAG,EAAC,MAAM,OAAO,CAAC;AAC1B,OAAO,EAAC,GAAG,EAAC,MAAM,OAAO,CAAC;AAC1B,OAAO,EAAC,GAAG,EAAC,MAAM,OAAO,CAAC;AAC1B,OAAO,EAAC,GAAG,EAAC,MAAM,OAAO,CAAC;AAC1B,OAAO,EAAC,EAAE,EAAC,MAAM,aAAa,CAAC;AAC/B,OAAO,EAAC,GAAG,EAAC,MAAM,OAAO,CAAC;AAC1B,OAAO,EAAC,GAAG,EAAC,MAAM,OAAO,CAAC;AAE1B;;;;;;;;;;;;;;;;;;;;GAoBG;AACH,SAAS,WAAW,CAAmB,MAAoB,EAAE,IAAI,GAAG,CAAC,CAAC;IACpE,MAAM,OAAO,GAAG,eAAe,CAAC,MAAM,EAAE,QAAQ,EAAE,YAAY,CAAC,CAAC;IAEhE,IAAI,IAAI,KAAK,CAAC,CAAC,EAAE;QACf,IAAI,GAAG,OAAO,CAAC,IAAI,GAAG,CAAC,CAAC;KACzB;IACD,IAAI,IAAI,KAAK,OAAO,CAAC,IAAI,GAAG,CAAC,EAAE;QAC7B,MAAM,KAAK,CACP,+DAA+D;YAC/D,mBAAmB,OAAO,CAAC,IAAI,iBAAiB,IAAI,EAAE,CAAC,CAAC;KAC7D;IAED,4DAA4D;IAC5D,2BAA2B;IAC3B,0CAA0C;IAC1C,uCAAuC;IACvC,kBAAkB;IAClB,kEAAkE;IAClE,qBAAqB;IACrB,mBAAmB;IACnB,kBAAkB;IAClB,KAAK;IAEL,iDAAiD;IACjD,MAAM,QAAQ,GAAG,UAAU,CAAC,CAAC,MAAc,EAAE,IAAkB,EAAE,EAAE;QACjE,MAAM,QAAQ,GAAG,IAAI,CAAC;QACtB,MAAM,IAAI,GAAG,GAAG,CAAC,MAAM,EAAE,IAAI,EAAE,IAAI,CAAC,CAAC;QACrC,MAAM,OAAO,GAAG,GAAG,CAAC,MAAM,EAAE,IAAI,CAAC,CAAC;QAClC,MAAM,KAAK,GACP,GAAG,CAAC,IAAI,CAAC,OAAO,EAAE,SAAS,CAAC,EAAE,GAAG,CAAC,GAAG,CAAC,GAAG,CAAC,OAAO,CAAC,EAAE,IAAI,EAAE,QAAQ,CAAC,CAAC,CAAC,CAAC;QAC1E,IAAI,CAAC,CAAC,KAAK,CAAC,CAAC,CAAC;QAEd,MAAM,QAAQ,GAAG,CAAC,EAAU,EAAE,KAAe,EAAE,EAAE;YAC/C,MAAM,CAAC,KAAK,CAAC,GAAG,KAAK,CAAC;YACtB,MAAM,QAAQ,GAAG,IAAI,CAAC;YACtB,MAAM,OAAO,GAAG,GAAG,CAAC,KAAK,CAAC,CAAC;YAC3B,OAAO,GAAG,CAAC,EAAE,EAAE,GAAG,CAAC,GAAG,CAAC,EAAE,EAAE,IAAI,EAAE,QAAQ,CAAC,EAAE,OAAO,CAAC,CAAC,CAAC;QACxD,CAAC,CAAC;QACF,OAAO,EAAC,KAAK,EAAE,QAAQ,EAAC,CAAC;IAC3B,CAAC,CAAC,CAAC;IAEH,OAAO,QAAQ,CAAC,OAAO,CAAM,CAAC;IAE9B,yEAAyE;IACzE,sDAAsD;IACtD,yCAAyC;IACzC,2BAA2B;IAC3B,yDAAyD;IACzD,2CAA2C;AAC7C,CAAC;AAED,MAAM,CAAC,MAAM,UAAU,GAAG,EAAE,CAAC,EAAC,WAAW,EAAC,CAAC,CAAC","sourcesContent":["/**\n * @license\n * Copyright 2020 Google Inc. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {customGrad} from '../gradients';\n\nimport {Tensor} from '../tensor';\nimport {GradSaveFunc} from '../tensor_types';\nimport {convertToTensor} from '../tensor_util_env';\nimport {TensorLike} from '../types';\n\nimport {cast} from './cast';\nimport {exp} from './exp';\nimport {log} from './log';\nimport {max} from './max';\nimport {mul} from './mul';\nimport {op} from './operation';\nimport {sub} from './sub';\nimport {sum} from './sum';\n\n/**\n * Computes the log softmax.\n *\n * ```js\n * const a = tf.tensor1d([1, 2, 3]);\n *\n * a.logSoftmax().print();  // or tf.logSoftmax(a)\n * ```\n *\n * ```js\n * const a = tf.tensor2d([2, 4, 6, 1, 2, 3], [2, 3]);\n *\n * a.logSoftmax().print();  // or tf.logSoftmax(a)\n * ```\n *\n * @param logits The logits array.\n * @param axis The dimension softmax would be performed on. Defaults to `-1`\n *     which indicates the last dimension.\n *\n * @doc {heading: 'Operations', subheading: 'Normalization'}\n */\nfunction logSoftmax_<T extends Tensor>(logits: T|TensorLike, axis = -1): T {\n  const $logits = convertToTensor(logits, 'logits', 'logSoftmax');\n\n  if (axis === -1) {\n    axis = $logits.rank - 1;\n  }\n  if (axis !== $logits.rank - 1) {\n    throw Error(\n        'Log Softmax along a non-last dimension is not yet supported. ' +\n        `Logits was rank ${$logits.rank} and axis was ${axis}`);\n  }\n\n  // const forward: ForwardFunc<Tensor> = (backend, save) => {\n  //   const keepDims = true;\n  //   const xMax = max(logits, axis, true);\n  //   const shifted = sub(logits, xMax);\n  //   const value =\n  //       sub(cast(shifted, 'float32'), log(sum(exp(shifted), axis,\n  //       keepDims)));\n  //   save([value]);\n  //   return value;\n  // };\n\n  // Use a custom gradient for numerical stability.\n  const customOp = customGrad((logits: Tensor, save: GradSaveFunc) => {\n    const keepDims = true;\n    const xMax = max(logits, axis, true);\n    const shifted = sub(logits, xMax);\n    const value =\n        sub(cast(shifted, 'float32'), log(sum(exp(shifted), axis, keepDims)));\n    save([value]);\n\n    const gradFunc = (dy: Tensor, saved: Tensor[]) => {\n      const [value] = saved;\n      const keepDims = true;\n      const softmax = exp(value);\n      return sub(dy, mul(sum(dy, axis, keepDims), softmax));\n    };\n    return {value, gradFunc};\n  });\n\n  return customOp($logits) as T;\n\n  // TODO Use Engine.runKernel when CPU/WebGL/WASM backends implement this.\n  // const inputs: LogSoftmaxInputs = {logits: $logits};\n  // const attrs: LogSoftmaxAttrs = {axis};\n  // return ENGINE.runKernel(\n  //            LogSoftmax, inputs as {} as NamedTensorMap,\n  //            attrs as {} as NamedAttrMap);\n}\n\nexport const logSoftmax = op({logSoftmax_});\n"]}