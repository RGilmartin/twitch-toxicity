{"version":3,"file":"LeakyRelu_grad.js","sourceRoot":"","sources":["../../src/gradients/LeakyRelu_grad.ts"],"names":[],"mappings":"AAAA;;;;;;;;;;;;;;;GAeG;AACH,OAAO,EAAC,SAAS,EAAiB,MAAM,iBAAiB,CAAC;AAE1D,OAAO,EAAC,OAAO,EAAC,MAAM,gBAAgB,CAAC;AACvC,OAAO,EAAC,GAAG,EAAC,MAAM,YAAY,CAAC;AAC/B,OAAO,EAAC,KAAK,EAAC,MAAM,cAAc,CAAC;AAGnC,MAAM,CAAC,MAAM,mBAAmB,GAAe;IAC7C,UAAU,EAAE,SAAS;IACrB,YAAY,EAAE,CAAC,GAAG,CAAC;IACnB,QAAQ,EAAE,CAAC,EAAU,EAAE,KAAe,EAAE,KAAmB,EAAE,EAAE;QAC7D,MAAM,CAAC,CAAC,CAAC,GAAG,KAAK,CAAC;QAClB,MAAM,EAAC,KAAK,EAAC,GAAG,KAA6B,CAAC;QAC9C,MAAM,IAAI,GAAG,OAAO,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC;QAE3B,yEAAyE;QACzE,OAAO;QACP,OAAO,EAAC,CAAC,EAAE,GAAG,EAAE,CAAC,KAAK,CAAC,IAAI,EAAE,EAAE,EAAE,GAAG,CAAC,EAAE,EAAE,KAAK,CAAC,CAAC,EAAC,CAAC;IACpD,CAAC;CACF,CAAC","sourcesContent":["/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport {LeakyRelu, LeakyReluAttrs} from '../kernel_names';\nimport {GradConfig, NamedAttrMap} from '../kernel_registry';\nimport {greater} from '../ops/greater';\nimport {mul} from '../ops/mul';\nimport {where} from '../ops/where';\nimport {Tensor} from '../tensor';\n\nexport const leakyReluGradConfig: GradConfig = {\n  kernelName: LeakyRelu,\n  inputsToSave: ['x'],\n  gradFunc: (dy: Tensor, saved: Tensor[], attrs: NamedAttrMap) => {\n    const [x] = saved;\n    const {alpha} = attrs as {} as LeakyReluAttrs;\n    const mask = greater(x, 0);\n\n    // Returns `gradients * (features > 0) + alpha * gradients * (features <=\n    // 0)`.\n    return {x: () => where(mask, dy, mul(dy, alpha))};\n  }\n};\n"]}