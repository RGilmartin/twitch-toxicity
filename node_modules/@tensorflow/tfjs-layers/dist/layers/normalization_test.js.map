{"version":3,"file":"normalization_test.js","sourceRoot":"","sources":["../../src/layers/normalization_test.ts"],"names":[],"mappings":"AAAA;;;;;;;;GAQG;AAEH;;GAEG;AAEH,OAAO,EAAC,OAAO,EAAE,MAAM,EAAE,QAAQ,EAAE,MAAM,EAAU,QAAQ,EAAE,QAAQ,EAAE,QAAQ,EAAE,QAAQ,EAAE,SAAS,EAAE,KAAK,EAAE,KAAK,EAAE,SAAS,EAAC,MAAM,uBAAuB,CAAC;AAE5J,OAAO,EAAC,cAAc,EAAC,MAAM,oBAAoB,CAAC;AAClD,OAAO,KAAK,GAAG,MAAM,UAAU,CAAC;AAChC,OAAO,EAAC,mBAAmB,EAAE,mBAAmB,EAAC,MAAM,8BAA8B,CAAC;AACtF,OAAO,EAAC,eAAe,EAAE,qBAAqB,EAAE,kBAAkB,EAAC,MAAM,qBAAqB,CAAC;AAE/F,OAAO,EAAC,kBAAkB,EAAE,wBAAwB,EAAC,MAAM,iBAAiB,CAAC;AAE7E,qBAAqB,CAAC,0BAA0B,EAAE,GAAG,EAAE;IACrD,4EAA4E;IAC5E,oBAAoB;IACpB,YAAY;IACZ,eAAe;IACf,qBAAqB;IACrB,0BAA0B;IAC1B,EAAE;IACF,6BAA6B;IAC7B,8BAA8B;IAC9B,0EAA0E;IAC1E,kEAAkE;IAClE,iEAAiE;IACjE,yBAAyB;IACzB,wEAAwE;IACxE,wCAAwC;IACxC,kBAAkB;IAClB,gBAAgB;IAChB,oBAAoB;IACpB,MAAM;IAEN,EAAE,CAAC,qBAAqB,EAAE,GAAG,EAAE;QAC7B,MAAM,CAAC,GAAG,QAAQ,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,EAAE,EAAE,EAAE,EAAE,EAAE,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;QAC1E,MAAM,KAAK,GAAG,QAAQ,CAAC,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;QACrC,MAAM,IAAI,GAAG,QAAQ,CAAC,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;QACpC,MAAM,aAAa,GAAG,CAAC,CAAC,CAAC,CAAC;QAC1B,MAAM,CAAC,MAAM,EAAE,IAAI,EAAE,QAAQ,CAAC,GAC1B,wBAAwB,CAAC,CAAC,EAAE,KAAK,EAAE,IAAI,EAAE,aAAa,CAAC,CAAC;QAC5D,kBAAkB,CACd,MAAM,EACN,QAAQ,CACJ;YACE,CAAC,CAAC,QAAQ,EAAE,CAAC,SAAS,EAAE,CAAC,SAAS,EAAE,CAAC,SAAS,CAAC;YAC/C,CAAC,CAAC,SAAS,EAAE,CAAC,SAAS,EAAE,CAAC,UAAU,EAAE,UAAU,CAAC;YACjD,CAAC,SAAS,EAAE,SAAS,EAAE,SAAS,EAAE,UAAU,CAAC;SAC9C,EACD,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC;QACjB,kBAAkB,CAAC,IAAI,EAAE,QAAQ,CAAC,CAAC,GAAG,EAAE,SAAS,EAAE,SAAS,EAAE,GAAG,CAAC,CAAC,CAAC,CAAC;QACrE,kBAAkB,CACd,QAAQ,EAAE,QAAQ,CAAC,CAAC,SAAS,EAAE,SAAS,EAAE,QAAQ,EAAE,SAAS,CAAC,CAAC,CAAC,CAAC;IACvE,CAAC,CAAC,CAAC;IAEH,EAAE,CAAC,qBAAqB,EAAE,GAAG,EAAE;QAC7B,MAAM,CAAC,GAAG,QAAQ,CACd,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,EAAE,EAAE,CAAC,EAAE,CAAC,EAAE,EAAE,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;QAC1E,MAAM,KAAK,GAAG,QAAQ,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;QAC/B,MAAM,IAAI,GAAG,QAAQ,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;QAC9B,MAAM,aAAa,GAAG,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC;QAC7B,MAAM,CAAC,MAAM,EAAE,IAAI,EAAE,QAAQ,CAAC,GAC1B,wBAAwB,CAAC,CAAC,EAAE,KAAK,EAAE,IAAI,EAAE,aAAa,CAAC,CAAC;QAC5D,kBAAkB,CACd,MAAM,EACN,QAAQ,CACJ;YACE,CAAC,CAAC,CAAC,SAAS,EAAE,CAAC,SAAS,CAAC,EAAE,CAAC,CAAC,SAAS,EAAE,CAAC,SAAS,CAAC,CAAC;YACpD,CAAC,CAAC,CAAC,SAAS,EAAE,CAAC,SAAS,CAAC,EAAE,CAAC,UAAU,EAAE,SAAS,CAAC,CAAC;YACnD,CAAC,CAAC,SAAS,EAAE,SAAS,CAAC,EAAE,CAAC,SAAS,EAAE,SAAS,CAAC,CAAC;SACjD,EACD,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC;QACpB,kBAAkB,CAAC,IAAI,EAAE,QAAQ,CAAC,CAAC,SAAS,EAAE,SAAS,CAAC,CAAC,CAAC,CAAC;QAC3D,kBAAkB,CAAC,QAAQ,EAAE,QAAQ,CAAC,CAAC,QAAQ,EAAE,SAAS,CAAC,CAAC,CAAC,CAAC;IAChE,CAAC,CAAC,CAAC;IAEH,EAAE,CAAC,kBAAkB,EAAE,GAAG,EAAE;QAC1B,MAAM,CAAC,GAAG,QAAQ,CACd,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,EAAE,EAAE,CAAC,EAAE,CAAC,EAAE,EAAE,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;QAC1E,MAAM,KAAK,GAAG,QAAQ,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;QACjD,MAAM,IAAI,GAAG,QAAQ,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;QAChD,MAAM,aAAa,GAAG,CAAC,CAAC,CAAC,CAAC;QAC1B,MAAM,CAAC,MAAM,EAAE,IAAI,EAAE,QAAQ,CAAC,GAC1B,wBAAwB,CAAC,CAAC,EAAE,KAAK,EAAE,IAAI,EAAE,aAAa,CAAC,CAAC;QAC5D,kBAAkB,CACd,MAAM,EACN,QAAQ,CACJ;YACE,CAAC,CAAC,CAAC,QAAQ,EAAE,CAAC,SAAS,CAAC,EAAE,CAAC,CAAC,SAAS,EAAE,CAAC,SAAS,CAAC,CAAC;YACnD,CAAC,CAAC,CAAC,SAAS,EAAE,CAAC,SAAS,CAAC,EAAE,CAAC,CAAC,UAAU,EAAE,UAAU,CAAC,CAAC;YACrD,CAAC,CAAC,SAAS,EAAE,SAAS,CAAC,EAAE,CAAC,SAAS,EAAE,UAAU,CAAC,CAAC;SAClD,EACD,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC;QACpB,kBAAkB,CACd,IAAI,EAAE,QAAQ,CAAC,CAAC,CAAC,CAAC,EAAE,SAAS,CAAC,EAAE,CAAC,SAAS,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC;QAC9D,kBAAkB,CACd,QAAQ,EACR,QAAQ,CAAC,CAAC,CAAC,SAAS,EAAE,SAAS,CAAC,EAAE,CAAC,QAAQ,EAAE,SAAS,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC;IACzE,CAAC,CAAC,CAAC;IAEH,EAAE,CAAC,kBAAkB,EAAE,GAAG,EAAE;QAC1B,MAAM,CAAC,GAAG,QAAQ,CACd,CAAC,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,EAAE,EAAE,CAAC,EAAE,CAAC,EAAE,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC,EAC3D,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;QAClB,MAAM,KAAK,GAAG,QAAQ,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;QACjD,MAAM,IAAI,GAAG,QAAQ,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;QAChD,MAAM,aAAa,GAAG,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC;QAC7B,MAAM,CAAC,MAAM,EAAE,IAAI,EAAE,QAAQ,CAAC,GAC1B,wBAAwB,CAAC,CAAC,EAAE,KAAK,EAAE,IAAI,EAAE,aAAa,CAAC,CAAC;QAC5D,kBAAkB,CACd,MAAM,EACN,QAAQ,CACJ,CAAC;gBACC,CAAC,CAAC,CAAC,QAAQ,EAAE,CAAC,SAAS,CAAC,EAAE,CAAC,CAAC,SAAS,EAAE,CAAC,SAAS,CAAC,CAAC;gBACnD,CAAC,CAAC,CAAC,SAAS,EAAE,CAAC,SAAS,CAAC,EAAE,CAAC,CAAC,UAAU,EAAE,UAAU,CAAC,CAAC;gBACrD,CAAC,CAAC,SAAS,EAAE,SAAS,CAAC,EAAE,CAAC,SAAS,EAAE,UAAU,CAAC,CAAC;aAClD,CAAC,EACF,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC;QACvB,kBAAkB,CACd,IAAI,EAAE,QAAQ,CAAC,CAAC,CAAC,CAAC,EAAE,SAAS,CAAC,EAAE,CAAC,SAAS,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC;QAC9D,kBAAkB,CACd,QAAQ,EACR,QAAQ,CAAC,CAAC,CAAC,SAAS,EAAE,SAAS,CAAC,EAAE,CAAC,QAAQ,EAAE,SAAS,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC;IACzE,CAAC,CAAC,CAAC;AACL,CAAC,CAAC,CAAC;AAEH,qBAAqB,CAAC,oBAAoB,EAAE,GAAG,EAAE;IAC/C,EAAE,CAAC,qCAAqC,EAAE,GAAG,EAAE;QAC7C,MAAM,CAAC,GAAG,QAAQ,CAAC,CAAC,CAAC,EAAE,EAAE,EAAE,CAAC,EAAE,CAAC,EAAE,EAAE,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;QACjD,MAAM,IAAI,GAAG,QAAQ,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;QAChD,MAAM,QAAQ,GAAG,QAAQ,CAAC,CAAC,CAAC,CAAC,EAAE,EAAE,CAAC,EAAE,CAAC,CAAC,EAAE,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;QACtD,kBAAkB,CACd,kBAAkB,CAAC,CAAC,EAAE,IAAI,EAAE,QAAQ,EAAE,IAAI,EAAE,IAAI,EAAE,CAAC,CAAC,EACpD,QAAQ,CAAC,CAAC,CAAC,GAAG,EAAE,IAAI,CAAC,EAAE,CAAC,IAAI,EAAE,IAAI,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC;IACrD,CAAC,CAAC,CAAC;IACH,EAAE,CAAC,qDAAqD,EAAE,GAAG,EAAE;QAC7D,MAAM,CAAC,GAAG,QAAQ,CAAC,CAAC,CAAC,EAAE,EAAE,EAAE,CAAC,EAAE,CAAC,EAAE,EAAE,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;QACjD,MAAM,IAAI,GAAG,QAAQ,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;QAChD,MAAM,QAAQ,GAAG,QAAQ,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;QACpD,kBAAkB,CACd,kBAAkB,CAAC,CAAC,EAAE,IAAI,EAAE,QAAQ,EAAE,IAAI,EAAE,IAAI,EAAE,CAAC,CAAC,EACpD,QAAQ,CAAC,CAAC,CAAC,EAAE,EAAE,EAAE,CAAC,EAAE,CAAC,EAAE,EAAE,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC;IAC9C,CAAC,CAAC,CAAC;IACH,EAAE,CAAC,kCAAkC,EAAE,GAAG,EAAE;QAC1C,MAAM,CAAC,GAAG,QAAQ,CAAC,CAAC,CAAC,EAAE,EAAE,EAAE,CAAC,EAAE,CAAC,EAAE,EAAE,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;QACjD,MAAM,IAAI,GAAG,QAAQ,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;QAChD,MAAM,QAAQ,GAAG,QAAQ,CAAC,CAAC,CAAC,CAAC,EAAE,EAAE,CAAC,EAAE,CAAC,CAAC,EAAE,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;QACtD,MAAM,KAAK,GAAG,QAAQ,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;QACjD,kBAAkB,CACd,kBAAkB,CAAC,CAAC,EAAE,IAAI,EAAE,QAAQ,EAAE,IAAI,EAAE,KAAK,EAAE,CAAC,CAAC,EACrD,QAAQ,CAAC,CAAC,CAAC,GAAG,EAAE,GAAG,CAAC,EAAE,CAAC,IAAI,EAAE,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC;IAClD,CAAC,CAAC,CAAC;IACH,EAAE,CAAC,+BAA+B,EAAE,GAAG,EAAE;QACvC,MAAM,CAAC,GAAG,QAAQ,CAAC,CAAC,CAAC,EAAE,EAAE,EAAE,CAAC,EAAE,CAAC,EAAE,EAAE,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;QACjD,MAAM,IAAI,GAAG,QAAQ,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;QAChD,MAAM,QAAQ,GAAG,QAAQ,CAAC,CAAC,CAAC,CAAC,EAAE,EAAE,CAAC,EAAE,CAAC,CAAC,EAAE,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;QACtD,MAAM,KAAK,GAAG,QAAQ,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;QACjD,MAAM,IAAI,GAAG,QAAQ,CAAC,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;QACpD,kBAAkB,CACd,kBAAkB,CAAC,CAAC,EAAE,IAAI,EAAE,QAAQ,EAAE,IAAI,EAAE,KAAK,EAAE,CAAC,CAAC,EACrD,QAAQ,CAAC,CAAC,CAAC,GAAG,EAAE,GAAG,CAAC,EAAE,CAAC,IAAI,EAAE,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC;IAClD,CAAC,CAAC,CAAC;IACH,EAAE,CAAC,4BAA4B,EAAE,GAAG,EAAE;QACpC,MAAM,CAAC,GAAG,QAAQ,CAAC,CAAC,CAAC,EAAE,EAAE,EAAE,CAAC,EAAE,CAAC,EAAE,EAAE,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;QACjD,MAAM,IAAI,GAAG,QAAQ,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;QAC9B,MAAM,QAAQ,GAAG,QAAQ,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;QAClC,MAAM,KAAK,GAAG,QAAQ,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;QAC/B,MAAM,IAAI,GAAG,QAAQ,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC;QAChC,kBAAkB,CACd,kBAAkB,CAAC,CAAC,EAAE,IAAI,EAAE,QAAQ,EAAE,IAAI,EAAE,KAAK,EAAE,CAAC,CAAC,EACrD,QAAQ,CAAC,CAAC,CAAC,EAAE,EAAE,EAAE,CAAC,EAAE,CAAC,EAAE,EAAE,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC;IAC9C,CAAC,CAAC,CAAC;IACH,EAAE,CAAC,qCAAqC,EAAE,GAAG,EAAE;QAC7C,MAAM,CAAC,GAAG,QAAQ,CAAC,CAAC,CAAC,CAAC,EAAE,EAAE,EAAE,CAAC,EAAE,CAAC,EAAE,EAAE,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,EAAE,EAAE,CAAC,EAAE,CAAC,EAAE,EAAE,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;QAC5E,MAAM,IAAI,GAAG,QAAQ,CAAC,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;QACvE,MAAM,QAAQ,GACV,QAAQ,CAAC,CAAC,CAAC,CAAC,CAAC,EAAE,EAAE,CAAC,EAAE,CAAC,CAAC,EAAE,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,EAAE,EAAE,CAAC,EAAE,CAAC,EAAE,EAAE,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;QACpE,kBAAkB,CACd,kBAAkB,CAAC,CAAC,EAAE,IAAI,EAAE,QAAQ,EAAE,IAAI,EAAE,IAAI,EAAE,CAAC,CAAC,EACpD,QAAQ,CACJ,CAAC,CAAC,CAAC,GAAG,EAAE,IAAI,CAAC,EAAE,CAAC,IAAI,EAAE,IAAI,CAAC,CAAC,EAAE,CAAC,CAAC,IAAI,EAAE,CAAC,CAAC,EAAE,CAAC,IAAI,EAAE,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC;IAC7E,CAAC,CAAC,CAAC;IACH,EAAE,CAAC,+BAA+B,EAAE,GAAG,EAAE;QACvC,MAAM,CAAC,GAAG,QAAQ,CAAC,CAAC,CAAC,CAAC,EAAE,EAAE,EAAE,CAAC,EAAE,CAAC,EAAE,EAAE,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,EAAE,EAAE,CAAC,EAAE,CAAC,EAAE,EAAE,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;QAC5E,MAAM,IAAI,GAAG,QAAQ,CAAC,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;QACvE,MAAM,QAAQ,GACV,QAAQ,CAAC,CAAC,CAAC,CAAC,CAAC,EAAE,EAAE,CAAC,EAAE,CAAC,CAAC,EAAE,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,EAAE,EAAE,CAAC,EAAE,CAAC,EAAE,EAAE,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;QACpE,MAAM,KAAK,GAAG,QAAQ,CAAC,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;QACxE,MAAM,IAAI,GACN,QAAQ,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;QACtE,kBAAkB,CACd,kBAAkB,CAAC,CAAC,EAAE,IAAI,EAAE,QAAQ,EAAE,IAAI,EAAE,KAAK,EAAE,CAAC,CAAC,EACrD,QAAQ,CAAC,CAAC,CAAC,CAAC,CAAC,EAAE,GAAG,CAAC,EAAE,CAAC,EAAE,EAAE,IAAI,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,EAAE,CAAC,EAAE,CAAC,EAAE,EAAE,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC;IAC1E,CAAC,CAAC,CAAC;IACH,EAAE,CAAC,4BAA4B,EAAE,GAAG,EAAE;QACpC,MAAM,CAAC,GAAG,QAAQ,CAAC,CAAC,CAAC,CAAC,EAAE,EAAE,EAAE,CAAC,EAAE,CAAC,EAAE,EAAE,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,EAAE,EAAE,CAAC,EAAE,CAAC,EAAE,EAAE,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;QAC5E,MAAM,IAAI,GAAG,QAAQ,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;QAC9B,MAAM,QAAQ,GAAG,QAAQ,CAAC,CAAC,CAAC,EAAE,EAAE,CAAC,CAAC,CAAC;QACnC,MAAM,KAAK,GAAG,QAAQ,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;QAC/B,MAAM,IAAI,GAAG,QAAQ,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC;QAChC,kBAAkB,CACd,kBAAkB,CAAC,CAAC,EAAE,IAAI,EAAE,QAAQ,EAAE,IAAI,EAAE,KAAK,EAAE,CAAC,CAAC,EACrD,QAAQ,CAAC,CAAC,CAAC,CAAC,CAAC,EAAE,EAAE,CAAC,EAAE,CAAC,EAAE,EAAE,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,EAAE,CAAC,EAAE,CAAC,EAAE,EAAE,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC;IACvE,CAAC,CAAC,CAAC;IACH,EAAE,CAAC,qCAAqC,EAAE,GAAG,EAAE;QAC7C,MAAM,CAAC,GAAG,QAAQ,CACd;YACE,CAAC,CAAC,CAAC,EAAE,EAAE,EAAE,CAAC,EAAE,CAAC,EAAE,EAAE,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,EAAE,EAAE,CAAC,EAAE,CAAC,EAAE,EAAE,EAAE,CAAC,CAAC,CAAC;YAC5C,CAAC,CAAC,CAAC,CAAC,EAAE,EAAE,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,EAAE,EAAE,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,EAAE,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,EAAE,EAAE,CAAC,EAAE,CAAC,CAAC,CAAC;SACrD,EACD,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;QAClB,MAAM,IAAI,GAAG,QAAQ,CACjB;YACE,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;YACpC,CAAC,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC;SAC7C,EACD,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;QAClB,MAAM,QAAQ,GAAG,QAAQ,CACrB;YACE,CAAC,CAAC,CAAC,CAAC,EAAE,EAAE,CAAC,EAAE,CAAC,CAAC,EAAE,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,EAAE,EAAE,CAAC,EAAE,CAAC,EAAE,EAAE,EAAE,CAAC,CAAC,CAAC;YAC1C,CAAC,CAAC,CAAC,CAAC,EAAE,EAAE,CAAC,EAAE,CAAC,CAAC,EAAE,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,EAAE,EAAE,CAAC,EAAE,CAAC,EAAE,EAAE,EAAE,CAAC,CAAC,CAAC;SAC3C,EACD,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;QAClB,kBAAkB,CACd,kBAAkB,CAAC,CAAC,EAAE,IAAI,EAAE,QAAQ,EAAE,IAAI,EAAE,IAAI,EAAE,CAAC,CAAC,EACpD,QAAQ,CACJ;YACE,CAAC,CAAC,CAAC,GAAG,EAAE,IAAI,CAAC,EAAE,CAAC,IAAI,EAAE,IAAI,CAAC,CAAC,EAAE,CAAC,CAAC,IAAI,EAAE,CAAC,CAAC,EAAE,CAAC,IAAI,EAAE,CAAC,CAAC,CAAC,CAAC;YACrD,CAAC,CAAC,CAAC,CAAC,GAAG,EAAE,CAAC,IAAI,CAAC,EAAE,CAAC,CAAC,IAAI,EAAE,CAAC,IAAI,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,IAAI,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,IAAI,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC;SAC9D,EACD,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC;IACzB,CAAC,CAAC,CAAC;IACH,EAAE,CAAC,+BAA+B,EAAE,GAAG,EAAE;QACvC,MAAM,CAAC,GAAG,QAAQ,CACd;YACE,CAAC,CAAC,CAAC,EAAE,EAAE,EAAE,CAAC,EAAE,CAAC,EAAE,EAAE,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,EAAE,EAAE,CAAC,EAAE,CAAC,EAAE,EAAE,EAAE,CAAC,CAAC,CAAC;YAC5C,CAAC,CAAC,CAAC,CAAC,EAAE,EAAE,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,EAAE,EAAE,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,EAAE,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,EAAE,EAAE,CAAC,EAAE,CAAC,CAAC,CAAC;SACrD,EACD,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;QAClB,MAAM,IAAI,GAAG,QAAQ,CACjB;YACE,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;YACpC,CAAC,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC;SAC7C,EACD,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;QAClB,MAAM,QAAQ,GAAG,QAAQ,CACrB;YACE,CAAC,CAAC,CAAC,CAAC,EAAE,EAAE,CAAC,EAAE,CAAC,CAAC,EAAE,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,EAAE,EAAE,CAAC,EAAE,CAAC,EAAE,EAAE,EAAE,CAAC,CAAC,CAAC;YAC1C,CAAC,CAAC,CAAC,CAAC,EAAE,EAAE,CAAC,EAAE,CAAC,CAAC,EAAE,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,EAAE,EAAE,CAAC,EAAE,CAAC,EAAE,EAAE,EAAE,CAAC,CAAC,CAAC;SAC3C,EACD,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;QAClB,MAAM,KAAK,GAAG,QAAQ,CAClB;YACE,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;YACpC,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;SACrC,EACD,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;QAClB,MAAM,IAAI,GAAG,QAAQ,CACjB;YACE,CAAC,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC;YAC5C,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;SACrC,EACD,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;QAClB,kBAAkB,CACd,kBAAkB,CAAC,CAAC,EAAE,IAAI,EAAE,QAAQ,EAAE,IAAI,EAAE,KAAK,EAAE,CAAC,CAAC,EACrD,QAAQ,CACJ;YACE,CAAC,CAAC,CAAC,CAAC,EAAE,GAAG,CAAC,EAAE,CAAC,EAAE,EAAE,IAAI,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,EAAE,CAAC,EAAE,CAAC,EAAE,EAAE,EAAE,CAAC,CAAC,CAAC;YAC7C,CAAC,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,GAAG,CAAC,EAAE,CAAC,CAAC,EAAE,EAAE,CAAC,IAAI,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,EAAE,EAAE,CAAC,EAAE,CAAC,CAAC,CAAC;SACtD,EACD,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC;IACzB,CAAC,CAAC,CAAC;IACH,EAAE,CAAC,4BAA4B,EAAE,GAAG,EAAE;QACpC,MAAM,CAAC,GAAG,QAAQ,CACd,CAAC,CAAC,CAAC,CAAC,EAAE,EAAE,EAAE,CAAC,EAAE,CAAC,EAAE,EAAE,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,EAAE,EAAE,CAAC,EAAE,CAAC,EAAE,EAAE,EAAE,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;QACpE,MAAM,IAAI,GAAG,QAAQ,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;QAC9B,MAAM,QAAQ,GAAG,QAAQ,CAAC,CAAC,CAAC,EAAE,EAAE,CAAC,CAAC,CAAC;QACnC,MAAM,KAAK,GAAG,QAAQ,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;QAC/B,MAAM,IAAI,GAAG,QAAQ,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC;QAChC,kBAAkB,CACd,kBAAkB,CAAC,CAAC,EAAE,IAAI,EAAE,QAAQ,EAAE,IAAI,EAAE,KAAK,EAAE,CAAC,CAAC,EACrD,QAAQ,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,EAAE,EAAE,CAAC,EAAE,CAAC,EAAE,EAAE,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,EAAE,EAAE,CAAC,EAAE,CAAC,EAAE,EAAE,EAAE,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC;IAC9E,CAAC,CAAC,CAAC;AACL,CAAC,CAAC,CAAC;AAEH,eAAe,CAAC,qCAAqC,EAAE,GAAG,EAAE;IAC1D,MAAM,gBAAgB,GAAG,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;IAC3D,KAAK,MAAM,UAAU,IAAI,gBAAgB,EAAE;QACzC,MAAM,SAAS,GAAG,SAAS,IAAI,CAAC,SAAS,CAAC,UAAU,CAAC,EAAE,CAAC;QACxD,EAAE,CAAC,SAAS,EAAE,GAAG,EAAE;YACjB,MAAM,CAAC,GAAG,IAAI,cAAc,CAAC,SAAS,EAAE,UAAU,EAAE,IAAI,EAAE,EAAE,EAAE,IAAI,CAAC,CAAC;YACpE,MAAM,KAAK,GAAG,GAAG,CAAC,MAAM,CAAC,kBAAkB,CAAC,EAAE,CAAC,CAAC;YAChD,MAAM,CAAC,GAAG,KAAK,CAAC,KAAK,CAAC,CAAC,CAAmB,CAAC;YAC3C,MAAM,CAAC,CAAC,CAAC,KAAK,CAAC,CAAC,OAAO,CAAC,CAAC,CAAC,KAAK,CAAC,CAAC;YACjC,MAAM,CAAC,CAAC,CAAC,KAAK,CAAC,CAAC,OAAO,CAAC,CAAC,CAAC,KAAK,CAAC,CAAC;QACnC,CAAC,CAAC,CAAC;KACJ;IAED,EAAE,CAAC,2CAA2C,EAAE,GAAG,EAAE;QACnD,MAAM,CAAC,GAAG,IAAI,cAAc,CAAC,SAAS,EAAE,CAAC,IAAI,EAAE,CAAC,EAAE,CAAC,CAAC,EAAE,IAAI,EAAE,EAAE,EAAE,IAAI,CAAC,CAAC;QACtE,MAAM,KAAK,GAAG,GAAG,CAAC,MAAM,CAAC,kBAAkB,CAAC,EAAC,IAAI,EAAE,CAAC,EAAC,CAAC,CAAC;QACvD,MAAM,CAAC,GAAG,EAAE,CAAC,KAAK,CAAC,KAAK,CAAC,CAAC,CAAC,CAAC;aACvB,YAAY,CACT,0DAA0D,CAAC,CAAC;IACtE,CAAC,CAAC,CAAC;IAEH,EAAE,CAAC,kDAAkD,EAAE,GAAG,EAAE;QAC1D,MAAM,KAAK,GAAG,GAAG,CAAC,MAAM,CAAC,kBAAkB,EAAE,CAAC;QAC9C,MAAM,CAAC,KAAK,CAAC,SAAS,EAAE,CAAC,IAAI,CAAC,CAAC,OAAO,CAAC,CAAC,CAAC,CAAC,CAAC;IAC7C,CAAC,CAAC,CAAC;AACL,CAAC,CAAC,CAAC;AAEH,qBAAqB,CAAC,mCAAmC,EAAE,GAAG,EAAE;IAC9D,MAAM,UAAU,GAAG,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,CAAC;IAC7B,MAAM,UAAU,GAAG,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;IAE3B,KAAK,MAAM,GAAG,IAAI,UAAU,EAAE;QAC5B,KAAK,MAAM,IAAI,IAAI,UAAU,EAAE;YAC7B,MAAM,SAAS,GAAG,cAAc,GAAG,WAAW,IAAI,EAAE,CAAC;YACrD,EAAE,CAAC,SAAS,EAAE,GAAG,EAAE;gBACjB,MAAM,KAAK,GAAG,GAAG,CAAC,MAAM,CAAC,kBAAkB,CAAC,EAAC,IAAI,EAAC,CAAC,CAAC;gBACpD,IAAI,CAAS,CAAC;gBACd,IAAI,GAAG,KAAK,CAAC,EAAE;oBACb,CAAC,GAAG,QAAQ,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;iBACxC;qBAAM,IAAI,GAAG,KAAK,CAAC,EAAE;oBACpB,CAAC,GAAG,QAAQ,CAAC,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;iBACnE;qBAAM,IAAI,GAAG,KAAK,CAAC,EAAE;oBACpB,CAAC,GAAG,QAAQ,CACR;wBACE,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC;wBACxC,CAAC,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;qBACzC,EACD,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;iBACnB;gBACD,MAAM,CAAC,GAAG,KAAK,CAAC,KAAK,CAAC,CAAC,EAAE,EAAC,QAAQ,EAAE,KAAK,EAAC,CAAW,CAAC;gBACtD,kBAAkB,CAAC,CAAC,EAAE,CAAC,EAAE,IAAI,CAAC,CAAC;YACjC,CAAC,CAAC,CAAC;SACJ;KACF;IAED,EAAE,CAAC,WAAW,EAAE,GAAG,EAAE;QACnB,MAAM,KAAK,GAAG,GAAG,CAAC,MAAM,CAAC,kBAAkB,CAAC,EAAC,MAAM,EAAE,KAAK,EAAE,IAAI,EAAE,CAAC,EAAC,CAAC,CAAC;QACtE,MAAM,CAAC,GAAG,QAAQ,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;QAC7C,kBAAkB,CAAC,KAAK,CAAC,KAAK,CAAC,CAAC,CAAW,EAAE,CAAC,EAAE,IAAI,CAAC,CAAC;QACtD,MAAM,CAAC,KAAK,CAAC,UAAU,EAAE,CAAC,MAAM,CAAC,CAAC,OAAO,CAAC,CAAC,CAAC,CAAC;QAC7C,wBAAwB;QACxB,kBAAkB,CAAC,KAAK,CAAC,UAAU,EAAE,CAAC,CAAC,CAAC,EAAE,QAAQ,CAAC,KAAK,CAAC,UAAU,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC;QAC3E,gCAAgC;QAChC,kBAAkB,CAAC,KAAK,CAAC,UAAU,EAAE,CAAC,CAAC,CAAC,EAAE,SAAS,CAAC,KAAK,CAAC,UAAU,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC;QAC5E,mCAAmC;QACnC,kBAAkB,CAAC,KAAK,CAAC,UAAU,EAAE,CAAC,CAAC,CAAC,EAAE,QAAQ,CAAC,KAAK,CAAC,UAAU,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC;IAC7E,CAAC,CAAC,CAAC;IAEH,EAAE,CAAC,UAAU,EAAE,GAAG,EAAE;QAClB,MAAM,KAAK,GAAG,GAAG,CAAC,MAAM,CAAC,kBAAkB,CAAC,EAAC,KAAK,EAAE,KAAK,EAAE,IAAI,EAAE,CAAC,EAAC,CAAC,CAAC;QACrE,MAAM,CAAC,GAAG,QAAQ,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;QAC7C,kBAAkB,CAAC,KAAK,CAAC,KAAK,CAAC,CAAC,CAAW,EAAE,CAAC,EAAE,IAAI,CAAC,CAAC;QACtD,MAAM,CAAC,KAAK,CAAC,UAAU,EAAE,CAAC,MAAM,CAAC,CAAC,OAAO,CAAC,CAAC,CAAC,CAAC;QAC7C,uBAAuB;QACvB,kBAAkB,CAAC,KAAK,CAAC,UAAU,EAAE,CAAC,CAAC,CAAC,EAAE,SAAS,CAAC,KAAK,CAAC,UAAU,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC;QAC5E,gCAAgC;QAChC,kBAAkB,CAAC,KAAK,CAAC,UAAU,EAAE,CAAC,CAAC,CAAC,EAAE,SAAS,CAAC,KAAK,CAAC,UAAU,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC;QAC5E,mCAAmC;QACnC,kBAAkB,CAAC,KAAK,CAAC,UAAU,EAAE,CAAC,CAAC,CAAC,EAAE,QAAQ,CAAC,KAAK,CAAC,UAAU,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC;IAC7E,CAAC,CAAC,CAAC;IAEH,EAAE,CAAC,qBAAqB,EAAE,GAAG,EAAE;QAC7B,MAAM,KAAK,GAAG,GAAG,CAAC,MAAM,CAAC,kBAAkB,CAAC,EAAC,KAAK,EAAE,KAAK,EAAE,MAAM,EAAE,KAAK,EAAC,CAAC,CAAC;QAC3E,MAAM,CAAC,GAAG,QAAQ,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;QAC7C,kBAAkB,CAAC,KAAK,CAAC,KAAK,CAAC,CAAC,CAAW,EAAE,CAAC,EAAE,IAAI,CAAC,CAAC;QACtD,MAAM,CAAC,KAAK,CAAC,UAAU,EAAE,CAAC,MAAM,CAAC,CAAC,OAAO,CAAC,CAAC,CAAC,CAAC;QAC7C,+BAA+B;QAC/B,kBAAkB,CAAC,KAAK,CAAC,UAAU,EAAE,CAAC,CAAC,CAAC,EAAE,SAAS,CAAC,KAAK,CAAC,UAAU,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC;QAC5E,oCAAoC;QACpC,kBAAkB,CAAC,KAAK,CAAC,UAAU,EAAE,CAAC,CAAC,CAAC,EAAE,QAAQ,CAAC,KAAK,CAAC,UAAU,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC;IAC7E,CAAC,CAAC,CAAC;IAEH,2EAA2E;IAC3E,YAAY;IACZ,+BAA+B;IAC/B,qBAAqB;IACrB,EAAE;IACF,6DAA6D;IAC7D,qCAAqC;IACrC,EAAE;IACF,4DAA4D;IAC5D,EAAE;IACF,iBAAiB;IACjB,uEAAuE;IACvE,wBAAwB;IACxB,8BAA8B;IAC9B,sDAAsD;IACtD,yBAAyB;IACzB,8BAA8B;IAC9B,MAAM;IACN,EAAE,CAAC,+BAA+B,EAAE,KAAK,IAAI,EAAE;QAC7C,MAAM,MAAM,GAAG,GAAG,CAAC,MAAM,CAAC,kBAAkB,CAAC,EAAC,UAAU,EAAE,CAAC,CAAC,CAAC,EAAC,CAAC,CAAC;QAChE,MAAM,KAAK,GAAG,GAAG,CAAC,UAAU,CAAC,EAAC,MAAM,EAAE,CAAC,MAAM,CAAC,EAAC,CAAC,CAAC;QACjD,KAAK,CAAC,OAAO,CAAC,EAAC,IAAI,EAAE,kBAAkB,EAAE,SAAS,EAAE,KAAK,EAAC,CAAC,CAAC;QAE5D,MAAM,GAAG,GAAG,QAAQ,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,EAAE,EAAE,EAAE,EAAE,EAAE,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;QAC5E,MAAM,EAAE,GAAG,KAAK,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;QACzB,MAAM,OAAO,GAAG,MAAM,KAAK,CAAC,GAAG,CAAC,GAAG,EAAE,EAAE,EAAE,EAAC,MAAM,EAAE,CAAC,EAAE,SAAS,EAAE,CAAC,EAAC,CAAC,CAAC;QACpE,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,MAAM,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,WAAW,CAAC,kBAAkB,CAAC,CAAC;QACnE,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,MAAM,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,WAAW,CAAC,kBAAkB,CAAC,CAAC;QACnE,MAAM,UAAU,GAAG,MAAM,CAAC,UAAU,EAAE,CAAC,CAAC,CAAC,CAAC;QAC1C,kBAAkB,CACd,UAAU,EAAE,CAAC,SAAS,EAAE,SAAS,EAAE,SAAS,EAAE,SAAS,CAAC,CAAC,CAAC;QAC9D,MAAM,SAAS,GAAG,MAAM,CAAC,UAAU,EAAE,CAAC,CAAC,CAAC,CAAC;QACzC,kBAAkB,CACd,SAAS,EACT,CAAC,aAAa,EAAE,aAAa,EAAE,aAAa,EAAE,CAAC,aAAa,CAAC,CAAC,CAAC;QACnE,MAAM,eAAe,GAAG,MAAM,CAAC,UAAU,EAAE,CAAC,CAAC,CAAC,CAAC;QAC/C,kBAAkB,CACd,eAAe,EAAE,CAAC,UAAU,EAAE,UAAU,EAAE,UAAU,EAAE,UAAU,CAAC,CAAC,CAAC;QACvE,MAAM,mBAAmB,GAAG,MAAM,CAAC,UAAU,EAAE,CAAC,CAAC,CAAC,CAAC;QACnD,kBAAkB,CACd,mBAAmB,EAAE,CAAC,SAAS,EAAE,SAAS,EAAE,SAAS,EAAE,SAAS,CAAC,CAAC,CAAC;IACzE,CAAC,CAAC,CAAC;IAEH,gEAAgE;IAChE,aAAa;IACb,YAAY;IACZ,+BAA+B;IAC/B,qBAAqB;IACrB,EAAE;IACF,+BAA+B;IAC/B,sEAAsE;IACtE,6CAA6C;IAC7C,4DAA4D;IAC5D,8CAA8C;IAC9C,qDAAqD;IACrD,EAAE;IACF,2CAA2C;IAC3C,gEAAgE;IAChE,EAAE;IACF,+DAA+D;IAC/D,mCAAmC;IACnC,wBAAwB;IACxB,sDAAsD;IACtD,EAAE;IACF,yBAAyB;IACzB,8BAA8B;IAC9B,8BAA8B;IAC9B,8BAA8B;IAC9B,MAAM;IACN,EAAE,CAAC,mDAAmD,EAAE,KAAK,IAAI,EAAE;QACjE,MAAM,MAAM,GAAG,GAAG,CAAC,MAAM,CAAC,KAAK,CAC3B,EAAC,KAAK,EAAE,CAAC,EAAE,iBAAiB,EAAE,MAAM,EAAE,OAAO,EAAE,KAAK,EAAE,UAAU,EAAE,CAAC,CAAC,CAAC,EAAC,CAAC,CAAC;QAC5E,MAAM,MAAM,GAAG,GAAG,CAAC,MAAM,CAAC,kBAAkB,CAAC,EAAC,UAAU,EAAE,CAAC,CAAC,CAAC,EAAC,CAAC,CAAC;QAChE,MAAM,MAAM,GACR,GAAG,CAAC,MAAM,CAAC,KAAK,CAAC,EAAC,KAAK,EAAE,CAAC,EAAE,iBAAiB,EAAE,MAAM,EAAE,OAAO,EAAE,KAAK,EAAC,CAAC,CAAC;QAC5E,MAAM,KAAK,GAAG,GAAG,CAAC,UAAU,CAAC,EAAC,MAAM,EAAE,CAAC,MAAM,EAAE,MAAM,EAAE,MAAM,CAAC,EAAC,CAAC,CAAC;QAEjE,MAAM,SAAS,GAAG,KAAK,CAAC,GAAG,CAAC,GAAG,CAAC,CAAC;QACjC,KAAK,CAAC,OAAO,CAAC,EAAC,IAAI,EAAE,kBAAkB,EAAE,SAAS,EAAC,CAAC,CAAC;QAErD,MAAM,GAAG,GAAG,QAAQ,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,EAAE,EAAE,EAAE,EAAE,EAAE,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;QAC5E,MAAM,EAAE,GAAG,KAAK,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;QACzB,MAAM,OAAO,GAAG,MAAM,KAAK,CAAC,GAAG,CAAC,GAAG,EAAE,EAAE,EAAE,EAAC,MAAM,EAAE,CAAC,EAAE,SAAS,EAAE,CAAC,EAAC,CAAC,CAAC;QACpE,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,MAAM,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,WAAW,CAAC,kBAAkB,CAAC,CAAC;QACnE,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,MAAM,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,WAAW,CAAC,oBAAoB,CAAC,CAAC;QACrE,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,MAAM,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,WAAW,CAAC,oBAAoB,CAAC,CAAC;QACrE,MAAM,iBAAiB,GAAG,MAAM,CAAC,UAAU,EAAE,CAAC,CAAC,CAAC,CAAC;QACjD,kBAAkB,CACd,iBAAiB,EACjB,QAAQ,CACJ;YACE,CAAC,UAAU,EAAE,UAAU,EAAE,UAAU,EAAE,UAAU,CAAC;YAChD,CAAC,SAAS,EAAE,SAAS,EAAE,SAAS,EAAE,SAAS,CAAC;YAC5C,CAAC,QAAQ,EAAE,QAAQ,EAAE,QAAQ,EAAE,QAAQ,CAAC;YACxC,CAAC,UAAU,EAAE,UAAU,EAAE,UAAU,EAAE,UAAU,CAAC;SACjD,EACD,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC;QACjB,MAAM,UAAU,GAAG,MAAM,CAAC,UAAU,EAAE,CAAC,CAAC,CAAC,CAAC;QAC1C,kBAAkB,CACd,UAAU,EAAE,CAAC,UAAU,EAAE,UAAU,EAAE,UAAU,EAAE,UAAU,CAAC,CAAC,CAAC;QAClE,MAAM,SAAS,GAAG,MAAM,CAAC,UAAU,EAAE,CAAC,CAAC,CAAC,CAAC;QACzC,kBAAkB,CACd,SAAS,EACT,CAAC,aAAa,EAAE,aAAa,EAAE,aAAa,EAAE,aAAa,CAAC,CAAC,CAAC;QAClE,MAAM,eAAe,GAAG,MAAM,CAAC,UAAU,EAAE,CAAC,CAAC,CAAC,CAAC;QAC/C,uCAAuC;QACvC,kBAAkB,CACd,eAAe,EAAE,CAAC,SAAS,EAAE,SAAS,EAAE,SAAS,EAAE,SAAS,CAAC,CAAC,CAAC;QACnE,MAAM,mBAAmB,GAAG,MAAM,CAAC,UAAU,EAAE,CAAC,CAAC,CAAC,CAAC;QACnD,kBAAkB,CACd,mBAAmB,EAAE,CAAC,QAAQ,EAAE,QAAQ,EAAE,QAAQ,EAAE,QAAQ,CAAC,CAAC,CAAC;QACnE,MAAM,iBAAiB,GAAG,MAAM,CAAC,UAAU,EAAE,CAAC,CAAC,CAAC,CAAC;QACjD,kBAAkB,CACd,iBAAiB,EACjB,QAAQ,CACJ,CAAC,CAAC,UAAU,CAAC,EAAE,CAAC,UAAU,CAAC,EAAE,CAAC,UAAU,CAAC,EAAE,CAAC,UAAU,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC;IAC7E,CAAC,CAAC,CAAC;IAEH,yBAAyB;IACzB,YAAY;IACZ,qBAAqB;IACrB,+BAA+B;IAC/B,EAAE;IACF,6BAA6B;IAC7B,iCAAiC;IACjC,SAAS;IACT,SAAS;IACT,iCAAiC;IACjC,gCAAgC;IAChC,8BAA8B;IAC9B,+CAA+C;IAC/C,oCAAoC;IACpC,gCAAgC;IAChC,SAAS;IACT,iCAAiC;IACjC,iCAAiC;IACjC,EAAE;IACF,6CAA6C;IAC7C,EAAE;IACF,sDAAsD;IACtD,4BAA4B;IAC5B,kCAAkC;IAClC,mBAAmB;IACnB,MAAM;IACN,EAAE,CAAC,wBAAwB,EAAE,KAAK,IAAI,EAAE;QACtC,MAAM,KAAK,GAAG,GAAG,CAAC,UAAU,EAAE,CAAC;QAC/B,KAAK,CAAC,GAAG,CAAC,GAAG,CAAC,MAAM,CAAC,MAAM,CAAC;YAC1B,OAAO,EAAE,CAAC;YACV,UAAU,EAAE,CAAC;YACb,iBAAiB,EAAE,MAAM;YACzB,eAAe,EAAE,OAAO;YACxB,UAAU,EAAE,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC;SACtB,CAAC,CAAC,CAAC;QACJ,KAAK,CAAC,GAAG,CAAC,GAAG,CAAC,MAAM,CAAC,kBAAkB,EAAE,CAAC,CAAC;QAC3C,KAAK,CAAC,GAAG,CAAC,GAAG,CAAC,MAAM,CAAC,OAAO,EAAE,CAAC,CAAC;QAChC,KAAK,CAAC,GAAG,CAAC,GAAG,CAAC,MAAM,CAAC,KAAK,CACtB,EAAC,KAAK,EAAE,CAAC,EAAE,iBAAiB,EAAE,MAAM,EAAE,eAAe,EAAE,OAAO,EAAC,CAAC,CAAC,CAAC;QACtE,KAAK,CAAC,OAAO,CAAC,EAAC,IAAI,EAAE,kBAAkB,EAAE,SAAS,EAAE,KAAK,EAAC,CAAC,CAAC;QAE5D,MAAM,MAAM,GAAG,EAAE,CAAC;QAClB,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,CAAC,GAAG,CAAC,GAAG,CAAC,GAAG,CAAC,EAAE,EAAE,CAAC,EAAE;YACtC,MAAM,CAAC,IAAI,CAAC,CAAC,CAAC,CAAC;SAChB;QACD,MAAM,EAAE,GAAG,QAAQ,CAAC,MAAM,EAAE,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;QAC1C,MAAM,EAAE,GAAG,QAAQ,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;QAEpC,MAAM,CAAC,GAAG,MAAM,KAAK,CAAC,GAAG,CAAC,EAAE,EAAE,EAAE,EAAE,EAAC,MAAM,EAAE,CAAC,EAAC,CAAC,CAAC;QAC/C,kBAAkB,CACd,CAAC,CAAC,OAAO,CAAC,IAAgB,EAAE,CAAC,SAAS,EAAE,SAAS,CAAC,EAAE,IAAI,CAAC,CAAC;IAChE,CAAC,CAAC,CAAC;IAEH,yBAAyB;IACzB,YAAY;IACZ,qBAAqB;IACrB,0BAA0B;IAC1B,+BAA+B;IAC/B,EAAE;IACF,6BAA6B;IAC7B,0CAA0C;IAC1C,SAAS;IACT,SAAS;IACT,iCAAiC;IACjC,gCAAgC;IAChC,8BAA8B;IAC9B,+CAA+C;IAC/C,oCAAoC;IACpC,gCAAgC;IAChC,SAAS;IACT,iCAAiC;IACjC,iCAAiC;IACjC,EAAE;IACF,6CAA6C;IAC7C,EAAE;IACF,yEAAyE;IACzE,2BAA2B;IAC3B,4BAA4B;IAC5B,EAAE;IACF,uCAAuC;IACvC,kCAAkC;IAClC,mBAAmB;IACnB,uCAAuC;IACvC,MAAM;IACN,EAAE,CAAC,iCAAiC,EAAE,KAAK,IAAI,EAAE;QAC/C,MAAM,KAAK,GAAG,GAAG,CAAC,UAAU,EAAE,CAAC;QAC/B,KAAK,CAAC,GAAG,CAAC,GAAG,CAAC,MAAM,CAAC,eAAe,CAAC;YACnC,OAAO,EAAE,CAAC;YACV,UAAU,EAAE,CAAC;YACb,iBAAiB,EAAE,MAAM;YACzB,eAAe,EAAE,OAAO;YACxB,UAAU,EAAE,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC;SACtB,CAAC,CAAC,CAAC;QACJ,KAAK,CAAC,GAAG,CAAC,GAAG,CAAC,MAAM,CAAC,kBAAkB,EAAE,CAAC,CAAC;QAC3C,KAAK,CAAC,GAAG,CAAC,GAAG,CAAC,MAAM,CAAC,OAAO,EAAE,CAAC,CAAC;QAChC,KAAK,CAAC,GAAG,CAAC,GAAG,CAAC,MAAM,CAAC,KAAK,CACtB,EAAC,KAAK,EAAE,CAAC,EAAE,iBAAiB,EAAE,MAAM,EAAE,eAAe,EAAE,OAAO,EAAC,CAAC,CAAC,CAAC;QACtE,KAAK,CAAC,OAAO,CAAC,EAAC,IAAI,EAAE,kBAAkB,EAAE,SAAS,EAAE,KAAK,EAAC,CAAC,CAAC;QAE5D,MAAM,MAAM,GAAG,EAAE,CAAC;QAClB,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,CAAC,GAAG,CAAC,GAAG,CAAC,GAAG,CAAC,EAAE,EAAE,CAAC,EAAE;YACtC,MAAM,CAAC,IAAI,CAAC,CAAC,CAAC,CAAC;SAChB;QACD,MAAM,EAAE,GACJ,CAAC,QAAQ,CAAC,MAAM,EAAE,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,GAAG,CAAC,MAAM,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,GAAG,CAAC,MAAM,CAAC,GAAG,CAAC,CAAC,CAAC;QACtE,MAAM,EAAE,GAAG,QAAQ,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;QAEpC,MAAM,CAAC,GAAG,MAAM,KAAK,CAAC,GAAG,CAAC,EAAE,EAAE,EAAE,EAAE,EAAC,MAAM,EAAE,CAAC,EAAC,CAAC,CAAC;QAC/C,MAAM,CAAC,CAAC,CAAC,OAAO,CAAC,IAAI,CAAC,CAAC,CAAC,CAAC,CAAC,WAAW,CAAC,UAAU,CAAC,CAAC;QAClD,MAAM,CAAC,CAAC,CAAC,OAAO,CAAC,IAAI,CAAC,CAAC,CAAC,CAAC,CAAC,WAAW,CAAC,SAAS,EAAE,CAAC,CAAC,CAAC,CAAC;QACrD,MAAM,OAAO,GAAG,KAAK,CAAC,MAAM,CAAC,CAAC,CAAC,CAAC,UAAU,EAAE,CAAC;QAC7C,MAAM,CAAC,OAAO,CAAC,MAAM,CAAC,CAAC,OAAO,CAAC,CAAC,CAAC,CAAC;QAClC,kBAAkB,CACd,OAAO,CAAC,CAAC,CAAC,EAAE,QAAQ,CAAC,CAAC,SAAS,EAAE,SAAS,EAAE,SAAS,EAAE,SAAS,CAAC,CAAC,EAClE,IAAI,CAAC,CAAC;QACV,kBAAkB,CACd,OAAO,CAAC,CAAC,CAAC,EAAE,QAAQ,CAAC,CAAC,CAAC,SAAS,EAAE,CAAC,SAAS,EAAE,CAAC,SAAS,EAAE,CAAC,SAAS,CAAC,CAAC,EACtE,IAAI,CAAC,CAAC;QACV,kBAAkB,CACd,OAAO,CAAC,CAAC,CAAC,EACV,QAAQ,CAAC,CAAC,CAAC,UAAU,EAAE,CAAC,UAAU,EAAE,CAAC,UAAU,EAAE,CAAC,UAAU,CAAC,CAAC,CAAC,CAAC;QACpE,kBAAkB,CACd,OAAO,CAAC,CAAC,CAAC,EAAE,QAAQ,CAAC,CAAC,UAAU,EAAE,UAAU,EAAE,UAAU,EAAE,UAAU,CAAC,CAAC,CAAC,CAAC;IAC9E,CAAC,CAAC,CAAC;IAEH,2EAA2E;IAC3E,YAAY;IACZ,+BAA+B;IAC/B,qBAAqB;IACrB,EAAE;IACF,+DAA+D;IAC/D,qCAAqC;IACrC,EAAE;IACF,4DAA4D;IAC5D,EAAE;IACF,iBAAiB;IACjB,iEAAiE;IACjE,wBAAwB;IACxB,6CAA6C;IAC7C,8BAA8B;IAC9B,sDAAsD;IACtD,yBAAyB;IACzB,8BAA8B;IAC9B,MAAM;IACN,EAAE,CAAC,+BAA+B,EAAE,KAAK,IAAI,EAAE;QAC7C,MAAM,MAAM,GAAG,GAAG,CAAC,MAAM,CAAC,kBAAkB,CAAC,EAAC,UAAU,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAC,CAAC,CAAC;QACnE,MAAM,KAAK,GAAG,GAAG,CAAC,UAAU,CAAC,EAAC,MAAM,EAAE,CAAC,MAAM,CAAC,EAAC,CAAC,CAAC;QACjD,KAAK,CAAC,OAAO,CAAC,EAAC,IAAI,EAAE,kBAAkB,EAAE,SAAS,EAAE,KAAK,EAAC,CAAC,CAAC;QAE5D,MAAM,GAAG,GAAG,QAAQ,CAChB,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,EAAE,EAAE,CAAC,EAAE,CAAC,EAAE,EAAE,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;QAC1E,MAAM,EAAE,GAAG,KAAK,CAAC,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;QAC5B,MAAM,OAAO,GAAG,MAAM,KAAK,CAAC,GAAG,CAAC,GAAG,EAAE,EAAE,EAAE,EAAC,MAAM,EAAE,CAAC,EAAE,SAAS,EAAE,CAAC,EAAC,CAAC,CAAC;QACpE,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,MAAM,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,WAAW,CAAC,kBAAkB,CAAC,CAAC;QACnE,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,MAAM,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,WAAW,CAAC,iBAAiB,CAAC,CAAC;QAClE,MAAM,UAAU,GAAG,MAAM,CAAC,UAAU,EAAE,CAAC,CAAC,CAAC,CAAC;QAC1C,kBAAkB,CAAC,UAAU,EAAE,CAAC,UAAU,EAAE,UAAU,CAAC,CAAC,CAAC;QACzD,MAAM,SAAS,GAAG,MAAM,CAAC,UAAU,EAAE,CAAC,CAAC,CAAC,CAAC;QACzC,kBAAkB,CAAC,SAAS,EAAE,CAAC,CAAC,aAAa,EAAE,aAAa,CAAC,CAAC,CAAC;QAC/D,MAAM,eAAe,GAAG,MAAM,CAAC,UAAU,EAAE,CAAC,CAAC,CAAC,CAAC;QAC/C,kBAAkB,CAAC,eAAe,EAAE,CAAC,UAAU,EAAE,UAAU,CAAC,CAAC,CAAC;QAC9D,MAAM,mBAAmB,GAAG,MAAM,CAAC,UAAU,EAAE,CAAC,CAAC,CAAC,CAAC;QACnD,kBAAkB,CAAC,mBAAmB,EAAE,CAAC,SAAS,EAAE,SAAS,CAAC,EAAE,IAAI,CAAC,CAAC;IACxE,CAAC,CAAC,CAAC;AACL,CAAC,CAAC,CAAC;AAEH,qBAAqB,CAAC,oCAAoC,EAAE,GAAG,EAAE;IAC/D,EAAE,CAAC,+CAA+C,EAAE,GAAG,EAAE;QACvD,MAAM,CAAC,GAAG,EAAE,CAAC,GAAG,CAAC,MAAM,CAAC,kBAAkB,CAAC;YACzC,kCAAkC;YAClC,IAAI,EAAE,KAAY;SACnB,CAAC,CAAC,CAAC,YAAY,CAAC,gCAAgC,CAAC,CAAC;QACnD,MAAM,CAAC,GAAG,EAAE,CAAC,GAAG,CAAC,MAAM,CAAC,kBAAkB,CAAC;YACzC,IAAI,EAAE,GAAG;SACV,CAAC,CAAC,CAAC,YAAY,CAAC,gCAAgC,CAAC,CAAC;QACnD,MAAM,CAAC,GAAG,EAAE,CAAC,GAAG,CAAC,MAAM,CAAC,kBAAkB,CAAC;YACzC,IAAI,EAAE,CAAC,CAAC,EAAE,GAAG,CAAC;SACf,CAAC,CAAC,CAAC,YAAY,CAAC,0CAA0C,CAAC,CAAC;IAC/D,CAAC,CAAC,CAAC;IAEH,EAAE,CAAC,0BAA0B,EAAE,KAAK,IAAI,EAAE;QACxC,MAAM,KAAK,GAAG,GAAG,CAAC,MAAM,CAAC,kBAAkB,CACvC,EAAC,IAAI,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,MAAM,EAAE,IAAI,EAAE,KAAK,EAAE,KAAK,EAAC,CAAC,CAAC;QAClD,MAAM,cAAc,GAAG,mBAAmB,CAAC,KAAK,CAAC,SAAS,EAAE,CAAC,CAAC;QAC9D,kCAAkC;QAClC,MAAM,QAAQ,GAAG,mBAAmB,CAAC,cAAc,CAAQ,CAAC;QAC5D,MAAM,UAAU,GAAG,GAAG,CAAC,MAAM,CAAC,kBAAkB,CAAC,QAAQ,CAAC,CAAC;QAC3D,MAAM,CAAC,UAAU,CAAC,SAAS,EAAE,CAAC,CAAC,OAAO,CAAC,KAAK,CAAC,SAAS,EAAE,CAAC,CAAC;IAC5D,CAAC,CAAC,CAAC;IAEH,EAAE,CAAC,wCAAwC,EAAE,KAAK,IAAI,EAAE;QACtD,iCAAiC;QACjC,MAAM,eAAe,GACjB,iiDAAiiD,CAAC;QACtiD,gCAAgC;QAChC,MAAM,KAAK,GAAG,MAAM,GAAG,CAAC,MAAM,CAAC,aAAa,CAAC,IAAI,CAAC,KAAK,CAAC,eAAe,CAAC,CAAC,CAAC;QAC1E,MAAM,EAAE,GAAG,KAAK,CAAC,OAAO,CAAC,KAAK,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAW,CAAC;QAClD,MAAM,CAAC,EAAE,CAAC,KAAK,CAAC,CAAC,OAAO,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;QACjC,MAAM,CAAC,KAAK,CAAC,MAAM,CAAC,CAAC,CAAC,CAAC,UAAU,EAAE,CAAC,MAAM,CAAC,CAAC,OAAO,CAAC,CAAC,CAAC,CAAC;IACzD,CAAC,CAAC,CAAC;AACL,CAAC,CAAC,CAAC;AAEH,qBAAqB,CAAC,kCAAkC,EAAE,GAAG,EAAE;IAC7D,yBAAyB;IACzB,QAAQ;IACR,qBAAqB;IACrB,0BAA0B;IAC1B,EAAE;IACF,8BAA8B;IAC9B,EAAE;IACF,+CAA+C;IAC/C,2DAA2D;IAC3D,iBAAiB;IACjB,YAAY;IACZ,MAAM;IACN,EAAE,CAAC,iCAAiC,EAAE,GAAG,EAAE;QACzC,MAAM,KAAK,GAAG,GAAG,CAAC,MAAM,CAAC,kBAAkB,EAAE,CAAC;QAC9C,MAAM,EAAE,GAAG,QAAQ,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,EAAE,EAAE,CAAC,CAAC,CAAC,CAAC;QAC7C,MAAM,EAAE,GAAG,KAAK,CAAC,KAAK,CAAC,EAAE,CAAW,CAAC;QACrC,kBAAkB,CACd,EAAE,EACF,QAAQ,CACJ,CAAC,CAAC,CAAC,SAAS,EAAE,CAAC,EAAE,SAAS,CAAC,EAAE,CAAC,CAAC,SAAS,EAAE,CAAC,SAAS,EAAE,QAAQ,CAAC,CAAC,CAAC,CAAC,CAAC;IAC7E,CAAC,CAAC,CAAC;IAEH,EAAE,CAAC,yBAAyB,EAAE,GAAG,EAAE;QACjC,MAAM,KAAK,GAAG,GAAG,CAAC,MAAM,CAAC,kBAAkB,EAAE,CAAC;QAC9C,MAAM,EAAE,GAAG,QAAQ,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,EAAE,EAAE,CAAC,CAAC,CAAC,CAAC;QAC7C,OAAO,CAAC,KAAK,CAAC,KAAK,CAAC,EAAE,CAAW,CAAC,CAAC,CAAE,WAAW;QAChD,MAAM,WAAW,GAAG,MAAM,EAAE,CAAC,UAAU,CAAC;QACxC,OAAO,CAAC,KAAK,CAAC,KAAK,CAAC,EAAE,CAAW,CAAC,CAAC;QACnC,MAAM,CAAC,MAAM,EAAE,CAAC,UAAU,CAAC,CAAC,OAAO,CAAC,WAAW,CAAC,CAAC;IACnD,CAAC,CAAC,CAAC;IAEH,yBAAyB;IACzB,QAAQ;IACR,qBAAqB;IACrB,0BAA0B;IAC1B,EAAE;IACF,8BAA8B;IAC9B,EAAE;IACF,+CAA+C;IAC/C,yDAAyD;IACzD,qDAAqD;IACrD,iBAAiB;IACjB,YAAY;IACZ,MAAM;IACN,EAAE,CAAC,iCAAiC,EAAE,GAAG,EAAE;QACzC,MAAM,KAAK,GAAG,GAAG,CAAC,MAAM,CAAC,kBAAkB,EAAE,CAAC;QAC9C,MAAM,EAAE,GAAG,QAAQ,CAAC,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,EAAE,CAAC,EAAE,CAAC,EAAE,CAAC,EAAE,CAAC,EAAE,CAAC,EAAE,EAAE,EAAE,CAAC,EAAE,EAAE,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;QACxE,MAAM,EAAE,GAAG,KAAK,CAAC,KAAK,CAAC,EAAE,CAAW,CAAC;QACrC,kBAAkB,CACd,EAAE,EAAE,QAAQ,CAAC;YACX,CAAC,CAAC,CAAC,SAAS,EAAE,CAAC,EAAE,SAAS,CAAC,EAAE,CAAC,SAAS,EAAE,CAAC,EAAE,CAAC,SAAS,CAAC,CAAC;YACxD;gBACE,CAAC,CAAC,SAAS,EAAE,CAAC,SAAS,EAAE,QAAQ,CAAC;gBAClC,CAAC,CAAC,SAAS,EAAE,UAAU,EAAE,SAAS,CAAC;aACpC;SACF,CAAC,CAAC,CAAC;IACV,CAAC,CAAC,CAAC;IAEH,yBAAyB;IACzB,QAAQ;IACR,qBAAqB;IACrB,0BAA0B;IAC1B,EAAE;IACF,8BAA8B;IAC9B,EAAE;IACF,0DAA0D;IAC1D,yDAAyD;IACzD,qDAAqD;IACrD,iBAAiB;IACjB,YAAY;IACZ,MAAM;IACN,MAAM,oBAAoB,GAAe,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC;IAC5D,KAAK,MAAM,cAAc,IAAI,oBAAoB,EAAE;QACjD,EAAE,CAAC,wCAAwC,cAAc,EAAE,EAAE,GAAG,EAAE;YAChE,MAAM,KAAK,GAAG,GAAG,CAAC,MAAM,CAAC,kBAAkB,CAAC,EAAC,IAAI,EAAE,cAAc,EAAC,CAAC,CAAC;YACpE,MAAM,EAAE,GAAG,QAAQ,CAAC,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,EAAE,CAAC,EAAE,CAAC,EAAE,CAAC,EAAE,CAAC,EAAE,CAAC,EAAE,EAAE,EAAE,CAAC,EAAE,EAAE,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;YACxE,MAAM,EAAE,GAAG,KAAK,CAAC,KAAK,CAAC,EAAE,CAAW,CAAC;YACrC,kBAAkB,CAAC,EAAE,EAAE,QAAQ,CAAC;gBACX;oBACE,CAAC,CAAC,SAAS,EAAE,CAAC,SAAS,EAAE,CAAC,UAAU,CAAC;oBACrC,CAAC,SAAS,EAAE,SAAS,EAAE,UAAU,CAAC;iBACnC;gBACD;oBACE,CAAC,CAAC,SAAS,EAAE,UAAU,EAAE,QAAQ,CAAC;oBAClC,CAAC,CAAC,SAAS,EAAE,CAAC,UAAU,EAAE,UAAU,CAAC;iBACtC;aACF,CAAC,CAAC,CAAC;QACzB,CAAC,CAAC,CAAC;KACJ;IAED,EAAE,CAAC,oDAAoD,EAAE,GAAG,EAAE;QAC5D,MAAM,MAAM,GAAG,GAAG,CAAC,MAAM,CAAC,kBAAkB,CAAC,EAAC,IAAI,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAC,CAAC,CAAC;QACnE,MAAM,EAAE,GAAG,QAAQ,CAAC,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,EAAE,CAAC,EAAE,CAAC,EAAE,CAAC,EAAE,CAAC,EAAE,CAAC,EAAE,EAAE,EAAE,CAAC,EAAE,EAAE,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;QACxE,MAAM,CAAC,GAAG,EAAE,CAAC,MAAM,CAAC,KAAK,CAAC,EAAE,CAAC,CAAC,CAAC,YAAY,CAAC,gBAAgB,CAAC,CAAC;IAChE,CAAC,CAAC,CAAC;IAEH,yBAAyB;IACzB,QAAQ;IACR,qBAAqB;IACrB,0BAA0B;IAC1B,EAAE;IACF,gCAAgC;IAChC,kEAAkE;IAClE,iEAAiE;IACjE,6CAA6C;IAC7C,EAAE;IACF,uEAAuE;IACvE,oDAAoD;IACpD,mDAAmD;IACnD,yBAAyB;IACzB,MAAM;IACN,EAAE,CAAC,4BAA4B,EAAE,KAAK,IAAI,EAAE;QAC1C,MAAM,KAAK,GAAG,GAAG,CAAC,UAAU,EAAE,CAAC;QAC/B,KAAK,CAAC,GAAG,CAAC,GAAG,CAAC,MAAM,CAAC,kBAAkB,CAAC,EAAC,UAAU,EAAE,CAAC,CAAC,CAAC,EAAC,CAAC,CAAC,CAAC;QAC5D,KAAK,CAAC,GAAG,CAAC,GAAG,CAAC,MAAM,CAAC,KAAK,CAAC,EAAC,KAAK,EAAE,CAAC,EAAE,iBAAiB,EAAE,MAAM,EAAC,CAAC,CAAC,CAAC;QACnE,KAAK,CAAC,OAAO,CAAC,EAAC,IAAI,EAAE,kBAAkB,EAAE,SAAS,EAAE,KAAK,EAAC,CAAC,CAAC;QAC5D,MAAM,EAAE,GAAG,QAAQ,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,EAAE,EAAE,CAAC,EAAE,CAAC,EAAE,EAAE,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC;QACzD,MAAM,EAAE,GAAG,QAAQ,CAAC,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC;QACtC,MAAM,OAAO,GAAG,MAAM,KAAK,CAAC,GAAG,CAAC,EAAE,EAAE,EAAE,EAAE,EAAC,MAAM,EAAE,CAAC,EAAC,CAAC,CAAC;QACrD,SAAS,CAAC,iBAAiB,CAAC,OAAO,CAAC,OAAO,CAAC,IAAgB,EAAE;YAC5D,kBAAkB,EAAE,kBAAkB,EAAE,kBAAkB;YAC1D,kBAAkB,EAAE,kBAAkB;SACvC,CAAC,CAAC;IACL,CAAC,CAAC,CAAC;IAEH,yBAAyB;IACzB,QAAQ;IACR,qBAAqB;IACrB,0BAA0B;IAC1B,EAAE;IACF,gCAAgC;IAChC,iEAAiE;IACjE,qEAAqE;IACrE,iEAAiE;IACjE,6CAA6C;IAC7C,EAAE;IACF,gEAAgE;IAChE,2EAA2E;IAC3E,mDAAmD;IACnD,yBAAyB;IACzB,MAAM;IACN,EAAE,CAAC,mDAAmD,EAAE,KAAK,IAAI,EAAE;QACjE,MAAM,KAAK,GAAG,GAAG,CAAC,UAAU,EAAE,CAAC;QAC/B,KAAK,CAAC,GAAG,CAAC,GAAG,CAAC,MAAM,CAAC,KAAK,CACtB,EAAC,KAAK,EAAE,EAAE,EAAE,iBAAiB,EAAE,MAAM,EAAE,UAAU,EAAE,CAAC,CAAC,CAAC,EAAC,CAAC,CAAC,CAAC;QAC9D,KAAK,CAAC,GAAG,CAAC,GAAG,CAAC,MAAM,CAAC,kBAAkB,EAAE,CAAC,CAAC;QAC3C,KAAK,CAAC,GAAG,CAAC,GAAG,CAAC,MAAM,CAAC,KAAK,CAAC,EAAC,KAAK,EAAE,CAAC,EAAE,iBAAiB,EAAE,MAAM,EAAC,CAAC,CAAC,CAAC;QACnE,KAAK,CAAC,OAAO,CAAC,EAAC,IAAI,EAAE,kBAAkB,EAAE,SAAS,EAAE,KAAK,EAAC,CAAC,CAAC;QAC5D,MAAM,EAAE,GAAG,QAAQ,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,EAAE,EAAE,CAAC,EAAE,CAAC,EAAE,EAAE,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC;QACpE,MAAM,EAAE,GAAG,QAAQ,CAAC,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC;QAC3C,MAAM,OAAO,GAAG,MAAM,KAAK,CAAC,GAAG,CAAC,EAAE,EAAE,EAAE,EAAE,EAAC,MAAM,EAAE,CAAC,EAAC,CAAC,CAAC;QACrD,SAAS,CAAC,iBAAiB,CAAC,OAAO,CAAC,OAAO,CAAC,IAAgB,EAAE;YAC5D,GAAG,EAAE,kBAAkB,EAAE,kBAAkB,EAAE,kBAAkB;YAC/D,kBAAkB;SACnB,CAAC,CAAC;IACL,CAAC,CAAC,CAAC;IAEH,EAAE,CAAC,0BAA0B,EAAE,KAAK,IAAI,EAAE;QACxC,MAAM,KAAK,GAAG,GAAG,CAAC,UAAU,EAAE,CAAC;QAC/B,KAAK,CAAC,GAAG,CAAC,GAAG,CAAC,MAAM,CAAC,kBAAkB,CAAC,EAAC,UAAU,EAAE,CAAC,CAAC,CAAC,EAAC,CAAC,CAAC,CAAC;QAC5D,KAAK,CAAC,GAAG,CAAC,GAAG,CAAC,MAAM,CAAC,KAAK,CAAC,EAAC,KAAK,EAAE,CAAC,EAAE,iBAAiB,EAAE,MAAM,EAAC,CAAC,CAAC,CAAC;QACnE,KAAK,CAAC,OAAO,CAAC,EAAC,IAAI,EAAE,kBAAkB,EAAE,SAAS,EAAE,KAAK,EAAC,CAAC,CAAC;QAC5D,MAAM,EAAE,GAAG,QAAQ,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,EAAE,EAAE,CAAC,EAAE,CAAC,EAAE,EAAE,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC;QACpE,MAAM,EAAE,GAAG,QAAQ,CAAC,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC;QAC3C,MAAM,KAAK,CAAC,GAAG,CAAC,EAAE,EAAE,EAAE,EAAE,EAAC,MAAM,EAAE,CAAC,EAAC,CAAC,CAAC,CAAE,WAAW;QAElD,MAAM,WAAW,GAAG,MAAM,EAAE,CAAC,UAAU,CAAC;QACxC,MAAM,KAAK,CAAC,GAAG,CAAC,EAAE,EAAE,EAAE,EAAE,EAAC,MAAM,EAAE,CAAC,EAAC,CAAC,CAAC;QACrC,MAAM,CAAC,MAAM,EAAE,CAAC,UAAU,CAAC,CAAC,OAAO,CAAC,WAAW,CAAC,CAAC;IACnD,CAAC,CAAC,CAAC;IAEH,yBAAyB;IACzB,QAAQ;IACR,qBAAqB;IACrB,0BAA0B;IAC1B,EAAE;IACF,gCAAgC;IAChC,oEAAoE;IACpE,uCAAuC;IACvC,iEAAiE;IACjE,6CAA6C;IAC7C,EAAE;IACF,oEAAoE;IACpE,2EAA2E;IAC3E,gEAAgE;IAChE,MAAM;IACN,EAAE,CAAC,4BAA4B,EAAE,KAAK,IAAI,EAAE;QAC1C,MAAM,KAAK,GAAG,GAAG,CAAC,UAAU,EAAE,CAAC;QAC/B,KAAK,CAAC,GAAG,CAAC,GAAG,CAAC,MAAM,CAAC,kBAAkB,CAAC,EAAC,UAAU,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAC,CAAC,CAAC,CAAC;QAC/D,KAAK,CAAC,GAAG,CAAC,GAAG,CAAC,MAAM,CAAC,OAAO,EAAE,CAAC,CAAC;QAChC,KAAK,CAAC,GAAG,CAAC,GAAG,CAAC,MAAM,CAAC,KAAK,CAAC,EAAC,KAAK,EAAE,CAAC,EAAE,iBAAiB,EAAE,MAAM,EAAC,CAAC,CAAC,CAAC;QACnE,KAAK,CAAC,OAAO,CAAC,EAAC,IAAI,EAAE,kBAAkB,EAAE,SAAS,EAAE,KAAK,EAAC,CAAC,CAAC;QAC5D,MAAM,EAAE,GAAG,QAAQ,CAAC,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,EAAE,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,EAAE,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC;QACxE,MAAM,EAAE,GAAG,QAAQ,CAAC,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC;QACjC,MAAM,OAAO,GAAG,MAAM,KAAK,CAAC,GAAG,CAAC,EAAE,EAAE,EAAE,EAAE,EAAC,MAAM,EAAE,CAAC,EAAC,CAAC,CAAC;QACrD,SAAS,CAAC,iBAAiB,CAAC,OAAO,CAAC,OAAO,CAAC,IAAgB,EAAE;YAC5D,GAAG,EAAE,mBAAmB,EAAE,mBAAmB,EAAE,iBAAiB;YAChE,mBAAmB;SACpB,CAAC,CAAC;IACL,CAAC,CAAC,CAAC;IAEH,yBAAyB;IACzB,QAAQ;IACR,qBAAqB;IACrB,0BAA0B;IAC1B,EAAE;IACF,gCAAgC;IAChC,6EAA6E;IAC7E,6CAA6C;IAC7C,iEAAiE;IACjE,6CAA6C;IAC7C,EAAE;IACF,oEAAoE;IACpE,2EAA2E;IAC3E,gEAAgE;IAChE,MAAM;IACN,EAAE,CAAC,gCAAgC,EAAE,KAAK,IAAI,EAAE;QAC9C,MAAM,KAAK,GAAG,GAAG,CAAC,UAAU,EAAE,CAAC;QAC/B,KAAK,CAAC,GAAG,CACL,GAAG,CAAC,MAAM,CAAC,kBAAkB,CAAC,EAAC,UAAU,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,IAAI,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAC,CAAC,CAAC,CAAC;QACzE,KAAK,CAAC,GAAG,CAAC,GAAG,CAAC,MAAM,CAAC,OAAO,EAAE,CAAC,CAAC;QAChC,KAAK,CAAC,GAAG,CAAC,GAAG,CAAC,MAAM,CAAC,KAAK,CAAC,EAAC,KAAK,EAAE,CAAC,EAAE,iBAAiB,EAAE,MAAM,EAAC,CAAC,CAAC,CAAC;QACnE,KAAK,CAAC,OAAO,CAAC,EAAC,IAAI,EAAE,kBAAkB,EAAE,SAAS,EAAE,KAAK,EAAC,CAAC,CAAC;QAC5D,MAAM,EAAE,GAAG,QAAQ,CAAC,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,EAAE,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,EAAE,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC;QACxE,MAAM,EAAE,GAAG,QAAQ,CAAC,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC;QACjC,MAAM,OAAO,GAAG,MAAM,KAAK,CAAC,GAAG,CAAC,EAAE,EAAE,EAAE,EAAE,EAAC,MAAM,EAAE,CAAC,EAAC,CAAC,CAAC;QACrD,SAAS,CAAC,iBAAiB,CAAC,OAAO,CAAC,OAAO,CAAC,IAAgB,EAAE;YAC5D,GAAG,EAAE,kBAAkB,EAAE,mBAAmB,EAAE,mBAAmB;YACjE,kBAAkB;SACnB,CAAC,CAAC;IACL,CAAC,CAAC,CAAC;IAEH,yBAAyB;IACzB,QAAQ;IACR,qBAAqB;IACrB,0BAA0B;IAC1B,EAAE;IACF,gCAAgC;IAChC,+CAA+C;IAC/C,iEAAiE;IACjE,qCAAqC;IACrC,6BAA6B;IAC7B,kDAAkD;IAClD,EAAE;IACF,0EAA0E;IAC1E,kCAAkC;IAClC,oBAAoB;IACpB,EAAE;IACF,gCAAgC;IAChC,6DAA6D;IAC7D,0BAA0B;IAC1B,yBAAyB;IACzB,YAAY;IACZ,MAAM;IACN,EAAE,CAAC,uBAAuB,EAAE,GAAG,EAAE;QAC/B,MAAM,KAAK,GAAG,GAAG,CAAC,UAAU,EAAE,CAAC;QAC/B,MAAM,cAAc,GAAG,GAAG,CAAC,MAAM,CAAC,SAAS,CAAC;YAC1C,QAAQ,EAAE,CAAC;YACX,SAAS,EAAE,CAAC;YACZ,WAAW,EAAE,CAAC;YACd,QAAQ,EAAE,IAAI;YACd,qBAAqB,EAAE,MAAM;SAC9B,CAAC,CAAC;QACH,KAAK,CAAC,GAAG,CAAC,cAAc,CAAC,CAAC;QAC1B,sEAAsE;QACtE,KAAK,CAAC,GAAG,CAAC,GAAG,CAAC,MAAM,CAAC,kBAAkB,EAAE,CAAC,CAAC;QAE3C,MAAM,EAAE,GACJ,QAAQ,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC;QACvE,KAAK,CAAC,OAAO,CAAC,EAAE,CAAC,CAAC,CAAE,gDAAgD;QAEpE,cAAc,CAAC,UAAU,CACrB,CAAC,QAAQ,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC;QAC9D,MAAM,EAAE,GAAG,KAAK,CAAC,OAAO,CAAC,EAAE,CAAW,CAAC;QACvC,kBAAkB,CACd,EAAE,EAAE,QAAQ,CAAC;YACX;gBACE,CAAC,CAAC,SAAS,EAAE,CAAC,EAAE,SAAS,CAAC,EAAE,CAAC,CAAC,SAAS,EAAE,CAAC,EAAE,SAAS,CAAC;gBACtD,CAAC,CAAC,SAAS,EAAE,CAAC,EAAE,SAAS,CAAC,EAAE,CAAC,CAAC,SAAS,EAAE,CAAC,EAAE,SAAS,CAAC;aACvD;YACD;gBACE,CAAC,SAAS,EAAE,CAAC,EAAE,CAAC,SAAS,CAAC,EAAE,CAAC,CAAC,SAAS,EAAE,CAAC,EAAE,SAAS,CAAC;gBACtD,CAAC,CAAC,SAAS,EAAE,CAAC,EAAE,SAAS,CAAC,EAAE,CAAC,CAAC,SAAS,EAAE,CAAC,EAAE,SAAS,CAAC;aACvD;YACD;gBACE,CAAC,SAAS,EAAE,CAAC,EAAE,CAAC,SAAS,CAAC,EAAE,CAAC,CAAC,EAAE,SAAS,EAAE,CAAC,SAAS,CAAC;gBACtD,CAAC,CAAC,SAAS,EAAE,CAAC,EAAE,SAAS,CAAC,EAAE,CAAC,CAAC,SAAS,EAAE,CAAC,EAAE,SAAS,CAAC;aACvD;YACD;gBACE,CAAC,SAAS,EAAE,CAAC,EAAE,CAAC,SAAS,CAAC,EAAE,CAAC,CAAC,EAAE,SAAS,EAAE,CAAC,SAAS,CAAC;gBACtD,CAAC,SAAS,EAAE,CAAC,SAAS,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,SAAS,EAAE,CAAC,EAAE,SAAS,CAAC;aACvD;SACF,CAAC,CAAC,CAAC;IACV,CAAC,CAAC,CAAC;AACL,CAAC,CAAC,CAAC","sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\n/**\n * Unit tests for normalization layers.\n */\n\nimport {dispose, memory, onesLike, scalar, Tensor, tensor1d, tensor2d, tensor3d, tensor4d, test_util, train, zeros, zerosLike} from '@tensorflow/tfjs-core';\n\nimport {SymbolicTensor} from '../engine/topology';\nimport * as tfl from '../index';\nimport {convertPythonicToTs, convertTsToPythonic} from '../utils/serialization_utils';\nimport {describeMathCPU, describeMathCPUAndGPU, expectTensorsClose} from '../utils/test_utils';\n\nimport {batchNormalization, normalizeBatchInTraining} from './normalization';\n\ndescribeMathCPUAndGPU('normalizeBatchInTraining', () => {\n  // The reference values for assertion below can be obtained with Python code\n  // as the following:\n  // ```python\n  // import keras\n  // import numpy as np\n  // import tensorflow as tf\n  //\n  // with tf.Session() as sess:\n  //   x = tf.Variable(np.array(\n  //       [[1, 2, 3, 4], [2, 4, 6, 8], [12, 11, 10, 9]], dtype=np.float32))\n  //   gamma = tf.Variable(np.array([1, 1, 1, 1], dtype=np.float32))\n  //   beta = tf.Variable(np.array([0, 0, 0, 0], dtype=np.float32))\n  //   reduction_axes = [0]\n  //   normed, mean, variance = keras.backend.normalize_batch_in_training(\n  //       x, gamma, beta, reduction_axes)\n  //   print(normed)\n  //   print(mean)\n  //   print(variance)\n  // ```\n\n  it('2D, no broadcasting', () => {\n    const x = tensor2d([[1, 2, 3, 4], [2, 4, 6, 8], [12, 11, 10, 9]], [3, 4]);\n    const gamma = tensor1d([1, 1, 1, 1]);\n    const beta = tensor1d([0, 0, 0, 0]);\n    const reductionAxes = [0];\n    const [normed, mean, variance] =\n        normalizeBatchInTraining(x, gamma, beta, reductionAxes);\n    expectTensorsClose(\n        normed,\n        tensor2d(\n            [\n              [-0.805371, -0.9502233, -1.1624058, -1.3885813],\n              [-0.6040282, -0.4319197, -0.11624074, 0.46286058],\n              [1.4093992, 1.3821429, 1.2786462, 0.92572117]\n            ],\n            [3, 4]));\n    expectTensorsClose(mean, tensor1d([5.0, 5.6666665, 6.3333335, 7.0]));\n    expectTensorsClose(\n        variance, tensor1d([24.666666, 14.888889, 8.222222, 4.6666665]));\n  });\n\n  it('3D, no broadcasting', () => {\n    const x = tensor3d(\n        [[[1, 2], [3, 4]], [[2, 4], [6, 8]], [[12, 11], [10, 9]]], [3, 2, 2]);\n    const gamma = tensor1d([1, 1]);\n    const beta = tensor1d([0, 0]);\n    const reductionAxes = [0, 1];\n    const [normed, mean, variance] =\n        normalizeBatchInTraining(x, gamma, beta, reductionAxes);\n    expectTensorsClose(\n        normed,\n        tensor3d(\n            [\n              [[-1.1355163, -1.3552775], [-0.6488664, -0.7297648]],\n              [[-0.8921913, -0.7297648], [0.08110833, 0.5212605]],\n              [[1.5410578, 1.4595294], [1.0544081, 0.8340168]]\n            ],\n            [3, 2, 2]));\n    expectTensorsClose(mean, tensor1d([5.6666665, 6.3333335]));\n    expectTensorsClose(variance, tensor1d([16.88889, 10.222222]));\n  });\n\n  it('3D, broadcasting', () => {\n    const x = tensor3d(\n        [[[1, 2], [3, 4]], [[2, 4], [6, 8]], [[12, 11], [10, 9]]], [3, 2, 2]);\n    const gamma = tensor2d([[1, 1], [1, 1]], [2, 2]);\n    const beta = tensor2d([[0, 0], [0, 0]], [2, 2]);\n    const reductionAxes = [0];\n    const [normed, mean, variance] =\n        normalizeBatchInTraining(x, gamma, beta, reductionAxes);\n    expectTensorsClose(\n        normed,\n        tensor3d(\n            [\n              [[-0.805371, -0.9502233], [-1.1624058, -1.3885813]],\n              [[-0.6040282, -0.4319197], [-0.11624074, 0.46286058]],\n              [[1.4093992, 1.3821429], [1.2786462, 0.92572117]]\n            ],\n            [3, 2, 2]));\n    expectTensorsClose(\n        mean, tensor2d([[5, 5.6666665], [6.3333335, 7]], [2, 2]));\n    expectTensorsClose(\n        variance,\n        tensor2d([[24.666666, 14.888889], [8.222222, 4.6666665]], [2, 2]));\n  });\n\n  it('4D, broadcasting', () => {\n    const x = tensor4d(\n        [[[[1, 2], [3, 4]], [[2, 4], [6, 8]], [[12, 11], [10, 9]]]],\n        [1, 3, 2, 2]);\n    const gamma = tensor2d([[1, 1], [1, 1]], [2, 2]);\n    const beta = tensor2d([[0, 0], [0, 0]], [2, 2]);\n    const reductionAxes = [0, 1];\n    const [normed, mean, variance] =\n        normalizeBatchInTraining(x, gamma, beta, reductionAxes);\n    expectTensorsClose(\n        normed,\n        tensor4d(\n            [[\n              [[-0.805371, -0.9502233], [-1.1624058, -1.3885813]],\n              [[-0.6040282, -0.4319197], [-0.11624074, 0.46286058]],\n              [[1.4093992, 1.3821429], [1.2786462, 0.92572117]]\n            ]],\n            [1, 3, 2, 2]));\n    expectTensorsClose(\n        mean, tensor2d([[5, 5.6666665], [6.3333335, 7]], [2, 2]));\n    expectTensorsClose(\n        variance,\n        tensor2d([[24.666666, 14.888889], [8.222222, 4.6666665]], [2, 2]));\n  });\n});\n\ndescribeMathCPUAndGPU('batchNormalization', () => {\n  it('2D, no broadcast, no gamma, no beta', () => {\n    const x = tensor2d([[10, 20], [30, 40]], [2, 2]);\n    const mean = tensor2d([[5, 5], [5, 5]], [2, 2]);\n    const variance = tensor2d([[4, 16], [4, 16]], [2, 2]);\n    expectTensorsClose(\n        batchNormalization(x, mean, variance, null, null, 0),\n        tensor2d([[2.5, 3.75], [12.5, 8.75]], [2, 2]));\n  });\n  it('2D, no broadcast, no gamma, no beta, custom epsilon', () => {\n    const x = tensor2d([[30, 30], [60, 60]], [2, 2]);\n    const mean = tensor2d([[0, 0], [0, 0]], [2, 2]);\n    const variance = tensor2d([[7, 7], [7, 7]], [2, 2]);\n    expectTensorsClose(\n        batchNormalization(x, mean, variance, null, null, 2),\n        tensor2d([[10, 10], [20, 20]], [2, 2]));\n  });\n  it('2D, no broadcast, gamma, no beta', () => {\n    const x = tensor2d([[10, 20], [30, 40]], [2, 2]);\n    const mean = tensor2d([[5, 5], [5, 5]], [2, 2]);\n    const variance = tensor2d([[4, 16], [4, 16]], [2, 2]);\n    const gamma = tensor2d([[1, 2], [3, 4]], [2, 2]);\n    expectTensorsClose(\n        batchNormalization(x, mean, variance, null, gamma, 0),\n        tensor2d([[2.5, 7.5], [37.5, 35]], [2, 2]));\n  });\n  it('2D, no broadcast, gamma, beta', () => {\n    const x = tensor2d([[10, 20], [30, 40]], [2, 2]);\n    const mean = tensor2d([[5, 5], [5, 5]], [2, 2]);\n    const variance = tensor2d([[4, 16], [4, 16]], [2, 2]);\n    const gamma = tensor2d([[1, 2], [3, 4]], [2, 2]);\n    const beta = tensor2d([[-1, -1], [-2, -2]], [2, 2]);\n    expectTensorsClose(\n        batchNormalization(x, mean, variance, beta, gamma, 0),\n        tensor2d([[1.5, 6.5], [35.5, 33]], [2, 2]));\n  });\n  it('2D, broadcast, gamma, beta', () => {\n    const x = tensor2d([[10, 20], [30, 40]], [2, 2]);\n    const mean = tensor1d([2, 5]);\n    const variance = tensor1d([1, 4]);\n    const gamma = tensor1d([3, 4]);\n    const beta = tensor1d([-1, -2]);\n    expectTensorsClose(\n        batchNormalization(x, mean, variance, beta, gamma, 0),\n        tensor2d([[23, 28], [83, 68]], [2, 2]));\n  });\n  it('3D, no broadcast, no gamma, no beta', () => {\n    const x = tensor3d([[[10, 20], [30, 40]], [[10, 20], [30, 40]]], [2, 2, 2]);\n    const mean = tensor3d([[[5, 5], [5, 5]], [[5, 5], [5, 5]]], [2, 2, 2]);\n    const variance =\n        tensor3d([[[4, 16], [4, 16]], [[16, 25], [16, 25]]], [2, 2, 2]);\n    expectTensorsClose(\n        batchNormalization(x, mean, variance, null, null, 0),\n        tensor3d(\n            [[[2.5, 3.75], [12.5, 8.75]], [[1.25, 3], [6.25, 7]]], [2, 2, 2]));\n  });\n  it('3D, no broadcast, gamma, beta', () => {\n    const x = tensor3d([[[10, 20], [30, 40]], [[10, 20], [30, 40]]], [2, 2, 2]);\n    const mean = tensor3d([[[5, 5], [5, 5]], [[5, 5], [5, 5]]], [2, 2, 2]);\n    const variance =\n        tensor3d([[[4, 16], [4, 16]], [[16, 25], [16, 25]]], [2, 2, 2]);\n    const gamma = tensor3d([[[2, 2], [2, 2]], [[4, 4], [4, 4]]], [2, 2, 2]);\n    const beta =\n        tensor3d([[[-1, -1], [-2, -2]], [[-1, -1], [-2, -2]]], [2, 2, 2]);\n    expectTensorsClose(\n        batchNormalization(x, mean, variance, beta, gamma, 0),\n        tensor3d([[[4, 6.5], [23, 15.5]], [[4, 11], [23, 26]]], [2, 2, 2]));\n  });\n  it('3D, broadcast, gamma, beta', () => {\n    const x = tensor3d([[[10, 20], [30, 40]], [[10, 20], [30, 40]]], [2, 2, 2]);\n    const mean = tensor1d([5, 5]);\n    const variance = tensor1d([4, 16]);\n    const gamma = tensor1d([2, 4]);\n    const beta = tensor1d([-1, -2]);\n    expectTensorsClose(\n        batchNormalization(x, mean, variance, beta, gamma, 0),\n        tensor3d([[[4, 13], [24, 33]], [[4, 13], [24, 33]]], [2, 2, 2]));\n  });\n  it('4D, no broadcast, no gamma, no beta', () => {\n    const x = tensor4d(\n        [\n          [[[10, 20], [30, 40]], [[10, 20], [30, 40]]],\n          [[[-10, -20], [-30, -40]], [[-10, -20], [-30, -40]]]\n        ],\n        [2, 2, 2, 2]);\n    const mean = tensor4d(\n        [\n          [[[5, 5], [5, 5]], [[5, 5], [5, 5]]],\n          [[[-5, -5], [-5, -5]], [[-5, -5], [-5, -5]]]\n        ],\n        [2, 2, 2, 2]);\n    const variance = tensor4d(\n        [\n          [[[4, 16], [4, 16]], [[16, 25], [16, 25]]],\n          [[[4, 16], [4, 16]], [[16, 25], [16, 25]]]\n        ],\n        [2, 2, 2, 2]);\n    expectTensorsClose(\n        batchNormalization(x, mean, variance, null, null, 0),\n        tensor4d(\n            [\n              [[[2.5, 3.75], [12.5, 8.75]], [[1.25, 3], [6.25, 7]]],\n              [[[-2.5, -3.75], [-12.5, -8.75]], [[-1.25, -3], [-6.25, -7]]]\n            ],\n            [2, 2, 2, 2]));\n  });\n  it('4D, no broadcast, gamma, beta', () => {\n    const x = tensor4d(\n        [\n          [[[10, 20], [30, 40]], [[10, 20], [30, 40]]],\n          [[[-10, -20], [-30, -40]], [[-10, -20], [-30, -40]]]\n        ],\n        [2, 2, 2, 2]);\n    const mean = tensor4d(\n        [\n          [[[5, 5], [5, 5]], [[5, 5], [5, 5]]],\n          [[[-5, -5], [-5, -5]], [[-5, -5], [-5, -5]]]\n        ],\n        [2, 2, 2, 2]);\n    const variance = tensor4d(\n        [\n          [[[4, 16], [4, 16]], [[16, 25], [16, 25]]],\n          [[[4, 16], [4, 16]], [[16, 25], [16, 25]]]\n        ],\n        [2, 2, 2, 2]);\n    const gamma = tensor4d(\n        [\n          [[[2, 2], [2, 2]], [[4, 4], [4, 4]]],\n          [[[2, 2], [2, 2]], [[4, 4], [4, 4]]]\n        ],\n        [2, 2, 2, 2]);\n    const beta = tensor4d(\n        [\n          [[[-1, -1], [-2, -2]], [[-1, -1], [-2, -2]]],\n          [[[1, 1], [2, 2]], [[1, 1], [2, 2]]]\n        ],\n        [2, 2, 2, 2]);\n    expectTensorsClose(\n        batchNormalization(x, mean, variance, beta, gamma, 0),\n        tensor4d(\n            [\n              [[[4, 6.5], [23, 15.5]], [[4, 11], [23, 26]]],\n              [[[-4, -6.5], [-23, -15.5]], [[-4, -11], [-23, -26]]]\n            ],\n            [2, 2, 2, 2]));\n  });\n  it('4D, broadcast, gamma, beta', () => {\n    const x = tensor4d(\n        [[[[10, 20], [30, 40]]], [[[10, 20], [30, 40]]]], [2, 1, 2, 2]);\n    const mean = tensor1d([5, 5]);\n    const variance = tensor1d([4, 16]);\n    const gamma = tensor1d([2, 4]);\n    const beta = tensor1d([-1, -2]);\n    expectTensorsClose(\n        batchNormalization(x, mean, variance, beta, gamma, 0),\n        tensor4d([[[[4, 13], [24, 33]]], [[[4, 13], [24, 33]]]], [2, 1, 2, 2]));\n  });\n});\n\ndescribeMathCPU('BatchNormalization Layers: Symbolic', () => {\n  const validInputShapes = [[4, 6], [2, 3, 4], [2, 3, 4, 5]];\n  for (const inputShape of validInputShapes) {\n    const testTitle = `shape=${JSON.stringify(inputShape)}`;\n    it(testTitle, () => {\n      const x = new SymbolicTensor('float32', inputShape, null, [], null);\n      const layer = tfl.layers.batchNormalization({});\n      const y = layer.apply(x) as SymbolicTensor;\n      expect(y.dtype).toEqual(x.dtype);\n      expect(y.shape).toEqual(x.shape);\n    });\n  }\n\n  it('Undetermined dim axis leads to ValueError', () => {\n    const x = new SymbolicTensor('float32', [null, 2, 3], null, [], null);\n    const layer = tfl.layers.batchNormalization({axis: 0});\n    expect(() => layer.apply(x))\n        .toThrowError(\n            /Axis 0 of input tensor should have a defined dimension.*/);\n  });\n\n  it('batchNormalization constructor works without arg', () => {\n    const layer = tfl.layers.batchNormalization();\n    expect(layer.getConfig().axis).toEqual(-1);\n  });\n});\n\ndescribeMathCPUAndGPU('BatchNormalization Layers: Tensor', () => {\n  const dimensions = [2, 3, 4];\n  const axisValues = [0, -1];\n\n  for (const dim of dimensions) {\n    for (const axis of axisValues) {\n      const testTitle = `Inference, ${dim}D, axis=${axis}`;\n      it(testTitle, () => {\n        const layer = tfl.layers.batchNormalization({axis});\n        let x: Tensor;\n        if (dim === 2) {\n          x = tensor2d([[1, 2], [3, 4]], [2, 2]);\n        } else if (dim === 3) {\n          x = tensor3d([[[1, 2], [3, 4]], [[-1, -2], [-3, -4]]], [2, 2, 2]);\n        } else if (dim === 4) {\n          x = tensor4d(\n              [\n                [[[1, 2], [3, 4]], [[-1, -2], [-3, -4]]],\n                [[[-1, -2], [-3, -4]], [[1, 2], [3, 4]]]\n              ],\n              [2, 2, 2, 2]);\n        }\n        const y = layer.apply(x, {training: false}) as Tensor;\n        expectTensorsClose(y, x, 0.01);\n      });\n    }\n  }\n\n  it('no center', () => {\n    const layer = tfl.layers.batchNormalization({center: false, axis: 0});\n    const x = tensor2d([[1, 2], [3, 4]], [2, 2]);\n    expectTensorsClose(layer.apply(x) as Tensor, x, 0.01);\n    expect(layer.getWeights().length).toEqual(3);\n    // Firt weight is gamma.\n    expectTensorsClose(layer.getWeights()[0], onesLike(layer.getWeights()[0]));\n    // Second weight is moving mean.\n    expectTensorsClose(layer.getWeights()[1], zerosLike(layer.getWeights()[1]));\n    // Third weight is moving variance.\n    expectTensorsClose(layer.getWeights()[2], onesLike(layer.getWeights()[2]));\n  });\n\n  it('no scale', () => {\n    const layer = tfl.layers.batchNormalization({scale: false, axis: 0});\n    const x = tensor2d([[1, 2], [3, 4]], [2, 2]);\n    expectTensorsClose(layer.apply(x) as Tensor, x, 0.01);\n    expect(layer.getWeights().length).toEqual(3);\n    // Firt weight is beta.\n    expectTensorsClose(layer.getWeights()[0], zerosLike(layer.getWeights()[0]));\n    // Second weight is moving mean.\n    expectTensorsClose(layer.getWeights()[1], zerosLike(layer.getWeights()[1]));\n    // Third weight is moving variance.\n    expectTensorsClose(layer.getWeights()[2], onesLike(layer.getWeights()[2]));\n  });\n\n  it('no center, no scale', () => {\n    const layer = tfl.layers.batchNormalization({scale: false, center: false});\n    const x = tensor2d([[1, 2], [3, 4]], [2, 2]);\n    expectTensorsClose(layer.apply(x) as Tensor, x, 0.01);\n    expect(layer.getWeights().length).toEqual(2);\n    // First weight is moving mean.\n    expectTensorsClose(layer.getWeights()[0], zerosLike(layer.getWeights()[0]));\n    // Second weight is moving variance.\n    expectTensorsClose(layer.getWeights()[1], onesLike(layer.getWeights()[1]));\n  });\n\n  // Use the following Python code to get the reference values for assertion:\n  // ```python\n  // from tensorflow import keras\n  // import numpy as np\n  //\n  // layer1 = keras.layers.BatchNormalization(input_shape=(4,))\n  // model = keras.Sequential([layer1])\n  //\n  // model.compile(loss='mean_squared_error', optimizer='sgd')\n  //\n  // xs = np.array(\n  //     [[1, 2, 3, 4], [2, 4, 6, 8], [12, 11, 10, 9]], dtype=np.float32)\n  // ys = np.zeros([3, 4])\n  // print(layer1.get_weights())\n  // history = model.fit(xs, ys, epochs=2, batch_size=3)\n  // print(history.history)\n  // print(layer1.get_weights())\n  // ```\n  it('Fit: 2D, BatchNorm Layer Only', async () => {\n    const layer1 = tfl.layers.batchNormalization({inputShape: [4]});\n    const model = tfl.sequential({layers: [layer1]});\n    model.compile({loss: 'meanSquaredError', optimizer: 'sgd'});\n\n    const xs1 = tensor2d([[1, 2, 3, 4], [2, 4, 6, 8], [12, 11, 10, 9]], [3, 4]);\n    const ys = zeros([3, 4]);\n    const history = await model.fit(xs1, ys, {epochs: 2, batchSize: 3});\n    expect(history.history['loss'][0]).toBeCloseTo(0.9998891353607178);\n    expect(history.history['loss'][1]).toBeCloseTo(0.9899163246154785);\n    const gammaValue = layer1.getWeights()[0];\n    expectTensorsClose(\n        gammaValue, [0.9900254, 0.9900257, 0.9900262, 0.9900271]);\n    const betaValue = layer1.getWeights()[1];\n    expectTensorsClose(\n        betaValue,\n        [2.9802322e-10, 1.4901161e-10, 9.1269614e-10, -7.4505802e-10]);\n    const movingMeanValue = layer1.getWeights()[2];\n    expectTensorsClose(\n        movingMeanValue, [0.09949999, 0.11276666, 0.12603334, 0.13929999]);\n    const movingVarianceValue = layer1.getWeights()[3];\n    expectTensorsClose(\n        movingVarianceValue, [1.4709667, 1.2763889, 1.1437222, 1.0729666]);\n  });\n\n  // Use the following Python code to get the reference values for\n  // assertion:\n  // ```python\n  // from tensorflow import keras\n  // import numpy as np\n  //\n  // layer1 = keras.layers.Dense(\n  //     4, kernel_initializer='ones', use_bias=False, input_shape=(4,))\n  // layer2 = keras.layers.BatchNormalization()\n  // layer3 = keras.layers.Dense(1, kernel_initializer='ones',\n  //                             use_bias=False)\n  // model = keras.Sequential([layer1, layer2, layer3])\n  //\n  // optimizer = keras.optimizers.sgd(lr=0.1)\n  // model.compile(loss='mean_squared_error', optimizer=optimizer)\n  //\n  // xs = np.array([[1, 2, 3, 4], [2, 4, 6, 8], [12, 11, 10, 9]],\n  //                dtype=np.float32)\n  // ys = np.zeros([3, 1])\n  // history = model.fit(xs, ys, epochs=3, batch_size=3)\n  //\n  // print(history.history)\n  // print(layer1.get_weights())\n  // print(layer2.get_weights())\n  // print(layer3.get_weights())\n  // ```\n  it('Fit: 2D, BatchNorm Layer between two Dense Layers', async () => {\n    const layer1 = tfl.layers.dense(\n        {units: 4, kernelInitializer: 'ones', useBias: false, inputShape: [4]});\n    const layer2 = tfl.layers.batchNormalization({inputShape: [4]});\n    const layer3 =\n        tfl.layers.dense({units: 1, kernelInitializer: 'ones', useBias: false});\n    const model = tfl.sequential({layers: [layer1, layer2, layer3]});\n\n    const optimizer = train.sgd(0.1);\n    model.compile({loss: 'meanSquaredError', optimizer});\n\n    const xs1 = tensor2d([[1, 2, 3, 4], [2, 4, 6, 8], [12, 11, 10, 9]], [3, 4]);\n    const ys = zeros([3, 1]);\n    const history = await model.fit(xs1, ys, {epochs: 3, batchSize: 3});\n    expect(history.history['loss'][0]).toBeCloseTo(15.999907493591309);\n    expect(history.history['loss'][1]).toBeCloseTo(0.025602197274565697);\n    expect(history.history['loss'][2]).toBeCloseTo(0.022478966042399406);\n    const dense1KernelValue = layer1.getWeights()[0];\n    expectTensorsClose(\n        dense1KernelValue,\n        tensor2d(\n            [\n              [0.99999833, 0.99999833, 0.99999833, 0.99999833],\n              [0.9999987, 0.9999987, 0.9999987, 0.9999987],\n              [0.999999, 0.999999, 0.999999, 0.999999],\n              [0.99999934, 0.99999934, 0.99999934, 0.99999934]\n            ],\n            [4, 4]));\n    const gammaValue = layer2.getWeights()[0];\n    expectTensorsClose(\n        gammaValue, [0.18779878, 0.18779878, 0.18779878, 0.18779878]);\n    const betaValue = layer2.getWeights()[1];\n    expectTensorsClose(\n        betaValue,\n        [5.5367128e-08, 5.5367128e-08, 5.5367128e-08, 5.5367128e-08]);\n    const movingMeanValue = layer2.getWeights()[2];\n    // TODO(cais): Update this to tf.keras.\n    expectTensorsClose(\n        movingMeanValue, [0.7128234, 0.7128234, 0.7128234, 0.7128234]);\n    const movingVarianceValue = layer2.getWeights()[3];\n    expectTensorsClose(\n        movingVarianceValue, [6.276868, 6.276868, 6.276868, 6.276868]);\n    const dense2KernelValue = layer3.getWeights()[0];\n    expectTensorsClose(\n        dense2KernelValue,\n        tensor2d(\n            [[0.18779878], [0.18779878], [0.18779878], [0.18779878]], [4, 1]));\n  });\n\n  // Python reference code:\n  // ```python\n  // import numpy as np\n  // from tensorflow import keras\n  //\n  // model = keras.Sequential()\n  // model.add(keras.layers.Conv2D(\n  //     4,\n  //     2,\n  //     kernel_initializer='ones',\n  //     bias_initializer='zeros',\n  //     input_shape=[5, 5, 1]))\n  // model.add(keras.layers.BatchNormalization())\n  // model.add(keras.layers.Flatten())\n  // model.add(keras.layers.Dense(\n  //     1,\n  //     kernel_initializer='ones',\n  //     bias_initializer='zeros'))\n  //\n  // model.compile(loss='mse', optimizer='sgd')\n  //\n  // xs = np.arange(2 * 5 * 5 * 1).reshape([2, 5, 5, 1])\n  // ys = np.array([[0], [1]])\n  // h = model.fit(xs, ys, epochs=3)\n  // print(h.history)\n  // ```\n  it('Fit: Wtih conv2d layer', async () => {\n    const model = tfl.sequential();\n    model.add(tfl.layers.conv2d({\n      filters: 4,\n      kernelSize: 2,\n      kernelInitializer: 'ones',\n      biasInitializer: 'zeros',\n      inputShape: [5, 5, 1]\n    }));\n    model.add(tfl.layers.batchNormalization());\n    model.add(tfl.layers.flatten());\n    model.add(tfl.layers.dense(\n        {units: 1, kernelInitializer: 'ones', biasInitializer: 'zeros'}));\n    model.compile({loss: 'meanSquaredError', optimizer: 'sgd'});\n\n    const xsData = [];\n    for (let i = 0; i < 2 * 5 * 5 * 1; ++i) {\n      xsData.push(i);\n    }\n    const xs = tensor4d(xsData, [2, 5, 5, 1]);\n    const ys = tensor2d([0, 1], [2, 1]);\n\n    const h = await model.fit(xs, ys, {epochs: 2});\n    expectTensorsClose(\n        h.history.loss as number[], [3332.9971, 2122.5361], 0.01);\n  });\n\n  // Reference Python code:\n  // ```python\n  // import numpy as np\n  // import tensorflow as tf\n  // from tensorflow import keras\n  //\n  // model = keras.Sequential()\n  // model.add(keras.layers.Conv2DTranspose(\n  //     4,\n  //     2,\n  //     kernel_initializer='ones',\n  //     bias_initializer='zeros',\n  //     input_shape=[5, 5, 1]))\n  // model.add(keras.layers.BatchNormalization())\n  // model.add(keras.layers.Flatten())\n  // model.add(keras.layers.Dense(\n  //     1,\n  //     kernel_initializer='ones',\n  //     bias_initializer='zeros'))\n  //\n  // model.compile(loss='mse', optimizer='sgd')\n  //\n  // xs = np.arange(2 * 5 * 5 * 1).reshape([2, 5, 5, 1]).astype(np.float32)\n  // xs = (xs - 25.0) / 100.0\n  // ys = np.array([[0], [1]])\n  //\n  // print(model.layers[1].get_weights())\n  // h = model.fit(xs, ys, epochs=2)\n  // print(h.history)\n  // print(model.layers[1].get_weights())\n  // ```\n  it('Fit: Wtih conv2dTranspose layer', async () => {\n    const model = tfl.sequential();\n    model.add(tfl.layers.conv2dTranspose({\n      filters: 4,\n      kernelSize: 2,\n      kernelInitializer: 'ones',\n      biasInitializer: 'zeros',\n      inputShape: [5, 5, 1]\n    }));\n    model.add(tfl.layers.batchNormalization());\n    model.add(tfl.layers.flatten());\n    model.add(tfl.layers.dense(\n        {units: 1, kernelInitializer: 'ones', biasInitializer: 'zeros'}));\n    model.compile({loss: 'meanSquaredError', optimizer: 'sgd'});\n\n    const xsData = [];\n    for (let i = 0; i < 2 * 5 * 5 * 1; ++i) {\n      xsData.push(i);\n    }\n    const xs =\n        (tensor4d(xsData, [2, 5, 5, 1]).sub(scalar(25))).div(scalar(100));\n    const ys = tensor2d([0, 1], [2, 1]);\n\n    const h = await model.fit(xs, ys, {epochs: 2});\n    expect(h.history.loss[0]).toBeCloseTo(13922.4492);\n    expect(h.history.loss[1]).toBeCloseTo(106532048, -3);\n    const weights = model.layers[1].getWeights();\n    expect(weights.length).toEqual(4);\n    expectTensorsClose(\n        weights[0], tensor1d([7661.0874, 7661.0874, 7661.0874, 7661.0874]),\n        1e-2);\n    expectTensorsClose(\n        weights[1], tensor1d([-118.35103, -118.35103, -118.35103, -118.35103]),\n        1e-2);\n    expectTensorsClose(\n        weights[2],\n        tensor1d([-0.00026271, -0.00026271, -0.00026271, -0.00026271]));\n    expectTensorsClose(\n        weights[3], tensor1d([0.98333836, 0.98333836, 0.98333836, 0.98333836]));\n  });\n\n  // Use the following Python code to get the reference values for assertion:\n  // ```python\n  // from tensorflow import keras\n  // import numpy as np\n  //\n  // layer1 = keras.layers.BatchNormalization(input_shape=[2, 2])\n  // model = keras.Sequential([layer1])\n  //\n  // model.compile(loss='mean_squared_error', optimizer='sgd')\n  //\n  // xs = np.array(\n  //     [[[1, 2], [3, 4]], [[2, 4], [6, 8]], [[12, 11], [10, 9]]],\n  //     dtype=np.float32)\n  // ys = np.zeros([3, 2, 2], dtype=np.float32)\n  // print(layer1.get_weights())\n  // history = model.fit(xs, ys, epochs=2, batch_size=3)\n  // print(history.history)\n  // print(layer1.get_weights())\n  // ```\n  it('Fit: 3D, BatchNorm Layer Only', async () => {\n    const layer1 = tfl.layers.batchNormalization({inputShape: [2, 2]});\n    const model = tfl.sequential({layers: [layer1]});\n    model.compile({loss: 'meanSquaredError', optimizer: 'sgd'});\n\n    const xs1 = tensor3d(\n        [[[1, 2], [3, 4]], [[2, 4], [6, 8]], [[12, 11], [10, 9]]], [3, 2, 2]);\n    const ys = zeros([3, 2, 2]);\n    const history = await model.fit(xs1, ys, {epochs: 2, batchSize: 3});\n    expect(history.history['loss'][0]).toBeCloseTo(0.9999215006828308);\n    expect(history.history['loss'][1]).toBeCloseTo(0.980024516582489);\n    const gammaValue = layer1.getWeights()[0];\n    expectTensorsClose(gammaValue, [0.98010117, 0.98010194]);\n    const betaValue = layer1.getWeights()[1];\n    expectTensorsClose(betaValue, [-1.1175870e-09, 8.1956386e-10]);\n    const movingMeanValue = layer1.getWeights()[2];\n    expectTensorsClose(movingMeanValue, [0.11276666, 0.12603334]);\n    const movingVarianceValue = layer1.getWeights()[3];\n    expectTensorsClose(movingVarianceValue, [1.3161889, 1.1835222], 1e-5);\n  });\n});\n\ndescribeMathCPUAndGPU('LayerNormalization Layer: Symbolic', () => {\n  it('Invalid axis value leads to constructor error', () => {\n    expect(() => tfl.layers.layerNormalization({\n      // tslint:disable-next-line:no-any\n      axis: 'foo' as any\n    })).toThrowError(/Expected axis to be an integer/);\n    expect(() => tfl.layers.layerNormalization({\n      axis: 1.2\n    })).toThrowError(/Expected axis to be an integer/);\n    expect(() => tfl.layers.layerNormalization({\n      axis: [1, 1.5]\n    })).toThrowError(/Expected axis to be an array of integers/);\n  });\n\n  it('Serialization round trip', async () => {\n    const layer = tfl.layers.layerNormalization(\n        {axis: [-2, -1], center: true, scale: false});\n    const pythonicConfig = convertTsToPythonic(layer.getConfig());\n    // tslint:disable-next-line:no-any\n    const tsConfig = convertPythonicToTs(pythonicConfig) as any;\n    const layerPrime = tfl.layers.layerNormalization(tsConfig);\n    expect(layerPrime.getConfig()).toEqual(layer.getConfig());\n  });\n\n  it('Deserialize model with BatchNorm Layer', async () => {\n    // tslint:disable:max-line-length\n    const modelJSONString =\n        `{\"class_name\": \"Sequential\", \"config\": {\"name\": \"sequential\", \"layers\": [{\"class_name\": \"Dense\", \"config\": {\"name\": \"dense\", \"trainable\": true, \"batch_input_shape\": [null, 5], \"dtype\": \"float32\", \"units\": 10, \"activation\": \"linear\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}}, {\"class_name\": \"BatchNormalization\", \"config\": {\"name\": \"batch_normalization\", \"trainable\": true, \"dtype\": \"float32\", \"axis\": [1], \"momentum\": 0.99, \"epsilon\": 0.001, \"center\": true, \"scale\": true, \"beta_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"gamma_initializer\": {\"class_name\": \"Ones\", \"config\": {}}, \"moving_mean_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"moving_variance_initializer\": {\"class_name\": \"Ones\", \"config\": {}}, \"beta_regularizer\": null, \"gamma_regularizer\": null, \"beta_constraint\": null, \"gamma_constraint\": null}}, {\"class_name\": \"Dense\", \"config\": {\"name\": \"dense_1\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 1, \"activation\": \"sigmoid\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}}]}, \"keras_version\": \"2.2.4-tf\", \"backend\": \"tensorflow\"}`;\n    // tslint:enable:max-line-length\n    const model = await tfl.models.modelFromJSON(JSON.parse(modelJSONString));\n    const ys = model.predict(zeros([3, 5])) as Tensor;\n    expect(ys.shape).toEqual([3, 1]);\n    expect(model.layers[1].getWeights().length).toEqual(4);\n  });\n});\n\ndescribeMathCPUAndGPU('LayerNormalization Layer: Tensor', () => {\n  // Reference Python code:\n  // ```py\n  // import numpy as np\n  // import tensorflow as tf\n  //\n  // tf.enable_eager_execution()\n  //\n  // layer = tf.keras.layers.LayerNormalization()\n  // xs = np.array([[1, 2, 3], [3, 6, 24]], dtype=np.float32)\n  // ys = layer(xs)\n  // print(ys)\n  // ```\n  it('Forward, 2D input, default axis', () => {\n    const layer = tfl.layers.layerNormalization();\n    const xs = tensor2d([[1, 2, 3], [3, 6, 24]]);\n    const ys = layer.apply(xs) as Tensor;\n    expectTensorsClose(\n        ys,\n        tensor2d(\n            [[-1.2238274, 0, 1.2238274], [-0.8626572, -0.5391607, 1.401818]]));\n  });\n\n  it('Forward: no memory leak', () => {\n    const layer = tfl.layers.layerNormalization();\n    const xs = tensor2d([[1, 2, 3], [3, 6, 24]]);\n    dispose(layer.apply(xs) as Tensor);  // Warm up.\n    const numTensors0 = memory().numTensors;\n    dispose(layer.apply(xs) as Tensor);\n    expect(memory().numTensors).toEqual(numTensors0);\n  });\n\n  // Reference Python code:\n  // ```py\n  // import numpy as np\n  // import tensorflow as tf\n  //\n  // tf.enable_eager_execution()\n  //\n  // layer = tf.keras.layers.LayerNormalization()\n  // xs = np.array([1, 2, 3, 6, 5, 4, 3, 6, 24, -10, 0, 5],\n  //               dtype=np.float32).reshape((2, 2, 3))\n  // ys = layer(xs)\n  // print(ys)\n  // ```\n  it('Forward, 3D input, default axis', () => {\n    const layer = tfl.layers.layerNormalization();\n    const xs = tensor3d([1, 2, 3, 6, 5, 4, 3, 6, 24, -10, 0, 5], [2, 2, 3]);\n    const ys = layer.apply(xs) as Tensor;\n    expectTensorsClose(\n        ys, tensor3d([\n          [[-1.2238274, 0, 1.2238274], [1.2238274, 0, -1.2238274]],\n          [\n            [-0.8626572, -0.5391607, 1.401818],\n            [-1.3362889, 0.26725778, 1.0690311]\n          ]\n        ]));\n  });\n\n  // Reference Python code:\n  // ```py\n  // import numpy as np\n  // import tensorflow as tf\n  //\n  // tf.enable_eager_execution()\n  //\n  // layer = tf.keras.layers.LayerNormalization(axis=[1, 2])\n  // xs = np.array([1, 2, 3, 6, 5, 4, 3, 6, 24, -10, 0, 5],\n  //               dtype=np.float32).reshape((2, 2, 3))\n  // ys = layer(xs)\n  // print(ys)\n  // ```\n  const nonDefaultAxisValues: number[][] = [[1, 2], [-2, -1]];\n  for (const nonDefaultAxis of nonDefaultAxisValues) {\n    it(`Forward, 3D input, non-default axis: ${nonDefaultAxis}`, () => {\n      const layer = tfl.layers.layerNormalization({axis: nonDefaultAxis});\n      const xs = tensor3d([1, 2, 3, 6, 5, 4, 3, 6, 24, -10, 0, 5], [2, 2, 3]);\n      const ys = layer.apply(xs) as Tensor;\n      expectTensorsClose(ys, tensor3d([\n                           [\n                             [-1.4635992, -0.8781595, -0.29271984],\n                             [1.4635992, 0.8781595, 0.29271984]\n                           ],\n                           [\n                             [-0.1645762, 0.13166097, 1.909084],\n                             [-1.4482707, -0.46081337, 0.03291526]\n                           ]\n                         ]));\n    });\n  }\n\n  it('Duplicate items in axis leads to constructor error', () => {\n    const layers = tfl.layers.layerNormalization({axis: [-2, -1, -1]});\n    const xs = tensor3d([1, 2, 3, 6, 5, 4, 3, 6, 24, -10, 0, 5], [2, 2, 3]);\n    expect(() => layers.apply(xs)).toThrowError(/duplicate axes/);\n  });\n\n  // Reference Python code:\n  // ```py\n  // import numpy as np\n  // import tensorflow as tf\n  //\n  // model = tf.keras.Sequential()\n  // model.add(tf.keras.layers.LayerNormalization(input_shape=(3,)))\n  // model.add(tf.keras.layers.Dense(1, kernel_initializer='ones'))\n  // model.compile(loss='mse', optimizer='sgd')\n  //\n  // xs = np.array([[1, 2, 3], [3, 6, 24], [10, 5, 0]], dtype=np.float32)\n  // ys = np.array([[0], [-1], [2]], dtype=np.float32)\n  // history = model.fit(xs, ys, epochs=5, verbose=0)\n  // print(history.history)\n  // ```\n  it('Training: 2D: default axis', async () => {\n    const model = tfl.sequential();\n    model.add(tfl.layers.layerNormalization({inputShape: [3]}));\n    model.add(tfl.layers.dense({units: 1, kernelInitializer: 'ones'}));\n    model.compile({loss: 'meanSquaredError', optimizer: 'sgd'});\n    const xs = tensor2d([[1, 2, 3], [3, 6, 24], [10, 5, 0]]);\n    const ys = tensor2d([[0], [-1], [2]]);\n    const history = await model.fit(xs, ys, {epochs: 5});\n    test_util.expectArraysClose(history.history.loss as number[], [\n      1.6666666269302368, 1.4296358823776245, 1.2372404336929321,\n      1.0793765783309937, 0.9486551880836487\n    ]);\n  });\n\n  // Reference Python code:\n  // ```py\n  // import numpy as np\n  // import tensorflow as tf\n  //\n  // model = tf.keras.Sequential()\n  // model.add(tf.keras.layers.Dense(10, kernel_initializer='ones',\n  // input_shape=(3,))) model.add(tf.keras.layers.LayerNormalization())\n  // model.add(tf.keras.layers.Dense(1, kernel_initializer='ones'))\n  // model.compile(loss=\"mse\", optimizer=\"sgd\")\n  //\n  // xs = np.array([[1, 2, 3], [3, 6, 24], [10, 5, 0], [2, 7, 8]],\n  // dtype=np.float32) ys = np.array([[0], [-1], [2], [3]], dtype=np.float32)\n  // history = model.fit(xs, ys, epochs=5, verbose=0)\n  // print(history.history)\n  // ```\n  it('Training: 2D: as intermediate layer: default axis', async () => {\n    const model = tfl.sequential();\n    model.add(tfl.layers.dense(\n        {units: 10, kernelInitializer: 'ones', inputShape: [3]}));\n    model.add(tfl.layers.layerNormalization());\n    model.add(tfl.layers.dense({units: 1, kernelInitializer: 'ones'}));\n    model.compile({loss: 'meanSquaredError', optimizer: 'sgd'});\n    const xs = tensor2d([[1, 2, 3], [3, 6, 24], [10, 5, 0], [2, 7, 8]]);\n    const ys = tensor2d([[0], [-1], [2], [3]]);\n    const history = await model.fit(xs, ys, {epochs: 5});\n    test_util.expectArraysClose(history.history.loss as number[], [\n      3.5, 3.1083502769470215, 2.8706729412078857, 2.7243311405181885,\n      2.6366190910339355\n    ]);\n  });\n\n  it('Training: no memory leak', async () => {\n    const model = tfl.sequential();\n    model.add(tfl.layers.layerNormalization({inputShape: [3]}));\n    model.add(tfl.layers.dense({units: 1, kernelInitializer: 'ones'}));\n    model.compile({loss: 'meanSquaredError', optimizer: 'sgd'});\n    const xs = tensor2d([[1, 2, 3], [3, 6, 24], [10, 5, 0], [2, 7, 8]]);\n    const ys = tensor2d([[0], [-1], [2], [3]]);\n    await model.fit(xs, ys, {epochs: 1});  // Warm up.\n\n    const numTensors0 = memory().numTensors;\n    await model.fit(xs, ys, {epochs: 1});\n    expect(memory().numTensors).toEqual(numTensors0);\n  });\n\n  // Reference Python code:\n  // ```py\n  // import numpy as np\n  // import tensorflow as tf\n  //\n  // model = tf.keras.Sequential()\n  // model.add(tf.keras.layers.LayerNormalization(input_shape=(2, 3)))\n  // model.add(tf.keras.layers.Flatten())\n  // model.add(tf.keras.layers.Dense(1, kernel_initializer='ones'))\n  // model.compile(loss='mse', optimizer='sgd')\n  //\n  // xs = np.array([[[1, 2, 3], [3, 6, 24]], [[10, 5, 0], [2, 7, 8]]],\n  // dtype=np.float32) ys = np.array([[0], [-1]], dtype=np.float32) history =\n  // model.fit(xs, ys, epochs=5, verbose=0) print(history.history)\n  // ```\n  it('Training: 3D: default axis', async () => {\n    const model = tfl.sequential();\n    model.add(tfl.layers.layerNormalization({inputShape: [2, 3]}));\n    model.add(tfl.layers.flatten());\n    model.add(tfl.layers.dense({units: 1, kernelInitializer: 'ones'}));\n    model.compile({loss: 'meanSquaredError', optimizer: 'sgd'});\n    const xs = tensor3d([[[1, 2, 3], [3, 6, 24]], [[10, 5, 0], [2, 7, 8]]]);\n    const ys = tensor2d([[0], [-1]]);\n    const history = await model.fit(xs, ys, {epochs: 5});\n    test_util.expectArraysClose(history.history.loss as number[], [\n      0.5, 0.33119967579841614, 0.23371894657611847, 0.171361044049263,\n      0.12831644713878632\n    ]);\n  });\n\n  // Reference Python code:\n  // ```py\n  // import numpy as np\n  // import tensorflow as tf\n  //\n  // model = tf.keras.Sequential()\n  // model.add(tf.keras.layers.LayerNormalization(input_shape=(2, 3), axis=[-2,\n  // -1])) model.add(tf.keras.layers.Flatten())\n  // model.add(tf.keras.layers.Dense(1, kernel_initializer='ones'))\n  // model.compile(loss='mse', optimizer='sgd')\n  //\n  // xs = np.array([[[1, 2, 3], [3, 6, 24]], [[10, 5, 0], [2, 7, 8]]],\n  // dtype=np.float32) ys = np.array([[0], [-1]], dtype=np.float32) history =\n  // model.fit(xs, ys, epochs=5, verbose=0) print(history.history)\n  // ```\n  it('Training: 3D: non-default axis', async () => {\n    const model = tfl.sequential();\n    model.add(\n        tfl.layers.layerNormalization({inputShape: [2, 3], axis: [-2, -1]}));\n    model.add(tfl.layers.flatten());\n    model.add(tfl.layers.dense({units: 1, kernelInitializer: 'ones'}));\n    model.compile({loss: 'meanSquaredError', optimizer: 'sgd'});\n    const xs = tensor3d([[[1, 2, 3], [3, 6, 24]], [[10, 5, 0], [2, 7, 8]]]);\n    const ys = tensor2d([[0], [-1]]);\n    const history = await model.fit(xs, ys, {epochs: 5});\n    test_util.expectArraysClose(history.history.loss as number[], [\n      0.5, 0.3337608873844147, 0.23789873719215393, 0.17923809587955475,\n      0.1408553570508957\n    ]);\n  });\n\n  // Reference Python code:\n  // ```py\n  // import numpy as np\n  // import tensorflow as tf\n  //\n  // model = tf.keras.Sequential()\n  // embedding_layer = tf.keras.layers.Embedding(\n  //     input_dim=4, output_dim=3, input_length=4, mask_zero=True,\n  //     embeddings_initializer='ones')\n  // model.add(embedding_layer)\n  // model.add(tf.keras.layers.LayerNormalization())\n  //\n  // xs = np.array([[0, 0, 0, 0], [1, 0, 0, 0], [1, 2, 0, 0], [1, 2, 3, 0]],\n  //               dtype=np.float32)\n  // model.predict(xs)\n  //\n  // embedding_layer.set_weights([\n  //     np.array([[1, 2, 3], [3, 2, 1], [2, 3, 1], [3, 1, 2]],\n  //     dtype=np.float32)])\n  // ys = model.predict(xs)\n  // print(ys)\n  // ```\n  it('Forward, with masking', () => {\n    const model = tfl.sequential();\n    const embeddingLayer = tfl.layers.embedding({\n      inputDim: 4,\n      outputDim: 3,\n      inputLength: 4,\n      maskZero: true,\n      embeddingsInitializer: 'ones'\n    });\n    model.add(embeddingLayer);\n    // model.add(tfl.layers.dense({units: 1, kernelInitializer: 'ones'}));\n    model.add(tfl.layers.layerNormalization());\n\n    const xs =\n        tensor2d([[0, 0, 0, 0], [1, 0, 0, 0], [1, 2, 0, 0], [1, 2, 3, 0]]);\n    model.predict(xs);  // Make sure the embedding layer is built first.\n\n    embeddingLayer.setWeights(\n        [tensor2d([[1, 2, 3], [3, 2, 1], [2, 3, 1], [3, 1, 2]])]);\n    const ys = model.predict(xs) as Tensor;\n    expectTensorsClose(\n        ys, tensor3d([\n          [\n            [-1.2238274, 0, 1.2238274], [-1.2238274, 0, 1.2238274],\n            [-1.2238274, 0, 1.2238274], [-1.2238274, 0, 1.2238274]\n          ],\n          [\n            [1.2238274, 0, -1.2238274], [-1.2238274, 0, 1.2238274],\n            [-1.2238274, 0, 1.2238274], [-1.2238274, 0, 1.2238274]\n          ],\n          [\n            [1.2238274, 0, -1.2238274], [0, 1.2238274, -1.2238274],\n            [-1.2238274, 0, 1.2238274], [-1.2238274, 0, 1.2238274]\n          ],\n          [\n            [1.2238274, 0, -1.2238274], [0, 1.2238274, -1.2238274],\n            [1.2238274, -1.2238274, 0], [-1.2238274, 0, 1.2238274]\n          ]\n        ]));\n  });\n});\n"]}