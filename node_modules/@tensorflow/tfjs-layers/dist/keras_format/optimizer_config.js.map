{"version":3,"file":"optimizer_config.js","sourceRoot":"","sources":["../../src/keras_format/optimizer_config.ts"],"names":[],"mappings":"AAAA;;;;;;;;GAQG;AAiFH,4EAA4E;AAC5E,8EAA8E;AAE9E;;;;GAIG;AACH,MAAM,CAAC,MAAM,mBAAmB,GAC5B,CAAC,UAAU,EAAE,SAAS,EAAE,MAAM,EAAE,QAAQ,EAAE,UAAU,EAAE,SAAS,EAAE,KAAK,CAAC,CAAC","sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\nimport {BaseSerialization} from './types';\n\n// Because of the limitations in the current Keras spec, there is no clear\n// definition of what may or may not be the configuration of an optimizer.\n//\n// For now we'll represent the ones available in TF.js--but it will take more\n// thought to get this right in a cross-platform way.\n//\n// See internal issue: b/121033602\n\n// TODO(soergel): This is a stopgap that needs further thought.\n// Does it belong here?\n// Does it belong in tfjs-core?\n// See also the dormant https://github.com/tensorflow/tfjs-core/pull/1404\n\nexport type AdadeltaOptimizerConfig = {\n  learning_rate: number; rho: number; epsilon: number;\n};\n\nexport type AdadeltaSerialization =\n    BaseSerialization<'Adadelta', AdadeltaOptimizerConfig>;\n\nexport type AdagradOptimizerConfig = {\n  learning_rate: number;\n  initial_accumulator_value?: number;\n};\n\nexport type AdagradSerialization =\n    BaseSerialization<'Adagrad', AdagradOptimizerConfig>;\n\nexport type AdamOptimizerConfig = {\n  learning_rate: number; beta1: number; beta2: number;\n  epsilon?: number;\n};\n\nexport type AdamSerialization = BaseSerialization<'Adam', AdamOptimizerConfig>;\n\nexport type AdamaxOptimizerConfig = {\n  learning_rate: number; beta1: number; beta2: number;\n  epsilon?: number;\n  decay?: number;\n};\n\nexport type AdamaxSerialization =\n    BaseSerialization<'Adamax', AdamaxOptimizerConfig>;\n\nexport type MomentumOptimizerConfig = {\n  // extends SGDOptimizerConfig {\n  learning_rate: number; momentum: number;\n  use_nesterov?: boolean;\n};\n\nexport type MomentumSerialization =\n    BaseSerialization<'Momentum', MomentumOptimizerConfig>;\n\nexport type RMSPropOptimizerConfig = {\n  learning_rate: number;\n  decay?: number;\n  momentum?: number;\n  epsilon?: number;\n  centered?: boolean;\n};\n\nexport type RMSPropSerialization =\n    BaseSerialization<'RMSProp', RMSPropOptimizerConfig>;\n\nexport type SGDOptimizerConfig = {\n  learning_rate: number;\n};\n\nexport type SGDSerialization = BaseSerialization<'SGD', SGDOptimizerConfig>;\n\n// Update optimizerClassNames below in concert with this.\nexport type OptimizerSerialization = AdadeltaSerialization|AdagradSerialization|\n    AdamSerialization|AdamaxSerialization|MomentumSerialization|\n    RMSPropSerialization|SGDSerialization;\n\nexport type OptimizerClassName = OptimizerSerialization['class_name'];\n\n// We can't easily extract a string[] from the string union type, but we can\n// recapitulate the list, enforcing at compile time that the values are valid.\n\n/**\n * A string array of valid Optimizer class names.\n *\n * This is guaranteed to match the `OptimizerClassName` union type.\n */\nexport const optimizerClassNames: OptimizerClassName[] =\n    ['Adadelta', 'Adagrad', 'Adam', 'Adamax', 'Momentum', 'RMSProp', 'SGD'];\n"]}