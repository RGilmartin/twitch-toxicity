{"version":3,"file":"BatchMatMul_impl.js","sourceRoot":"","sources":["../../src/kernels/BatchMatMul_impl.ts"],"names":[],"mappings":"AAAA;;;;;;;;;;;;;;;GAeG;AAEH,OAAO,EAA2B,UAAU,EAAE,IAAI,EAAC,MAAM,uBAAuB,CAAC;AAGjF,OAAO,EAAC,4BAA4B,EAAC,MAAM,oCAAoC,CAAC;AAChF,OAAO,EAAC,mBAAmB,EAAC,MAAM,sBAAsB,CAAC;AAEzD,OAAO,EAAC,QAAQ,EAAC,MAAM,YAAY,CAAC;AACpC,OAAO,EAAC,OAAO,EAAC,MAAM,WAAW,CAAC;AAClC,OAAO,EAAC,GAAG,EAAC,MAAM,OAAO,CAAC;AAC1B,OAAO,EAAC,SAAS,EAAC,MAAM,aAAa,CAAC;AAEtC,8EAA8E;AAC9E,uEAAuE;AACvE,oEAAoE;AACpE,MAAM,CAAC,MAAM,2BAA2B,GAAG,IAAI,CAAC;AAchD,MAAM,UAAU,eAAe,CAAC,EAC9B,CAAC,EACD,CAAC,EACD,UAAU,EACV,UAAU,EACV,OAAO,EACP,IAAI,GAAG,IAAI,EACX,sBAAsB,GAAG,IAAI,EAC7B,cAAc,GAAG,CAAC,EAClB,UAAU,GAAG,IAAI,EACC;IAClB,MAAM,KAAK,GAAG,CAAC,CAAC,KAAK,CAAC,MAAM,CAAC;IAC7B,MAAM,KAAK,GAAG,CAAC,CAAC,KAAK,CAAC,MAAM,CAAC;IAE7B,MAAM,WAAW,GAAG,UAAU,CAAC,CAAC,CAAC,CAAC,CAAC,KAAK,CAAC,KAAK,GAAG,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,KAAK,CAAC,KAAK,GAAG,CAAC,CAAC,CAAC;IACzE,MAAM,WAAW,GAAG,UAAU,CAAC,CAAC,CAAC,CAAC,CAAC,KAAK,CAAC,KAAK,GAAG,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,KAAK,CAAC,KAAK,GAAG,CAAC,CAAC,CAAC;IAEzE,MAAM,WAAW,GAAG,UAAU,CAAC,CAAC,CAAC,CAAC,CAAC,KAAK,CAAC,KAAK,GAAG,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,KAAK,CAAC,KAAK,GAAG,CAAC,CAAC,CAAC;IACzE,MAAM,WAAW,GAAG,UAAU,CAAC,CAAC,CAAC,CAAC,CAAC,KAAK,CAAC,KAAK,GAAG,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,KAAK,CAAC,KAAK,GAAG,CAAC,CAAC,CAAC;IAEzE,MAAM,UAAU,GAAG,CAAC,CAAC,KAAK,CAAC,KAAK,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;IACxC,MAAM,UAAU,GAAG,CAAC,CAAC,KAAK,CAAC,KAAK,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;IAExC,MAAM,SAAS,GAAG,IAAI,CAAC,aAAa,CAAC,UAAU,CAAC,CAAC;IACjD,MAAM,SAAS,GAAG,IAAI,CAAC,aAAa,CAAC,UAAU,CAAC,CAAC;IAEjD,MAAM,mBAAmB,GACrB,SAAS,KAAK,SAAS,IAAI,SAAS,KAAK,CAAC,IAAI,SAAS,KAAK,CAAC,CAAC;IAElE,IAAI,CAAC,MAAM,CACP,KAAK,IAAI,CAAC,IAAI,KAAK,IAAI,CAAC,IAAI,mBAAmB,EAC/C,GAAG,EAAE,CAAC,iEAAiE;QACnE,kEAAkE;QAClE,wBAAwB,UAAU,UAAU,UAAU,IAAI,CAAC,CAAC;IAEpE,MAAM,iBAAiB,GACnB,SAAS,GAAG,SAAS,CAAC,CAAC,CAAC,CAAC,CAAC,KAAK,CAAC,KAAK,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,KAAK,CAAC,KAAK,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;IACxE,MAAM,QAAQ,GAAG,iBAAiB,CAAC,MAAM,CAAC,CAAC,WAAW,EAAE,WAAW,CAAC,CAAC,CAAC;IAEtE,IAAI,CAAC,MAAM,CACP,WAAW,KAAK,WAAW,EAC3B,GAAG,EAAE,CAAC,kCAAkC,WAAW,SAAS;QACxD,GAAG,WAAW,4BAA4B,CAAC,CAAC,KAAK,OAAO;QACxD,GAAG,CAAC,CAAC,KAAK,mBAAmB,UAAU,EAAE;QACzC,mBAAmB,UAAU,cAAc,CAAC,CAAC;IAErD,MAAM,QAAQ,GAA6B,UAAU,CAAC,CAAC;QACnD,CAAC,SAAS,EAAE,WAAW,EAAE,WAAW,CAAC,CAAC,CAAC;QACvC,CAAC,SAAS,EAAE,WAAW,EAAE,WAAW,CAAC,CAAC;IAC1C,MAAM,QAAQ,GAA6B,UAAU,CAAC,CAAC;QACnD,CAAC,SAAS,EAAE,WAAW,EAAE,WAAW,CAAC,CAAC,CAAC;QACvC,CAAC,SAAS,EAAE,WAAW,EAAE,WAAW,CAAC,CAAC;IAE1C,0EAA0E;IAC1E,MAAM,GAAG,GAAG,OAAO,CAAC,EAAC,MAAM,EAAE,EAAC,CAAC,EAAE,CAAC,EAAC,EAAE,OAAO,EAAE,KAAK,EAAE,EAAC,KAAK,EAAE,QAAQ,EAAC,EAAC,CAAC,CAAC;IACzE,MAAM,GAAG,GAAG,OAAO,CAAC,EAAC,MAAM,EAAE,EAAC,CAAC,EAAE,CAAC,EAAC,EAAE,OAAO,EAAE,KAAK,EAAE,EAAC,KAAK,EAAE,QAAQ,EAAC,EAAC,CAAC,CAAC;IAEzE,MAAM,aAAa,GAAiB,CAAC,GAAG,EAAE,GAAG,CAAC,CAAC;IAE/C,MAAM,QAAQ,GAAG,IAAI,CAAC,GAAG,CAAC,SAAS,EAAE,SAAS,CAAC,CAAC;IAChD,MAAM,SAAS,GAAG,UAAU,CAAC,CAAC,CAAC,GAAG,CAAC,KAAK,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,GAAG,CAAC,KAAK,CAAC,CAAC,CAAC,CAAC;IAE3D,MAAM,OAAO,GAAG,IAAI,IAAI,IAAI,CAAC;IAC7B,MAAM,yBAAyB,GAAG,sBAAsB,IAAI,IAAI,CAAC;IACjE,MAAM,iBAAiB,GAAG,UAAU,KAAK,WAAW,CAAC;IACrD,MAAM,eAAe,GAAG,UAAU,IAAI,IAAI,CAAC,CAAC;QACxC,4BAA4B,CAAC,UAAU,EAAE,IAAI,CAAC,CAAC,CAAC;QAChD,IAAI,CAAC;IACT,MAAM,gBAAgB,GAAG,OAAO,IAAI,yBAAyB;QACzD,iBAAiB,IAAI,eAAe,IAAI,IAAI,CAAC;IACjD,IAAI,GAAe,CAAC;IAEpB,mEAAmE;IACnE,yDAAyD;IACzD,IAAI,CAAC,WAAW,KAAK,CAAC,IAAI,WAAW,KAAK,CAAC,CAAC;QACxC,SAAS,GAAG,2BAA2B,IAAI,gBAAgB,KAAK,KAAK,EAAE;QACzE,IAAI,IAAI,GAAG,GAAG,CAAC;QACf,IAAI,IAAI,GAAG,GAAG,CAAC;QACf,IAAI,UAAU,EAAE;YACd,IAAI,GAAG,SAAS,CAAC,EAAC,MAAM,EAAE,EAAC,CAAC,EAAE,GAAG,EAAC,EAAE,OAAO,EAAE,KAAK,EAAE,EAAC,IAAI,EAAE,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,EAAC,EAAC,CAAC,CAAC;YACxE,aAAa,CAAC,IAAI,CAAC,IAAI,CAAC,CAAC;SAC1B;QACD,IAAI,UAAU,EAAE;YACd,IAAI,GAAG,SAAS,CAAC,EAAC,MAAM,EAAE,EAAC,CAAC,EAAE,GAAG,EAAC,EAAE,OAAO,EAAE,KAAK,EAAE,EAAC,IAAI,EAAE,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,EAAC,EAAC,CAAC,CAAC;YACxE,aAAa,CAAC,IAAI,CAAC,IAAI,CAAC,CAAC;SAC1B;QAED,MAAM,cAAc,GAAG,WAAW,KAAK,CAAC,CAAC;QACzC,MAAM,cAAc,GAAG,WAAW,KAAK,CAAC,CAAC;QAEzC,IAAI,MAAM,GAAG,IAAI,CAAC;QAClB,IAAI,cAAc,EAAE;YAClB,MAAM,GAAG,OAAO,CAAC;gBACf,MAAM,EAAE,EAAC,CAAC,EAAE,IAAI,EAAC;gBACjB,OAAO;gBACP,KAAK,EAAE,EAAC,KAAK,EAAE,CAAC,QAAQ,EAAE,SAAS,EAAE,CAAC,CAAC,EAAC;aACzC,CAAC,CAAC;YAEH,aAAa,CAAC,IAAI,CAAC,MAAM,CAAC,CAAC;SAC5B;QAED,MAAM,IAAI,GAAG,WAAW,KAAK,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC;QAEvC,IAAI,MAAM,GAAG,IAAI,CAAC;QAClB,IAAI,cAAc,EAAE;YAClB,MAAM,GAAG,OAAO,CAAC;gBACf,MAAM,EAAE,EAAC,CAAC,EAAE,IAAI,EAAC;gBACjB,OAAO;gBACP,KAAK,EAAE,EAAC,KAAK,EAAE,CAAC,QAAQ,EAAE,CAAC,EAAE,SAAS,CAAC,EAAC;aACzC,CAAC,CAAC;YAEH,aAAa,CAAC,IAAI,CAAC,MAAM,CAAC,CAAC;SAC5B;QAED,MAAM,OAAO,GAAG,QAAQ,CAAC,EAAC,MAAM,EAAE,EAAC,CAAC,EAAE,MAAM,EAAE,CAAC,EAAE,MAAM,EAAC,EAAE,OAAO,EAAC,CAAC,CAAC;QACpE,GAAG,GAAG,GAAG,CAAC,EAAC,MAAM,EAAE,EAAC,CAAC,EAAE,OAAO,EAAC,EAAE,OAAO,EAAE,KAAK,EAAE,EAAC,IAAI,EAAE,QAAQ,EAAE,IAAI,EAAC,EAAC,CAAC,CAAC;QAC1E,aAAa,CAAC,IAAI,CAAC,OAAO,CAAC,CAAC;KAC7B;SAAM;QACL,MAAM,KAAK,GAAG,UAAU,CAAC,CAAC,CAAC,KAAK,EAAE,CAAC,CAAC,KAAK,CAAC,CAAC;QAE3C,MAAM,OAAO,GAAG,IAAI,mBAAmB,CACnC,QAAQ,EAAE,QAAQ,EAAE,CAAC,QAAQ,EAAE,WAAW,EAAE,WAAW,CAAC,EAAE,UAAU,EACpE,UAAU,EAAE,OAAO,EAAE,eAAe,EAAE,yBAAyB,EAC/D,iBAAiB,CAAC,CAAC;QAEvB,MAAM,MAAM,GAAiB,CAAC,GAAG,EAAE,GAAG,CAAC,CAAC;QACxC,IAAI,IAAI,IAAI,IAAI,EAAE;YAChB,MAAM,CAAC,IAAI,CAAC,IAAI,CAAC,CAAC;SACnB;QACD,IAAI,yBAAyB,EAAE;YAC7B,MAAM,CAAC,IAAI,CAAC,sBAAsB,CAAC,CAAC;SACrC;QACD,IAAI,iBAAiB,EAAE;YACrB,MAAM,eAAe,GAAG,OAAO,CAAC,cAAc,CAC1C,EAAE,EAAE,SAAS,EACb,IAAI,CAAC,iBAAiB,CAAC,cAAiC,EAAE,SAAS,CAAC,CAAC,CAAC;YAC1E,MAAM,CAAC,IAAI,CAAC,eAAe,CAAC,CAAC;YAC7B,aAAa,CAAC,IAAI,CAAC,eAAe,CAAC,CAAC;SACrC;QAED,GAAG,GAAG,OAAO,CAAC,eAAe,CAAC,OAAO,EAAE,MAAM,EAAE,KAAK,CAAC,CAAC;KACvD;IAED,MAAM,WAAW,GACb,OAAO,CAAC,EAAC,MAAM,EAAE,EAAC,CAAC,EAAE,GAAG,EAAC,EAAE,OAAO,EAAE,KAAK,EAAE,EAAC,KAAK,EAAE,QAAQ,EAAC,EAAC,CAAC,CAAC;IACnE,aAAa,CAAC,IAAI,CAAC,GAAG,CAAC,CAAC;IACxB,KAAK,MAAM,CAAC,IAAI,aAAa,EAAE;QAC7B,OAAO,CAAC,6BAA6B,CAAC,CAAC,CAAC,CAAC;KAC1C;IACD,OAAO,WAAW,CAAC;AACrB,CAAC","sourcesContent":["/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {backend_util, TensorInfo, upcastType, util} from '@tensorflow/tfjs-core';\n\nimport {MathBackendWebGL} from '../backend_webgl';\nimport {mapActivationToShaderProgram} from '../kernel_utils/kernel_funcs_utils';\nimport {MatMulPackedProgram} from '../mulmat_packed_gpu';\n\nimport {multiply} from './Multiply';\nimport {reshape} from './Reshape';\nimport {sum} from './Sum';\nimport {transpose} from './Transpose';\n\n// Empirically determined minimal shared dimension in matmul before we forward\n// to a.mul(b).sum() in order to take advantage of GPU parallelism. See\n// https://github.com/tensorflow/tfjs-core/pull/1379 for benchmarks.\nexport const MATMUL_SHARED_DIM_THRESHOLD = 1000;\n\ntype BatchMatMulConfig = {\n  a: TensorInfo,\n  b: TensorInfo,\n  transposeA: boolean,\n  transposeB: boolean,\n  backend: MathBackendWebGL,\n  bias?: TensorInfo,\n  preluActivationWeights?: TensorInfo,\n  leakyreluAlpha?: number,\n  activation?: backend_util.Activation\n};\n\nexport function batchMatMulImpl({\n  a,\n  b,\n  transposeA,\n  transposeB,\n  backend,\n  bias = null,\n  preluActivationWeights = null,\n  leakyreluAlpha = 0,\n  activation = null\n}: BatchMatMulConfig): TensorInfo {\n  const aRank = a.shape.length;\n  const bRank = b.shape.length;\n\n  const innerShapeA = transposeA ? a.shape[aRank - 2] : a.shape[aRank - 1];\n  const innerShapeB = transposeB ? b.shape[bRank - 1] : b.shape[bRank - 2];\n\n  const outerShapeA = transposeA ? a.shape[aRank - 1] : a.shape[aRank - 2];\n  const outerShapeB = transposeB ? b.shape[bRank - 2] : b.shape[bRank - 1];\n\n  const outerDimsA = a.shape.slice(0, -2);\n  const outerDimsB = b.shape.slice(0, -2);\n\n  const batchDimA = util.sizeFromShape(outerDimsA);\n  const batchDimB = util.sizeFromShape(outerDimsB);\n\n  const batchDimsCompatible =\n      batchDimA === batchDimB || batchDimA === 1 || batchDimB === 1;\n\n  util.assert(\n      aRank >= 2 && bRank >= 2 && batchDimsCompatible,\n      () => `Error in matMul: the input batch dimensions must either be the ` +\n          `same or at least one input batch dimension must be 1. Got input ` +\n          `batch dimensions of (${outerDimsA}) and (${outerDimsB}).`);\n\n  const outShapeOuterDims =\n      batchDimA > batchDimB ? a.shape.slice(0, -2) : b.shape.slice(0, -2);\n  const outShape = outShapeOuterDims.concat([outerShapeA, outerShapeB]);\n\n  util.assert(\n      innerShapeA === innerShapeB,\n      () => `Error in matMul: inner shapes (${innerShapeA}) and (` +\n          `${innerShapeB}) of Tensors with shapes ${a.shape} and ` +\n          `${b.shape} and transposeA=${transposeA}` +\n          ` and transposeB=${transposeB} must match.`);\n\n  const a3dShape: [number, number, number] = transposeA ?\n      [batchDimA, innerShapeA, outerShapeA] :\n      [batchDimA, outerShapeA, innerShapeA];\n  const b3dShape: [number, number, number] = transposeB ?\n      [batchDimB, outerShapeB, innerShapeB] :\n      [batchDimB, innerShapeB, outerShapeB];\n\n  // The rest of the implementation is designed to operate on rank-3 tensors\n  const a3d = reshape({inputs: {x: a}, backend, attrs: {shape: a3dShape}});\n  const b3d = reshape({inputs: {x: b}, backend, attrs: {shape: b3dShape}});\n\n  const intermediates: TensorInfo[] = [a3d, b3d];\n\n  const batchDim = Math.max(batchDimA, batchDimB);\n  const sharedDim = transposeA ? a3d.shape[1] : a3d.shape[2];\n\n  const hasBias = bias != null;\n  const hasPreluActivationWeights = preluActivationWeights != null;\n  const hasLeakyreluAlpha = activation === 'leakyrelu';\n  const fusedActivation = activation != null ?\n      mapActivationToShaderProgram(activation, true) :\n      null;\n  const containsFusedOps = hasBias || hasPreluActivationWeights ||\n      hasLeakyreluAlpha || fusedActivation != null;\n  let out: TensorInfo;\n\n  // Since the matrices are vectors, it is faster to call mul().sum()\n  // because sum() is O(sqrt(N)) due to divide-and-conquer.\n  if ((outerShapeA === 1 || outerShapeB === 1) &&\n      sharedDim > MATMUL_SHARED_DIM_THRESHOLD && containsFusedOps === false) {\n    let aVec = a3d;\n    let bVec = b3d;\n    if (transposeA) {\n      aVec = transpose({inputs: {x: a3d}, backend, attrs: {perm: [0, 2, 1]}});\n      intermediates.push(aVec);\n    }\n    if (transposeB) {\n      bVec = transpose({inputs: {x: b3d}, backend, attrs: {perm: [0, 2, 1]}});\n      intermediates.push(bVec);\n    }\n\n    const shouldReshapeA = outerShapeB !== 1;\n    const shouldReshapeB = outerShapeB === 1;\n\n    let aVec3d = aVec;\n    if (shouldReshapeA) {\n      aVec3d = reshape({\n        inputs: {x: aVec},\n        backend,\n        attrs: {shape: [batchDim, sharedDim, 1]}\n      });\n\n      intermediates.push(aVec3d);\n    }\n\n    const axis = outerShapeB === 1 ? 2 : 1;\n\n    let bVec3d = bVec;\n    if (shouldReshapeB) {\n      bVec3d = reshape({\n        inputs: {x: bVec},\n        backend,\n        attrs: {shape: [batchDim, 1, sharedDim]}\n      });\n\n      intermediates.push(bVec3d);\n    }\n\n    const product = multiply({inputs: {a: aVec3d, b: bVec3d}, backend});\n    out = sum({inputs: {x: product}, backend, attrs: {axis, keepDims: true}});\n    intermediates.push(product);\n  } else {\n    const dtype = upcastType(a.dtype, b.dtype);\n\n    const program = new MatMulPackedProgram(\n        a3dShape, b3dShape, [batchDim, outerShapeA, outerShapeB], transposeA,\n        transposeB, hasBias, fusedActivation, hasPreluActivationWeights,\n        hasLeakyreluAlpha);\n\n    const inputs: TensorInfo[] = [a3d, b3d];\n    if (bias != null) {\n      inputs.push(bias);\n    }\n    if (hasPreluActivationWeights) {\n      inputs.push(preluActivationWeights);\n    }\n    if (hasLeakyreluAlpha) {\n      const $leakyreluAlpha = backend.makeTensorInfo(\n          [], 'float32',\n          util.createScalarValue(leakyreluAlpha as {} as 'float32', 'float32'));\n      inputs.push($leakyreluAlpha);\n      intermediates.push($leakyreluAlpha);\n    }\n\n    out = backend.runWebGLProgram(program, inputs, dtype);\n  }\n\n  const outReshaped =\n      reshape({inputs: {x: out}, backend, attrs: {shape: outShape}});\n  intermediates.push(out);\n  for (const i of intermediates) {\n    backend.disposeIntermediateTensorInfo(i);\n  }\n  return outReshaped;\n}\n"]}